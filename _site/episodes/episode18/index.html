<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<html>
<head>
<meta charset="utf-8">


<title>Episode 18 - Benchmarking AWS Flow Logs &mdash; Head in the Clouds</title>
<meta name="description" content="18 - Benchmarking AWS Flow Logs



Welcome to Episode 18 of the “Head in the Clouds” Video Series. I am Ken Hartman, a SANS Certified Instructor and content creator for the SANS Cloud Security Curriculum.

Today’s episode is titled: “Benchmarking AWS Flow Logs”

In many of the of the courses in the SANS Cloud Curriculum, we will have one or more labs that involves flow logs. However, a portion of the lab time is spent waiting for the flow logs to get provisioned. It’s kind of like watching and waiting for water to boil. Then, once the flow logs are provisioned, it takes time for the measured traffic to show up. Therefore, I thought it would be very interesting to determine what one should expect via experimentation and observation. Along the way, I made some very interesting discoveries, and I cannot wait to show them to you.

Objectives

Anytime we use a tool, technique, or service, we should be aware of its limitations and possible pitfalls. As such, here are the objectives for today:


  Experimentally determine the typical time to provision flow logs to an S3 bucket.
  Experimentally determine the typical time to provision flow logs to a CloudWatch logs group.
  Experimentally determine the typical delay for ongoing traffic to be captured in the flow log record.
  Contemplate the implications of our findings.


Setup

For this episode, we will assume that you have an AWS EC2 virtual machine running in a VPC that has a “Name” tag of “hitc-vpc.” The t2.micro type that is in the free tier will do just fine. I recommend that you use the Ubuntu 20.04 AMI for your region, as this will allow me to provide you with the exact commands to install some additional software later in this episode.

I used Terraform to deploy my environment as described in the HitC Episode titled Episode 10 - Demonstration of Terraform Modules; Deploy a VM in AWS, Azure and GCP at once as I had originally planned to benchmark all three cloud service providers. However, as I got deeper and deeper into my research into AWS flow logs, I decided to stay focused on just AWS and may circle back to Azure and GCP flow logs in one or more future episodes.

In this episode, we will be making extensive use of the AWS command line interface. This has two benefits:

  It will make it very easy for you to reproduce my work, and
  It will allow us to measure the time that it takes for things to happen.


The machine on which you are running the CLI commands will need jq in addition to the AWS command line interface.

OBJECTIVE 1: Experimentally determine the typical time to provision flow logs to an S3 bucket

To start with, we need to set some variables that will be used by our commands:

UNIQ_ID=$RANDOM; echo $UNIQ_ID
ACCOUNT=$(aws sts get-caller-identity | jq -r .Account); echo $ACCOUNT
YYYY=$(date -Idate | cut -d"-" -f1); echo $YYYY
MM=$(date -Idate | cut -d"-" -f2); echo $MM
DD=$(date -Idate | cut -d"-" -f3); echo $DD
REGION=$(aws ec2 describe-availability-zones --output text --query 'AvailabilityZones[0].[RegionName]'); echo $REGION



We generate a unique ID to ensure that the S3 bucket that we create is unique. Linux has a variable called $RANDOM that we can use for this purpose.

The YYYY, MM, and DD are captured so we can look into the bucket for our data.

Lastly, we use a trick from StackOverflow.com to get the region, as yours may be different from mine.

Next, we will create a variety of “helper commands.”

Get the VPC ID

To get the VPC ID of the VPC that is named “hitc-vpc” we can use:

VPC_ID=$(aws ec2 describe-vpcs --filters Name=tag:Name,Values=hitc-vpc --query "Vpcs[0].VpcId" --output text); echo $VPC_ID



Create the Bucket for the Flow Logs

Next, we need to create an S3 bucket to receive our flow logs:

aws s3 mb s3://hitc-benchmark-flowlogs-$UNIQ_ID



Set up the Flow Logs for the VPC and Send to S3

Our next task is to make sure we have the command to set the flow logs for the VPC and send them to S3:

aws ec2 create-flow-logs \
    --resource-type VPC \
    --resource-ids $VPC_ID \
    --traffic-type ALL \
    --log-destination-type s3 \
    --log-destination arn:aws:s3:::hitc-benchmark-flowlogs-$UNIQ_ID \
    --max-aggregation-interval 600 \
    --tag-specifications 'ResourceType=vpc-flow-log,Tags=[{Key=Name,Value=hitc-flow-log}]'



Describe the Flow Logs for the VPC

Having done that, we may find it helpful to be able to describe the flow logs for the VPC in question:

aws ec2 describe-flow-logs --filter "Name=resource-id,Values=$VPC_ID"



Delete the Flow Logs for the VPC

Next, we need to be able to delete the flow logs that we just created. To do this we need to know the Flow Log ID.

FLOW_LOG_ID=$(aws ec2 describe-flow-logs --filter "Name=resource-id,Values=$VPC_ID" --query 'FlowLogs[0].FlowLogId' --output text)
echo $FLOW_LOG_ID
aws ec2 delete-flow-logs --flow-log-ids $FLOW_LOG_ID



List the contents of the Bucket

To verify the presence of a flow log, we will need to be able to drill deep into the S3 bucket. This is facilitated by the variables we set earlier.

aws s3 ls s3://hitc-benchmark-flowlogs-$UNIQ_ID/AWSLogs/$ACCOUNT/vpcflowlogs/$REGION/$YYYY/$MM/$DD/



Empty the Bucket

We also need the ability to empty the bucket between successive iterations of our test:

aws s3 rm s3://hitc-benchmark-flowlogs-$UNIQ_ID --recursive



Remove the Bucket

It is also good idea to remove the bucket when we are done with our testing:

aws s3 rb s3://hitc-benchmark-flowlogs-$UNIQ_ID


Generate Flow Log Traffic

Now that we’ve got our helper scripts figured out and tested, we need to generate some traffic to get captured by our flow log. SSH into your easy to instance and run the following command:

while true ; do wget --timeout=10 http://1.1.1.1:81; sleep 20; done



Note that I selected CloudFlare as the target as they are big enough that this would not be considered abusive and by sending the traffic to port 81, the retries will keep working until exhausted after 20 attempts and will then start over after 20 seconds. Anything that repetitively generates traffic will work here.

TEST 1 - Time the provisioning of the flow Log until first flow log record in S3

With that done, we can now measure the time between the provisioning of the flow log until first flow log record in S3. Every 20 seconds we will check the S3 bucket to see if the flow log showed up yet. Our “TEST” checks to see if the length of response of the aws s3 ls command is zero. Once there is a flow log record in the bucket, the infinite loop will exit and we subtract the start time from the end time. Working in epoch time, makes this easy.

NOTE: This assumes that the S3 bucket exists, but is empty

# Set up the Flow Logs for the VPC
aws ec2 create-flow-logs \
    --resource-type VPC \
    --resource-ids $VPC_ID \
    --traffic-type ALL \
    --log-destination-type s3 \
    --log-destination arn:aws:s3:::hitc-benchmark-flowlogs-$UNIQ_ID \
    --max-aggregation-interval 600 \
    --tag-specifications 'ResourceType=vpc-flow-log,Tags=[{Key=Name,Value=hitc-flow-log}]'
# Watch for initial flow logs
printf "Waiting for the flow logs to apear in S3 bucket "
START_TIMER=$(date +%s)
TEST=""
while [ -z "$TEST" ] ; do
  printf "."
  sleep 20
  TEST=$(aws s3 ls s3://hitc-benchmark-flowlogs-$UNIQ_ID/AWSLogs/$ACCOUNT/vpcflowlogs/$REGION/$YYYY/$MM/$DD/)
done
STOP_TIMER=$(date +%s)
echo "DONE"
echo $((STOP_TIMER-START_TIMER))" seconds have elapsed"



While this script executes, it will print a dot to stdout every 20 seconds. When it is done, the output will look like this:

772 seconds have elapsed


Ok, 772 seconds is 12.87 minutes. We now have our TEST 1 measurement. Note that this can be off by 20 seconds due to the sleep timer, which can be removed.

Tear down TEST 1

To repeat the test, we need to delete the flow log and empty the S3 bucket, but leave the S3 bucket in place.

# Delete the Flow Logs for the VPC
FLOW_LOG_ID=$(aws ec2 describe-flow-logs --filter "Name=resource-id,Values=$VPC_ID" --query 'FlowLogs[0].FlowLogId' --output text)
echo $FLOW_LOG_ID
aws ec2 delete-flow-logs --flow-log-ids $FLOW_LOG_ID
# Empty the Bucket
aws s3 rm s3://hitc-benchmark-flowlogs-$UNIQ_ID --recursive



TEST 1 Summary

This table shows the results of repeating the test 3 more times:


  
    
      Script Results
      Minutes
    
  
  
    
      772 seconds have elapsed
      12.86 Minutes
    
    
      417 seconds have elapsed
      6.97 Minutes
    
    
      813 seconds have elapsed
      13.55 Minutes
    
    
      960 seconds have elapsed
      16.00 Minutes
    
  


Wow, it can take 16 minutes to get your first flow logs!

So, what is up with this? Well, the catch phrase that the cloud service providers use is eventual consistency. We know that the big guys use queuing and focus intently on optimization, so I am guessing that it is for those reasons that there is a provisioning delay.

Final Cleanup TEST 1

Now that we have obtained our results, make sure that you delete the flow log and empty the bucket again, using the last set of commands and then remove the bucket, as we no longer need it.

# Remove the Bucket
aws s3 rb s3://hitc-benchmark-flowlogs-$UNIQ_ID



OBJECTIVE 2: Experimentally determine the typical time to provision flow logs to a Cloud Watch logs group

Now, let’s take a look at Flow Logs sent to Cloud Watch Logs.

Create the CloudWatch Logs Group

First we need to create the CloudWatch Logs Group:

aws logs create-log-group --log-group-name hitc-flow-logs



Delete the Log Group

And, here is the corresponding command to delete the log group:

aws logs delete-log-group --log-group-name hitc-flow-logs



Create the CloudWatch Logs Role

Next, we need create the CloudWatch Logs Role. To do this we will create the Trust Policy JSON file, the Permissions Policy JSON file, and then attach them to a new role.

Create the Trust Policy JSON file. I like to use the “heredoc” technique, so that everything can be scripted:

cat &lt;&lt; EOF &gt; trust-policy.json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "vpc-flow-logs.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
EOF



Create the Permissions Policy JSON file:

cat &lt;&lt; EOF &gt; cw-flow-logs-policy.json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "logs:CreateLogGroup",
        "logs:CreateLogStream",
        "logs:PutLogEvents",
        "logs:DescribeLogGroups",
        "logs:DescribeLogStreams"
      ],
      "Resource": "*"
    }
  ]
}   
EOF



Next, run the following commands to create the policies and the role:

aws iam create-policy --policy-name hitc-cw-flow-logs --policy-document file://cw-flow-logs-policy.json
aws iam create-role --role-name hitc-cw-flow-logs --assume-role-policy-document file://trust-policy.json
aws iam attach-role-policy --policy-arn "arn:aws:iam::$ACCOUNT:policy/hitc-cw-flow-logs" --role-name hitc-cw-flow-logs
aws iam list-attached-role-policies --role-name hitc-cw-flow-logs



TEST 2 - Time the provisioning of the flow Log until first flow log record in CloudWatch

With our setup for Test 2 in place, we can now conduct it. This “TEST” checks to see if the length of response of the aws logs describe-log-streams command is zero. Once there is a flow log record in CW logs, the infinite loop will exit and we subtract the start time from the end time.

NOTE: This assumes that the CloudWatch Log Group exists, but is empty.

# Set up the Flow Logs for the VPC
aws ec2 create-flow-logs \
    --resource-type VPC \
    --resource-ids $VPC_ID \
    --traffic-type ALL \
    --log-destination-type cloud-watch-logs \
    --log-group-name hitc-flow-logs \
    --max-aggregation-interval 600 \
    --tag-specifications 'ResourceType=vpc-flow-log,Tags=[{Key=Name,Value=hitc-flow-log}]' \
    --deliver-logs-permission-arn 'arn:aws:iam::690634326977:role/hitc-cw-flow-logs'
# Watch for initial flow logs
printf "Waiting for the flow logs to appear in CW Logs Group "
START_TIMER=$(date +%s)
TEST=""
while [ -z "$TEST" ] ; do
  printf "."
  sleep 20
  TEST=$(aws logs describe-log-streams --log-group-name hitc-flow-logs --output text)
done
STOP_TIMER=$(date +%s)
echo "DONE"
echo $((STOP_TIMER-START_TIMER))" seconds have elapsed"



Tear down TEST 2

To re-run the test, delete the flow log and reset the CW Logs Group.

# Delete the Flow Logs for the VPC
FLOW_LOG_ID=$(aws ec2 describe-flow-logs --filter "Name=resource-id,Values=$VPC_ID" --query 'FlowLogs[0].FlowLogId' --output text)
echo $FLOW_LOG_ID
aws ec2 delete-flow-logs --flow-log-ids $FLOW_LOG_ID
# Delete and Recreate the CW Logs Group
aws logs delete-log-group --log-group-name hitc-flow-logs
aws logs create-log-group --log-group-name hitc-flow-logs



Final Cleanup TEST 2

And when we are all done, we can run the following commands to delete the role and associated policy. But hold off for now as we will need this role for Test 3.

# CLI commands to tear down the role and IAM policy
aws iam detach-role-policy --role-name hitc-cw-flow-logs --policy-arn "arn:aws:iam::$ACCOUNT:policy/hitc-cw-flow-logs"
aws iam delete-role --role-name hitc-cw-flow-logs
aws iam delete-policy --policy-arn "arn:aws:iam::$ACCOUNT:policy/hitc-cw-flow-logs"



TEST 2 Summary


  
    
      Script Results
      Minutes
    
  
  
    
      21 seconds have elapsed
      0.35 Minutes
    
    
      541 seconds have elapsed
      9.02 Minutes
    
    
      522 seconds have elapsed
      8.70 Minutes
    
    
      375 seconds have elapsed
      6.25 Minutes
    
  


This is much better, but it can still take up to 10 minutes and is somewhat variable.

OBJECTIVE 3: Experimentally determine the typical delay for ongoing traffic to be captured in the flow log record.

Now, onto what is probably the most interesting and has the biggest implications for security monitoring. We want to know how long will it take for traffic that occurs in my VPC to show up as recorded in my flow logs? In other words, “I want to know how much lag there is in any detective control that I implement (that is based on flow logs).”

The security implications of this are fairly significant.

The Problem Statement

Wouldn’t it be great to time stamp a packet, send it across the virtual network of our VPC, look at the packet in CloudWatch Logs, and determine how much delay there was between the time the packet was sent, and when it was recorded?

That would be fantastic. The only problem is that a flow log contains only metadata, and not any payload. Here is a screenshot of what we have to work with:



As it turns out, about the only thing that we have to fiddle with is the Source port and the Destination port. So, I will use the source port to encode the timestamp and send it to an unchanging port (TCP port 7777) at a target IP address outside of my VPC.

Will this work?

Let’s do some math here. We know that there are 65536 (2 to the power of 16) possible TCP ports.

If we send one packet per second from a different port, we could do that for 1092 Minutes:

&gt;&gt;&gt; (2 ** 16) / 60
1092.2666666666667


Or converting that to hours:

&gt;&gt;&gt; (2 ** 16) / 60 / 60
18.204444444444444


We get 18 hours. Hence, we can send traffic at a rate of one packet per second from a unique source port for over 18 hours before using up all possible ports!

Converting back and forth from the source port to the epoch time

To calculate which port to use, we will take the current epoch time and divide it by 65536 and then take the integer portion of the remainder and use that for the source port. The math function that returns the remainder is called “modulo” and is represented as a % in python

source port = int(current_epoch_time % 65536)


To convert back to the epoch time from the source port, take the current epoch time and divide it by 65536 and then keep the quotient portion. Divide the source port by 65536 and take that result and add it to the quotient. Multiply the new total by 65536 and the new result is the epoch time of when the data was sent.

epoch_timestamp = int((modulo / 65536.0 + int(current_epoch_time / 65536)) * 65536)


This conversion assumes that the current epoch time is within the 18 hour window represented by the source port value.

Introducing Scapy

To generate a packet per second with an ever increasing source port that represents the time that the packet was sent, I used scapy. Scapy is written as a python module and is ideal for programmatically crafting packets.

As we will be running Scapy on our EC2 instance to send traffic out of our VPC, we need to install Scapy, its dependencies, and the additional modules my python program will use:

sudo apt update
sudo apt install -y python3-pip tcpdump  python3-matplotlib
sudo pip3 install pytz
sudo pip install --pre scapy[complete]


Note that using sudo with pip to install software is generally a bad idea. See this link for a detailed discussion of the security issues.

However, in this case, Scapy needs to run as root to be able to send packets, and we will destroy our EC2 instance at the end of our experimentation.

To launch Scapy, just type sudo scapy at the Ubuntu command line as shown below:

ubuntu@ip-10-0-1-112:~$ sudo scapy
INFO: PyX dependencies are not installed ! Please install TexLive or MikTeX.

                     aSPY//YASa
             apyyyyCY//////////YCa       |
            sY//////YSpcs  scpCY//Pp     | Welcome to Scapy
 ayp ayyyyyyySCP//Pp           syY//C    | Version 2.4.5
 AYAsAYYYYYYYY///Ps              cY//S   |
         pCCCCY//p          cSSps y//Y   | https://github.com/secdev/scapy
         SPPPP///a          pP///AC//Y   |
              A//A            cyP////C   | Have fun!
              p///Ac            sC///a   |
              P////YCpc           A//A   | We are in France, we say Skappee.
       scccccp///pSP///p          p//Y   | OK? Merci.
      sY/////////y  caa           S//P   |             -- Sebastien Chabal
       cayCyayP//Ya              pY/Ya   |
        sY/PsY////YCc          aC//Yp
         sc  sccaCY//PCypaapyCP//YSs
                  spCPY//////YPSps
                       ccaacs
                                       using IPython 8.1.1
&gt;&gt;&gt;


I set up a target VM in another VPC with the security group set to allow inbound TCP port 7777 from the system with Scapy installed. As a test, I am running TCPdump on the target machine, using the following command:

sudo tcpdump -i eth0 'port 7777'



Next, use Scapy to send the target a packet. Enter the following commands at the Scapy IPython prompt, but substitute my target IP address (35.168.8.211) for the one you are using:

ip=IP(dst="35.168.8.211")
tcp=TCP(sport=1111,dport=7777,flags="S",options=[('Timestamp',(0,0))])
pay="This is Test3"
test3=ip/tcp/pay
send(test3)


The result should look like:

.
Sent 1 packets.


NOTE: If you get a “Operation not permitted” error, you forgot to run Scapy as root

Exit back to the Ubuntu command line using exit()

Looking over at the target system, we can see that TCPdump generated the following two records:

22:25:43.461121 IP ec2-34-201-148-40.compute-1.amazonaws.com.1111 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13
22:25:43.461156 IP ip-172-31-48-150.ec2.internal.7777 &gt; ec2-34-201-148-40.compute-1.amazonaws.com.1111: Flags [R.], seq 0, ack 14, win 0, length 0


We Sent a TCP “Syn” packet and a “Reset” was sent back because there is not a service listening on port 7777.

Putting the theory to work

Ok, so now that we know the Scapy commands to craft a packet. We can write a program to determine the Source Port to use, to craft the packet and sent it. We also can prove that the packet was received by the target system and see the timestamp printed to standard out by TCPdump.

The program is called generate-test-traffic.py and can be downloaded from my Github Repo

Copy this program to the EC2 instance that has Scapy Installed.

wget https://raw.githubusercontent.com/Resistor52/hitc-benchmark-flow-logs/hitc-ep18/generate-test-traffic.py



Run the program as root using

sudo python3 ./generate-test-traffic.py



Once the program runs, it will ask four questions as shown below and then start to send packets at a rate of one per second.

ubuntu@ip-10-0-1-112:~$ sudo python3 ./generate-test-traffic.py
Enter the name for the CSV log file (without the csv extension): test3A
Enter the desired target IP aggress in the format "111.111.111.111": 35.168.8.211
Enter the desired start time in the format "HH:MM" for local time zone: 19:00
Enter the desired end time in the format "HH:MM" for local time zone: 22:00
Traffic generation will start at: 19:00
Traffic generation will continue until: 22:00
**************** Start of Traffic **************************
.Epoch Time: 1646344800   Time: 2022-03-03 22:00:00 UTC+0000   srcPort: 14944   Remaining Time: 0 Seconds
***************** End of Traffic ***************************


In the example above, I am calling the log file test3A and provided 35.168.8.211 as the target IP. (Again, be sure to use the IP address of the target that you set up and has TCPdump listening.) I started the program at 19:00 hours UTC, simply because that was on the hour, and had it run for 3 hours, ending at 22:00. The program uses UTC because that is the time zone that AWS uses for all its cloud services, including EC2.

Now, while we are waiting for the traffic generation to start, rerun TCPdump on the target system, this time capturing the packets to a file:

sudo tcpdump -i eth0 'port 7777' -w tcpdump.pcap


Initial Look at the Data

The generate-test-traffic.py script created a log, which I named test3A.csv. Note that the script adds the “csv” extension to the filename. The top 10 rows of the file are shown below:

"Epoch_Time","Date_Time","Source_Port"
"1646334000","2022-03-03 19:00:00 UTC+0000","4144"
"1646334001","2022-03-03 19:00:01 UTC+0000","4145"
"1646334002","2022-03-03 19:00:02 UTC+0000","4146"
"1646334003","2022-03-03 19:00:03 UTC+0000","4147"
"1646334004","2022-03-03 19:00:04 UTC+0000","4148"
"1646334005","2022-03-03 19:00:05 UTC+0000","4149"
"1646334006","2022-03-03 19:00:06 UTC+0000","4150"
"1646334007","2022-03-03 19:00:07 UTC+0000","4151"
"1646334008","2022-03-03 19:00:08 UTC+0000","4152"


We can look at the first 10 packets received by the Target system using the following tcpdump command:

tcpdump -r tcpdump.pcap -nn -c10 'dst port 7777'


And the output is:

19:00:00.013928 IP ec2-34-201-148-40.compute-1.amazonaws.com.4144 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13
19:00:01.018892 IP ec2-34-201-148-40.compute-1.amazonaws.com.4145 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13
19:00:02.023309 IP ec2-34-201-148-40.compute-1.amazonaws.com.4146 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13
19:00:03.027319 IP ec2-34-201-148-40.compute-1.amazonaws.com.4147 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13
19:00:04.032328 IP ec2-34-201-148-40.compute-1.amazonaws.com.4148 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13
19:00:05.050076 IP ec2-34-201-148-40.compute-1.amazonaws.com.4149 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13
19:00:06.061365 IP ec2-34-201-148-40.compute-1.amazonaws.com.4150 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13
19:00:07.066456 IP ec2-34-201-148-40.compute-1.amazonaws.com.4151 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13
19:00:08.071536 IP ec2-34-201-148-40.compute-1.amazonaws.com.4152 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13
19:00:09.077085 IP ec2-34-201-148-40.compute-1.amazonaws.com.4153 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13


We can extract the two critical data elements from packet capture using the following command:

tcpdump -r tcpdump.pcap -nn -c10 'dst port 7777' | cut -d" " -f1,3 | cut -d"." -f1,6 | tr "." " "


And this command will output the timestamp and the source port of the packet received by the target system:

19:00:00 4144
19:00:01 4145
19:00:02 4146
19:00:03 4147
19:00:04 4148
19:00:05 4149
19:00:06 4150
19:00:07 4151
19:00:08 4152
19:00:09 4153


Great, everything correlates! Now lets write the whole packet to a file:

tcpdump -r tcpdump.pcap -nn 'dst port 7777' | cut -d" " -f1,3 | cut -d"." -f1,6 | tr "." " " &gt; test3A.log


We will save test3A.log for later, as it will come in handy.

Looking in CloudWatch Logs

The CloudWatch Logs Web Console is a great tool to slice and dice log data. Paste in the following query:

fields @ingestionTime, @timestamp, ((toMillis(@ingestionTime) - toMillis(@timestamp))/1000) as IngestDelaySecs, ((toMillis(@ingestionTime) - toMillis(@timestamp))/1000/60)
as IngestDelayMins, toMillis(@timestamp)/1000 as TimeStampEpoch, (65536*floor(TimeStampEpoch/65536))+srcPort as SentTimeEpoch, fromMillis(SentTimeEpoch * 1000) as SentTime,
TimeStampEpoch-SentTimeEpoch as TimeStampDelaySec, srcPort
| sort srcPort asc
| limit 10000
| filter srcAddr = "10.0.1.112" and dstAddr = "35.168.8.211" and dstPort != "22" and srcPort != "22")


Be sure to set the time filter in the upper window to include the window in which the packets were sent.



Looking at the output, we see that AWS provides two fields @ingestionTime and @timestamp so we can calculate the delay the Ingestion Delay Seconds (IngestDelaySecs) by converting both time values to Epoch time and subtracting them.

But how accurate is the timestamp? Well, we can use the CloudWatch Logs Query Language to calculate a SentTimeEpoch based on the srcPort. We can also convert that to SentTime so it is human-readable. If we subtract the SentTimeEpoch from the  TimeStampEpoch we get the TimeStampDelaySec.

Disregard the first record as that was captured when I demonstrated how to use Scapy with IPython using an arbitrary source port. (I generated the traffic using the Python Script a few hours beforehand and that explains the chronology of this timestamp.) This record will be removed from the downloaded dataset.

Another thing, that you may have noticed in the screenshot, is that the first srcPort is 4184 which is 40 seconds after we sent the packet with srcPort 4144!

Very interesting. It looks like we have “negative delays.” or to phrase it more accurately, the @timestamp value attached by AWS has an error that fluctuates within a range.

Use the “Export Results” button to save the file as logs-insights-results.csv

Looking at the Data Graphically

Since we see that we have an Ingest Delay and a Timestamp Error, it would be good to look at this data graphically. Toward that end, I have create a python program called plotObservations.py which can be downloaded from my Github Repo.

Copy this program to the EC2 instance that has Scapy Installed.

wget https://raw.githubusercontent.com/Resistor52/hitc-benchmark-flow-logs/hitc-ep18/plotObservations.py



Run the program as using:

python3 ./plotObservations.py



The program will read logs-insights-results.csv and will generate the four plots shown below.


PLOT 1 - Packets 1000 to 2000


PLOT 2 - Packets 1020 to 1200


PLOT 3 - The First 10,000 Packets


Delta - Seconds Between Missing Data

I saved the charts generated from data captured in the exact same manner 24 hours prior to this set, and those are shown below for comparison purposes.


PLOT 1b - Packets 1000 to 2000 (Previous Run)


PLOT 3b - The First 10,000 Packets (Previous Run)

Observations

Ingest Delay - We can see the ingest delay varies from a little over 60 seconds to almost 11 minutes every 10 minutes. Note that when we configured the flow logs we used a setting of max-aggregation-interval 600. This test should be repeated using a max-aggregation-interval 60 to measure the impact on the Ingest Delay.

Timestamp Error Seconds - The TimeStampDelaySec is calculated by subtracting the SentTimeEpoch from TimeStampEpoch. Chronologically, the SentTimeEpoch should occur first, as that is the timestamp of when Scapy sent the packet. Similarly, the TimeStampEpoch is when the VPC Flow Logs system detects the packet on the virtual network. I expected the error to be within milliseconds, but not almost a whole minute off! I am not sure how to explain this. At first, I thought that it might be a math error on my part. But we can take a look at a single packet as it was:


  Recorded in the flow log (logs-insights-results.csv),
  Sent by Scapy (test3A.csv), and
  Recorded by TCPdump at the Target System (tcpdump.pcap)


We can use grep -E ',-58,|srcPort' logs-insights-results.csv | head -n6 | cut -d"," -f5,6-9 to generate the following table:


  
    
      TimeStampEpoch
      SentTimeEpoch
      SentTime
      TimeStampDelaySec
      srcPort
    
  
  
    
      1646334040
      1646334098
      2022-03-03 19:01:38.000
      -58
      4242
    
    
      1646334099
      1646334157
      2022-03-03 19:02:37.000
      -58
      4301
    
    
      1646334160
      1646334218
      2022-03-03 19:03:38.000
      -58
      4362
    
    
      1646334219
      1646334277
      2022-03-03 19:04:37.000
      -58
      4421
    
    
      1646334280
      1646334338
      2022-03-03 19:05:38.000
      -58
      4482
    
  


Selected packets from logs-insights-results.csv

Now that we have identified the first five packets with the highest error (-58), we can look for those packets in the log generated by Scapy using grep -E '"4242"|"4301"|"4362"|"4421"|"4482"' test3A.csv


  
    
      Epoch_Time
      Date_Time
      Source_Port
    
  
  
    
      1646334098
      2022-03-03 19:01:38 UTC+0000
      4242
    
    
      1646334157
      2022-03-03 19:02:37 UTC+0000
      4301
    
    
      1646334218
      2022-03-03 19:03:38 UTC+0000
      4362
    
    
      1646334277
      2022-03-03 19:04:37 UTC+0000
      4421
    
    
      1646334338
      2022-03-03 19:05:38 UTC+0000
      4482
    
  


Selected packets from test3A.csv

We can see that the SentTimeEpoch as calculated by my CloudWatch query logic is performed correctly as it matches the Epoch_Time as logged by Scapy.

For completeness, let’s look at TCP dump:

tcpdump -tt -nn -r tcpdump.pcap 'src port 4242 or src port 4301 or src port 4362 or src port 4421 or src port 4482' | cut -d":" -f1


produces:

1646334098.569321 IP 34.201.148.40.4242 &gt; 172.31.48.150.7777
1646334157.873827 IP 34.201.148.40.4301 &gt; 172.31.48.150.7777
1646334218.182541 IP 34.201.148.40.4362 &gt; 172.31.48.150.7777
1646334277.489424 IP 34.201.148.40.4421 &gt; 172.31.48.150.7777
1646334338.805517 IP 34.201.148.40.4482 &gt; 172.31.48.150.7777


As expected, we can see that the epoch time recorded by TCPdump matches the other observations in the tables above.

Missing Data - Looking at the Missing Data plots in the figures above initially raised some concerns. Is the VPC dropping packets? To investigate what is going on, I tweaked plotObservations.py to generate the “Delta” plot to show the periodicity of the missing data as well as to output the srcPorts for which data is missing. That output is shown below:

The following srcPorts are missing:
[4327, 4506, 4694, 4890, 5082, 5276, 5471, 5666, 5855,
6049, 6245, 6439, 6632, 6824, 7018, 7211, 7407, 7600,
7791, 7987, 8174, 8365, 8555, 8750, 8942, 9135, 9330,
9524, 9716, 9911, 10102, 10293, 10485, 10680, 10866,
11058, 11253, 11427, 11620, 11811, 12005, 12198, 12386,
12583, 12778, 12972, 13166, 13361, 13555, 13750, 13945,
14138]


I also had plotObservations.py write the ports to a file, missingSrcPorts.csv so that we can grep for them:

grep -f missingSrcPorts.csv test3A.csv


Hmm, none of the missing srcPorts were logged by Scapy. The good news is that the VPC network did not drop any packets because all sent packets have been accounted for. The bad news is that there is bug in generate-test-traffic.py that occurs about every 195 seconds. I suspect that it is an issue with my generateTraffic function, most likely due to inaccuracies with time.sleep() compounded by variations in the amount of time it takes Scapy to write the packet out to the network.

def generateTraffic(stop_time_epoch):
    datetime_TZ = datetime.now(tz_TZ)
    seconds = int(time.time())             # Current Epoch Time
    modulo = seconds % (2 ** 16)           # Calculate the SrcPort
    datetime_time=datetime_TZ.strftime("%Y-%m-%d %H:%M:%S %Z%z")
    print(f"Epoch Time: {seconds}   Time: {datetime_time}   srcPort: {modulo}   {RemainingTimeString(stop_time_epoch)}      ", end = "\r")
    write_str='"'+str(seconds)+'","'+datetime_time+'","'+str(modulo)+'"\n'
    f.write(write_str)
    tcp=TCP(sport=modulo,dport=7777,flags="S",options=[('Timestamp',(0,0))])
    pay=logfile+" - "+str(modulo)
    # Trap the output of scapy send so it does not print to console
    state = sys.stdout
    sys.stdout = open('/dev/null', 'w')
    send(ip/tcp/pay)                        # Scapy
    sys.stdout.close()
    sys.stdout = state
    # Wait one second between packets
    time.sleep(1)
    return


OBJECTIVE 4 - Contemplate the implications of our findings

Tests 1 and 2 demonstrate that we need to budget ample time ( at least 16 minutes) to allow AWS to provision the flow logs. We observed that flow logs sent to CloudWatch Logs provision faster than logs sent to a S3 bucket.

Test 3 demonstrated that with a max-aggregation-interval set to “600,” it is possible to see an ingestion delay of up to 10.42 minutes (cat logs-insights-results.csv | cut -d"," -f4 | sort -u -n | tail -n1) and that it is a function of how long ago the last aggregation occurred. The minimum ingestion delay was 1.26 minutes (cat logs-insights-results.csv | cut -d"," -f4 | sort -u -n | head -n2). Lots of bad stuff could be detected late if you were expecting to use flow logs for real-time detection. I think that AWS would tell you that is not the use case that they were designed for. Instead use something like a Web Application Firewall (WAF).

We observed an unaccounted-for error in the ‘@timestamp’ field generated by Cloud Watch Logs Insights of +1 to -60 seconds during our test and +60 to -58 during a previous test. This demonstrates the value of benchmarking critical security systems.

In this experiment, we demonstrated a novel technique for timestamping packets that will be reduced to a flow log by using the source port field. We showed how to verify the accuracy of the technique by comparing the epoch time calculated from the source port with the time logged by TCPdump.

We also demonstrated how to identify any missing packets using this technique and in the process identified a minor bug in generate-test-traffic.py that caused a packet not to be sent about every 195 seconds. This demonstrates (again) the importance of understanding the limitations of the security tools that we use. At some point, I will circle back and fix this bug.

Remember that the limited amount of testing that was performed is not sufficient to draw too many conclusions. Tests 1 and 2 should be repeated multiple times at different times of day, over the course of several days, in different regions to get more precise statistics. Test 3 should be repeated multiple times and should consider the impact of changing the max-aggregation-interval setting. The more the testing process is automated the lower the burden of repeating each test. But, I think that I have taken this little experiment far enough for now.

Wrap Up

Wow, we covered quite a bit in this Episode:

  We made extensive use of the AWS CLI
  We used a Bash “while loop” to time AWS CLI calls
  We crafted packets using Scapy and read those packets using TCPdump.
  We also created a fairly sophisticated CloudWatch Logs Insights query, and
  Learned how we can use Python to visualize data.


To learn more about TCPdump, Scapy, and packet crafting check out SANS SEC503: Intrusion Analysis In-Depth. To learn more about using Python to create security tools like generate-test-traffic.py consider SANS SEC573: Automating Information Security with Python.

Closing Comments

If you have thoughts or comments on today’s episode, feel free to chime in on the comments for this YouTube video.

If you appreciate this video and want to see more like it, be sure to give it a “thumbs up.”

Stay tuned for another installment of “Head in the Clouds” as announcements of new episodes are made on the SANS Cloud Security Twitter feed.

Meanwhile, be sure to check out the other great videos on the SANS Cloud Security YouTube Channel.

Take care.
">
<meta name="keywords" content="">


<!-- Twitter Cards -->
<meta name="twitter:title" content="Episode 18 - Benchmarking AWS Flow Logs">
<meta name="twitter:description" content="18 - Benchmarking AWS Flow Logs



Welcome to Episode 18 of the “Head in the Clouds” Video Series. I am Ken Hartman, a SANS Certified Instructor and content creator for the SANS Cloud Security Curriculum.

Today’s episode is titled: “Benchmarking AWS Flow Logs”

In many of the of the courses in the SANS Cloud Curriculum, we will have one or more labs that involves flow logs. However, a portion of the lab time is spent waiting for the flow logs to get provisioned. It’s kind of like watching and waiting for water to boil. Then, once the flow logs are provisioned, it takes time for the measured traffic to show up. Therefore, I thought it would be very interesting to determine what one should expect via experimentation and observation. Along the way, I made some very interesting discoveries, and I cannot wait to show them to you.

Objectives

Anytime we use a tool, technique, or service, we should be aware of its limitations and possible pitfalls. As such, here are the objectives for today:


  Experimentally determine the typical time to provision flow logs to an S3 bucket.
  Experimentally determine the typical time to provision flow logs to a CloudWatch logs group.
  Experimentally determine the typical delay for ongoing traffic to be captured in the flow log record.
  Contemplate the implications of our findings.


Setup

For this episode, we will assume that you have an AWS EC2 virtual machine running in a VPC that has a “Name” tag of “hitc-vpc.” The t2.micro type that is in the free tier will do just fine. I recommend that you use the Ubuntu 20.04 AMI for your region, as this will allow me to provide you with the exact commands to install some additional software later in this episode.

I used Terraform to deploy my environment as described in the HitC Episode titled Episode 10 - Demonstration of Terraform Modules; Deploy a VM in AWS, Azure and GCP at once as I had originally planned to benchmark all three cloud service providers. However, as I got deeper and deeper into my research into AWS flow logs, I decided to stay focused on just AWS and may circle back to Azure and GCP flow logs in one or more future episodes.

In this episode, we will be making extensive use of the AWS command line interface. This has two benefits:

  It will make it very easy for you to reproduce my work, and
  It will allow us to measure the time that it takes for things to happen.


The machine on which you are running the CLI commands will need jq in addition to the AWS command line interface.

OBJECTIVE 1: Experimentally determine the typical time to provision flow logs to an S3 bucket

To start with, we need to set some variables that will be used by our commands:

UNIQ_ID=$RANDOM; echo $UNIQ_ID
ACCOUNT=$(aws sts get-caller-identity | jq -r .Account); echo $ACCOUNT
YYYY=$(date -Idate | cut -d"-" -f1); echo $YYYY
MM=$(date -Idate | cut -d"-" -f2); echo $MM
DD=$(date -Idate | cut -d"-" -f3); echo $DD
REGION=$(aws ec2 describe-availability-zones --output text --query 'AvailabilityZones[0].[RegionName]'); echo $REGION



We generate a unique ID to ensure that the S3 bucket that we create is unique. Linux has a variable called $RANDOM that we can use for this purpose.

The YYYY, MM, and DD are captured so we can look into the bucket for our data.

Lastly, we use a trick from StackOverflow.com to get the region, as yours may be different from mine.

Next, we will create a variety of “helper commands.”

Get the VPC ID

To get the VPC ID of the VPC that is named “hitc-vpc” we can use:

VPC_ID=$(aws ec2 describe-vpcs --filters Name=tag:Name,Values=hitc-vpc --query "Vpcs[0].VpcId" --output text); echo $VPC_ID



Create the Bucket for the Flow Logs

Next, we need to create an S3 bucket to receive our flow logs:

aws s3 mb s3://hitc-benchmark-flowlogs-$UNIQ_ID



Set up the Flow Logs for the VPC and Send to S3

Our next task is to make sure we have the command to set the flow logs for the VPC and send them to S3:

aws ec2 create-flow-logs \
    --resource-type VPC \
    --resource-ids $VPC_ID \
    --traffic-type ALL \
    --log-destination-type s3 \
    --log-destination arn:aws:s3:::hitc-benchmark-flowlogs-$UNIQ_ID \
    --max-aggregation-interval 600 \
    --tag-specifications 'ResourceType=vpc-flow-log,Tags=[{Key=Name,Value=hitc-flow-log}]'



Describe the Flow Logs for the VPC

Having done that, we may find it helpful to be able to describe the flow logs for the VPC in question:

aws ec2 describe-flow-logs --filter "Name=resource-id,Values=$VPC_ID"



Delete the Flow Logs for the VPC

Next, we need to be able to delete the flow logs that we just created. To do this we need to know the Flow Log ID.

FLOW_LOG_ID=$(aws ec2 describe-flow-logs --filter "Name=resource-id,Values=$VPC_ID" --query 'FlowLogs[0].FlowLogId' --output text)
echo $FLOW_LOG_ID
aws ec2 delete-flow-logs --flow-log-ids $FLOW_LOG_ID



List the contents of the Bucket

To verify the presence of a flow log, we will need to be able to drill deep into the S3 bucket. This is facilitated by the variables we set earlier.

aws s3 ls s3://hitc-benchmark-flowlogs-$UNIQ_ID/AWSLogs/$ACCOUNT/vpcflowlogs/$REGION/$YYYY/$MM/$DD/



Empty the Bucket

We also need the ability to empty the bucket between successive iterations of our test:

aws s3 rm s3://hitc-benchmark-flowlogs-$UNIQ_ID --recursive



Remove the Bucket

It is also good idea to remove the bucket when we are done with our testing:

aws s3 rb s3://hitc-benchmark-flowlogs-$UNIQ_ID


Generate Flow Log Traffic

Now that we’ve got our helper scripts figured out and tested, we need to generate some traffic to get captured by our flow log. SSH into your easy to instance and run the following command:

while true ; do wget --timeout=10 http://1.1.1.1:81; sleep 20; done



Note that I selected CloudFlare as the target as they are big enough that this would not be considered abusive and by sending the traffic to port 81, the retries will keep working until exhausted after 20 attempts and will then start over after 20 seconds. Anything that repetitively generates traffic will work here.

TEST 1 - Time the provisioning of the flow Log until first flow log record in S3

With that done, we can now measure the time between the provisioning of the flow log until first flow log record in S3. Every 20 seconds we will check the S3 bucket to see if the flow log showed up yet. Our “TEST” checks to see if the length of response of the aws s3 ls command is zero. Once there is a flow log record in the bucket, the infinite loop will exit and we subtract the start time from the end time. Working in epoch time, makes this easy.

NOTE: This assumes that the S3 bucket exists, but is empty

# Set up the Flow Logs for the VPC
aws ec2 create-flow-logs \
    --resource-type VPC \
    --resource-ids $VPC_ID \
    --traffic-type ALL \
    --log-destination-type s3 \
    --log-destination arn:aws:s3:::hitc-benchmark-flowlogs-$UNIQ_ID \
    --max-aggregation-interval 600 \
    --tag-specifications 'ResourceType=vpc-flow-log,Tags=[{Key=Name,Value=hitc-flow-log}]'
# Watch for initial flow logs
printf "Waiting for the flow logs to apear in S3 bucket "
START_TIMER=$(date +%s)
TEST=""
while [ -z "$TEST" ] ; do
  printf "."
  sleep 20
  TEST=$(aws s3 ls s3://hitc-benchmark-flowlogs-$UNIQ_ID/AWSLogs/$ACCOUNT/vpcflowlogs/$REGION/$YYYY/$MM/$DD/)
done
STOP_TIMER=$(date +%s)
echo "DONE"
echo $((STOP_TIMER-START_TIMER))" seconds have elapsed"



While this script executes, it will print a dot to stdout every 20 seconds. When it is done, the output will look like this:

772 seconds have elapsed


Ok, 772 seconds is 12.87 minutes. We now have our TEST 1 measurement. Note that this can be off by 20 seconds due to the sleep timer, which can be removed.

Tear down TEST 1

To repeat the test, we need to delete the flow log and empty the S3 bucket, but leave the S3 bucket in place.

# Delete the Flow Logs for the VPC
FLOW_LOG_ID=$(aws ec2 describe-flow-logs --filter "Name=resource-id,Values=$VPC_ID" --query 'FlowLogs[0].FlowLogId' --output text)
echo $FLOW_LOG_ID
aws ec2 delete-flow-logs --flow-log-ids $FLOW_LOG_ID
# Empty the Bucket
aws s3 rm s3://hitc-benchmark-flowlogs-$UNIQ_ID --recursive



TEST 1 Summary

This table shows the results of repeating the test 3 more times:


  
    
      Script Results
      Minutes
    
  
  
    
      772 seconds have elapsed
      12.86 Minutes
    
    
      417 seconds have elapsed
      6.97 Minutes
    
    
      813 seconds have elapsed
      13.55 Minutes
    
    
      960 seconds have elapsed
      16.00 Minutes
    
  


Wow, it can take 16 minutes to get your first flow logs!

So, what is up with this? Well, the catch phrase that the cloud service providers use is eventual consistency. We know that the big guys use queuing and focus intently on optimization, so I am guessing that it is for those reasons that there is a provisioning delay.

Final Cleanup TEST 1

Now that we have obtained our results, make sure that you delete the flow log and empty the bucket again, using the last set of commands and then remove the bucket, as we no longer need it.

# Remove the Bucket
aws s3 rb s3://hitc-benchmark-flowlogs-$UNIQ_ID



OBJECTIVE 2: Experimentally determine the typical time to provision flow logs to a Cloud Watch logs group

Now, let’s take a look at Flow Logs sent to Cloud Watch Logs.

Create the CloudWatch Logs Group

First we need to create the CloudWatch Logs Group:

aws logs create-log-group --log-group-name hitc-flow-logs



Delete the Log Group

And, here is the corresponding command to delete the log group:

aws logs delete-log-group --log-group-name hitc-flow-logs



Create the CloudWatch Logs Role

Next, we need create the CloudWatch Logs Role. To do this we will create the Trust Policy JSON file, the Permissions Policy JSON file, and then attach them to a new role.

Create the Trust Policy JSON file. I like to use the “heredoc” technique, so that everything can be scripted:

cat &lt;&lt; EOF &gt; trust-policy.json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "vpc-flow-logs.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
EOF



Create the Permissions Policy JSON file:

cat &lt;&lt; EOF &gt; cw-flow-logs-policy.json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "logs:CreateLogGroup",
        "logs:CreateLogStream",
        "logs:PutLogEvents",
        "logs:DescribeLogGroups",
        "logs:DescribeLogStreams"
      ],
      "Resource": "*"
    }
  ]
}   
EOF



Next, run the following commands to create the policies and the role:

aws iam create-policy --policy-name hitc-cw-flow-logs --policy-document file://cw-flow-logs-policy.json
aws iam create-role --role-name hitc-cw-flow-logs --assume-role-policy-document file://trust-policy.json
aws iam attach-role-policy --policy-arn "arn:aws:iam::$ACCOUNT:policy/hitc-cw-flow-logs" --role-name hitc-cw-flow-logs
aws iam list-attached-role-policies --role-name hitc-cw-flow-logs



TEST 2 - Time the provisioning of the flow Log until first flow log record in CloudWatch

With our setup for Test 2 in place, we can now conduct it. This “TEST” checks to see if the length of response of the aws logs describe-log-streams command is zero. Once there is a flow log record in CW logs, the infinite loop will exit and we subtract the start time from the end time.

NOTE: This assumes that the CloudWatch Log Group exists, but is empty.

# Set up the Flow Logs for the VPC
aws ec2 create-flow-logs \
    --resource-type VPC \
    --resource-ids $VPC_ID \
    --traffic-type ALL \
    --log-destination-type cloud-watch-logs \
    --log-group-name hitc-flow-logs \
    --max-aggregation-interval 600 \
    --tag-specifications 'ResourceType=vpc-flow-log,Tags=[{Key=Name,Value=hitc-flow-log}]' \
    --deliver-logs-permission-arn 'arn:aws:iam::690634326977:role/hitc-cw-flow-logs'
# Watch for initial flow logs
printf "Waiting for the flow logs to appear in CW Logs Group "
START_TIMER=$(date +%s)
TEST=""
while [ -z "$TEST" ] ; do
  printf "."
  sleep 20
  TEST=$(aws logs describe-log-streams --log-group-name hitc-flow-logs --output text)
done
STOP_TIMER=$(date +%s)
echo "DONE"
echo $((STOP_TIMER-START_TIMER))" seconds have elapsed"



Tear down TEST 2

To re-run the test, delete the flow log and reset the CW Logs Group.

# Delete the Flow Logs for the VPC
FLOW_LOG_ID=$(aws ec2 describe-flow-logs --filter "Name=resource-id,Values=$VPC_ID" --query 'FlowLogs[0].FlowLogId' --output text)
echo $FLOW_LOG_ID
aws ec2 delete-flow-logs --flow-log-ids $FLOW_LOG_ID
# Delete and Recreate the CW Logs Group
aws logs delete-log-group --log-group-name hitc-flow-logs
aws logs create-log-group --log-group-name hitc-flow-logs



Final Cleanup TEST 2

And when we are all done, we can run the following commands to delete the role and associated policy. But hold off for now as we will need this role for Test 3.

# CLI commands to tear down the role and IAM policy
aws iam detach-role-policy --role-name hitc-cw-flow-logs --policy-arn "arn:aws:iam::$ACCOUNT:policy/hitc-cw-flow-logs"
aws iam delete-role --role-name hitc-cw-flow-logs
aws iam delete-policy --policy-arn "arn:aws:iam::$ACCOUNT:policy/hitc-cw-flow-logs"



TEST 2 Summary


  
    
      Script Results
      Minutes
    
  
  
    
      21 seconds have elapsed
      0.35 Minutes
    
    
      541 seconds have elapsed
      9.02 Minutes
    
    
      522 seconds have elapsed
      8.70 Minutes
    
    
      375 seconds have elapsed
      6.25 Minutes
    
  


This is much better, but it can still take up to 10 minutes and is somewhat variable.

OBJECTIVE 3: Experimentally determine the typical delay for ongoing traffic to be captured in the flow log record.

Now, onto what is probably the most interesting and has the biggest implications for security monitoring. We want to know how long will it take for traffic that occurs in my VPC to show up as recorded in my flow logs? In other words, “I want to know how much lag there is in any detective control that I implement (that is based on flow logs).”

The security implications of this are fairly significant.

The Problem Statement

Wouldn’t it be great to time stamp a packet, send it across the virtual network of our VPC, look at the packet in CloudWatch Logs, and determine how much delay there was between the time the packet was sent, and when it was recorded?

That would be fantastic. The only problem is that a flow log contains only metadata, and not any payload. Here is a screenshot of what we have to work with:



As it turns out, about the only thing that we have to fiddle with is the Source port and the Destination port. So, I will use the source port to encode the timestamp and send it to an unchanging port (TCP port 7777) at a target IP address outside of my VPC.

Will this work?

Let’s do some math here. We know that there are 65536 (2 to the power of 16) possible TCP ports.

If we send one packet per second from a different port, we could do that for 1092 Minutes:

&gt;&gt;&gt; (2 ** 16) / 60
1092.2666666666667


Or converting that to hours:

&gt;&gt;&gt; (2 ** 16) / 60 / 60
18.204444444444444


We get 18 hours. Hence, we can send traffic at a rate of one packet per second from a unique source port for over 18 hours before using up all possible ports!

Converting back and forth from the source port to the epoch time

To calculate which port to use, we will take the current epoch time and divide it by 65536 and then take the integer portion of the remainder and use that for the source port. The math function that returns the remainder is called “modulo” and is represented as a % in python

source port = int(current_epoch_time % 65536)


To convert back to the epoch time from the source port, take the current epoch time and divide it by 65536 and then keep the quotient portion. Divide the source port by 65536 and take that result and add it to the quotient. Multiply the new total by 65536 and the new result is the epoch time of when the data was sent.

epoch_timestamp = int((modulo / 65536.0 + int(current_epoch_time / 65536)) * 65536)


This conversion assumes that the current epoch time is within the 18 hour window represented by the source port value.

Introducing Scapy

To generate a packet per second with an ever increasing source port that represents the time that the packet was sent, I used scapy. Scapy is written as a python module and is ideal for programmatically crafting packets.

As we will be running Scapy on our EC2 instance to send traffic out of our VPC, we need to install Scapy, its dependencies, and the additional modules my python program will use:

sudo apt update
sudo apt install -y python3-pip tcpdump  python3-matplotlib
sudo pip3 install pytz
sudo pip install --pre scapy[complete]


Note that using sudo with pip to install software is generally a bad idea. See this link for a detailed discussion of the security issues.

However, in this case, Scapy needs to run as root to be able to send packets, and we will destroy our EC2 instance at the end of our experimentation.

To launch Scapy, just type sudo scapy at the Ubuntu command line as shown below:

ubuntu@ip-10-0-1-112:~$ sudo scapy
INFO: PyX dependencies are not installed ! Please install TexLive or MikTeX.

                     aSPY//YASa
             apyyyyCY//////////YCa       |
            sY//////YSpcs  scpCY//Pp     | Welcome to Scapy
 ayp ayyyyyyySCP//Pp           syY//C    | Version 2.4.5
 AYAsAYYYYYYYY///Ps              cY//S   |
         pCCCCY//p          cSSps y//Y   | https://github.com/secdev/scapy
         SPPPP///a          pP///AC//Y   |
              A//A            cyP////C   | Have fun!
              p///Ac            sC///a   |
              P////YCpc           A//A   | We are in France, we say Skappee.
       scccccp///pSP///p          p//Y   | OK? Merci.
      sY/////////y  caa           S//P   |             -- Sebastien Chabal
       cayCyayP//Ya              pY/Ya   |
        sY/PsY////YCc          aC//Yp
         sc  sccaCY//PCypaapyCP//YSs
                  spCPY//////YPSps
                       ccaacs
                                       using IPython 8.1.1
&gt;&gt;&gt;


I set up a target VM in another VPC with the security group set to allow inbound TCP port 7777 from the system with Scapy installed. As a test, I am running TCPdump on the target machine, using the following command:

sudo tcpdump -i eth0 'port 7777'



Next, use Scapy to send the target a packet. Enter the following commands at the Scapy IPython prompt, but substitute my target IP address (35.168.8.211) for the one you are using:

ip=IP(dst="35.168.8.211")
tcp=TCP(sport=1111,dport=7777,flags="S",options=[('Timestamp',(0,0))])
pay="This is Test3"
test3=ip/tcp/pay
send(test3)


The result should look like:

.
Sent 1 packets.


NOTE: If you get a “Operation not permitted” error, you forgot to run Scapy as root

Exit back to the Ubuntu command line using exit()

Looking over at the target system, we can see that TCPdump generated the following two records:

22:25:43.461121 IP ec2-34-201-148-40.compute-1.amazonaws.com.1111 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13
22:25:43.461156 IP ip-172-31-48-150.ec2.internal.7777 &gt; ec2-34-201-148-40.compute-1.amazonaws.com.1111: Flags [R.], seq 0, ack 14, win 0, length 0


We Sent a TCP “Syn” packet and a “Reset” was sent back because there is not a service listening on port 7777.

Putting the theory to work

Ok, so now that we know the Scapy commands to craft a packet. We can write a program to determine the Source Port to use, to craft the packet and sent it. We also can prove that the packet was received by the target system and see the timestamp printed to standard out by TCPdump.

The program is called generate-test-traffic.py and can be downloaded from my Github Repo

Copy this program to the EC2 instance that has Scapy Installed.

wget https://raw.githubusercontent.com/Resistor52/hitc-benchmark-flow-logs/hitc-ep18/generate-test-traffic.py



Run the program as root using

sudo python3 ./generate-test-traffic.py



Once the program runs, it will ask four questions as shown below and then start to send packets at a rate of one per second.

ubuntu@ip-10-0-1-112:~$ sudo python3 ./generate-test-traffic.py
Enter the name for the CSV log file (without the csv extension): test3A
Enter the desired target IP aggress in the format "111.111.111.111": 35.168.8.211
Enter the desired start time in the format "HH:MM" for local time zone: 19:00
Enter the desired end time in the format "HH:MM" for local time zone: 22:00
Traffic generation will start at: 19:00
Traffic generation will continue until: 22:00
**************** Start of Traffic **************************
.Epoch Time: 1646344800   Time: 2022-03-03 22:00:00 UTC+0000   srcPort: 14944   Remaining Time: 0 Seconds
***************** End of Traffic ***************************


In the example above, I am calling the log file test3A and provided 35.168.8.211 as the target IP. (Again, be sure to use the IP address of the target that you set up and has TCPdump listening.) I started the program at 19:00 hours UTC, simply because that was on the hour, and had it run for 3 hours, ending at 22:00. The program uses UTC because that is the time zone that AWS uses for all its cloud services, including EC2.

Now, while we are waiting for the traffic generation to start, rerun TCPdump on the target system, this time capturing the packets to a file:

sudo tcpdump -i eth0 'port 7777' -w tcpdump.pcap


Initial Look at the Data

The generate-test-traffic.py script created a log, which I named test3A.csv. Note that the script adds the “csv” extension to the filename. The top 10 rows of the file are shown below:

"Epoch_Time","Date_Time","Source_Port"
"1646334000","2022-03-03 19:00:00 UTC+0000","4144"
"1646334001","2022-03-03 19:00:01 UTC+0000","4145"
"1646334002","2022-03-03 19:00:02 UTC+0000","4146"
"1646334003","2022-03-03 19:00:03 UTC+0000","4147"
"1646334004","2022-03-03 19:00:04 UTC+0000","4148"
"1646334005","2022-03-03 19:00:05 UTC+0000","4149"
"1646334006","2022-03-03 19:00:06 UTC+0000","4150"
"1646334007","2022-03-03 19:00:07 UTC+0000","4151"
"1646334008","2022-03-03 19:00:08 UTC+0000","4152"


We can look at the first 10 packets received by the Target system using the following tcpdump command:

tcpdump -r tcpdump.pcap -nn -c10 'dst port 7777'


And the output is:

19:00:00.013928 IP ec2-34-201-148-40.compute-1.amazonaws.com.4144 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13
19:00:01.018892 IP ec2-34-201-148-40.compute-1.amazonaws.com.4145 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13
19:00:02.023309 IP ec2-34-201-148-40.compute-1.amazonaws.com.4146 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13
19:00:03.027319 IP ec2-34-201-148-40.compute-1.amazonaws.com.4147 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13
19:00:04.032328 IP ec2-34-201-148-40.compute-1.amazonaws.com.4148 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13
19:00:05.050076 IP ec2-34-201-148-40.compute-1.amazonaws.com.4149 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13
19:00:06.061365 IP ec2-34-201-148-40.compute-1.amazonaws.com.4150 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13
19:00:07.066456 IP ec2-34-201-148-40.compute-1.amazonaws.com.4151 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13
19:00:08.071536 IP ec2-34-201-148-40.compute-1.amazonaws.com.4152 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13
19:00:09.077085 IP ec2-34-201-148-40.compute-1.amazonaws.com.4153 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13


We can extract the two critical data elements from packet capture using the following command:

tcpdump -r tcpdump.pcap -nn -c10 'dst port 7777' | cut -d" " -f1,3 | cut -d"." -f1,6 | tr "." " "


And this command will output the timestamp and the source port of the packet received by the target system:

19:00:00 4144
19:00:01 4145
19:00:02 4146
19:00:03 4147
19:00:04 4148
19:00:05 4149
19:00:06 4150
19:00:07 4151
19:00:08 4152
19:00:09 4153


Great, everything correlates! Now lets write the whole packet to a file:

tcpdump -r tcpdump.pcap -nn 'dst port 7777' | cut -d" " -f1,3 | cut -d"." -f1,6 | tr "." " " &gt; test3A.log


We will save test3A.log for later, as it will come in handy.

Looking in CloudWatch Logs

The CloudWatch Logs Web Console is a great tool to slice and dice log data. Paste in the following query:

fields @ingestionTime, @timestamp, ((toMillis(@ingestionTime) - toMillis(@timestamp))/1000) as IngestDelaySecs, ((toMillis(@ingestionTime) - toMillis(@timestamp))/1000/60)
as IngestDelayMins, toMillis(@timestamp)/1000 as TimeStampEpoch, (65536*floor(TimeStampEpoch/65536))+srcPort as SentTimeEpoch, fromMillis(SentTimeEpoch * 1000) as SentTime,
TimeStampEpoch-SentTimeEpoch as TimeStampDelaySec, srcPort
| sort srcPort asc
| limit 10000
| filter srcAddr = "10.0.1.112" and dstAddr = "35.168.8.211" and dstPort != "22" and srcPort != "22")


Be sure to set the time filter in the upper window to include the window in which the packets were sent.



Looking at the output, we see that AWS provides two fields @ingestionTime and @timestamp so we can calculate the delay the Ingestion Delay Seconds (IngestDelaySecs) by converting both time values to Epoch time and subtracting them.

But how accurate is the timestamp? Well, we can use the CloudWatch Logs Query Language to calculate a SentTimeEpoch based on the srcPort. We can also convert that to SentTime so it is human-readable. If we subtract the SentTimeEpoch from the  TimeStampEpoch we get the TimeStampDelaySec.

Disregard the first record as that was captured when I demonstrated how to use Scapy with IPython using an arbitrary source port. (I generated the traffic using the Python Script a few hours beforehand and that explains the chronology of this timestamp.) This record will be removed from the downloaded dataset.

Another thing, that you may have noticed in the screenshot, is that the first srcPort is 4184 which is 40 seconds after we sent the packet with srcPort 4144!

Very interesting. It looks like we have “negative delays.” or to phrase it more accurately, the @timestamp value attached by AWS has an error that fluctuates within a range.

Use the “Export Results” button to save the file as logs-insights-results.csv

Looking at the Data Graphically

Since we see that we have an Ingest Delay and a Timestamp Error, it would be good to look at this data graphically. Toward that end, I have create a python program called plotObservations.py which can be downloaded from my Github Repo.

Copy this program to the EC2 instance that has Scapy Installed.

wget https://raw.githubusercontent.com/Resistor52/hitc-benchmark-flow-logs/hitc-ep18/plotObservations.py



Run the program as using:

python3 ./plotObservations.py



The program will read logs-insights-results.csv and will generate the four plots shown below.


PLOT 1 - Packets 1000 to 2000


PLOT 2 - Packets 1020 to 1200


PLOT 3 - The First 10,000 Packets


Delta - Seconds Between Missing Data

I saved the charts generated from data captured in the exact same manner 24 hours prior to this set, and those are shown below for comparison purposes.


PLOT 1b - Packets 1000 to 2000 (Previous Run)


PLOT 3b - The First 10,000 Packets (Previous Run)

Observations

Ingest Delay - We can see the ingest delay varies from a little over 60 seconds to almost 11 minutes every 10 minutes. Note that when we configured the flow logs we used a setting of max-aggregation-interval 600. This test should be repeated using a max-aggregation-interval 60 to measure the impact on the Ingest Delay.

Timestamp Error Seconds - The TimeStampDelaySec is calculated by subtracting the SentTimeEpoch from TimeStampEpoch. Chronologically, the SentTimeEpoch should occur first, as that is the timestamp of when Scapy sent the packet. Similarly, the TimeStampEpoch is when the VPC Flow Logs system detects the packet on the virtual network. I expected the error to be within milliseconds, but not almost a whole minute off! I am not sure how to explain this. At first, I thought that it might be a math error on my part. But we can take a look at a single packet as it was:


  Recorded in the flow log (logs-insights-results.csv),
  Sent by Scapy (test3A.csv), and
  Recorded by TCPdump at the Target System (tcpdump.pcap)


We can use grep -E ',-58,|srcPort' logs-insights-results.csv | head -n6 | cut -d"," -f5,6-9 to generate the following table:


  
    
      TimeStampEpoch
      SentTimeEpoch
      SentTime
      TimeStampDelaySec
      srcPort
    
  
  
    
      1646334040
      1646334098
      2022-03-03 19:01:38.000
      -58
      4242
    
    
      1646334099
      1646334157
      2022-03-03 19:02:37.000
      -58
      4301
    
    
      1646334160
      1646334218
      2022-03-03 19:03:38.000
      -58
      4362
    
    
      1646334219
      1646334277
      2022-03-03 19:04:37.000
      -58
      4421
    
    
      1646334280
      1646334338
      2022-03-03 19:05:38.000
      -58
      4482
    
  


Selected packets from logs-insights-results.csv

Now that we have identified the first five packets with the highest error (-58), we can look for those packets in the log generated by Scapy using grep -E '"4242"|"4301"|"4362"|"4421"|"4482"' test3A.csv


  
    
      Epoch_Time
      Date_Time
      Source_Port
    
  
  
    
      1646334098
      2022-03-03 19:01:38 UTC+0000
      4242
    
    
      1646334157
      2022-03-03 19:02:37 UTC+0000
      4301
    
    
      1646334218
      2022-03-03 19:03:38 UTC+0000
      4362
    
    
      1646334277
      2022-03-03 19:04:37 UTC+0000
      4421
    
    
      1646334338
      2022-03-03 19:05:38 UTC+0000
      4482
    
  


Selected packets from test3A.csv

We can see that the SentTimeEpoch as calculated by my CloudWatch query logic is performed correctly as it matches the Epoch_Time as logged by Scapy.

For completeness, let’s look at TCP dump:

tcpdump -tt -nn -r tcpdump.pcap 'src port 4242 or src port 4301 or src port 4362 or src port 4421 or src port 4482' | cut -d":" -f1


produces:

1646334098.569321 IP 34.201.148.40.4242 &gt; 172.31.48.150.7777
1646334157.873827 IP 34.201.148.40.4301 &gt; 172.31.48.150.7777
1646334218.182541 IP 34.201.148.40.4362 &gt; 172.31.48.150.7777
1646334277.489424 IP 34.201.148.40.4421 &gt; 172.31.48.150.7777
1646334338.805517 IP 34.201.148.40.4482 &gt; 172.31.48.150.7777


As expected, we can see that the epoch time recorded by TCPdump matches the other observations in the tables above.

Missing Data - Looking at the Missing Data plots in the figures above initially raised some concerns. Is the VPC dropping packets? To investigate what is going on, I tweaked plotObservations.py to generate the “Delta” plot to show the periodicity of the missing data as well as to output the srcPorts for which data is missing. That output is shown below:

The following srcPorts are missing:
[4327, 4506, 4694, 4890, 5082, 5276, 5471, 5666, 5855,
6049, 6245, 6439, 6632, 6824, 7018, 7211, 7407, 7600,
7791, 7987, 8174, 8365, 8555, 8750, 8942, 9135, 9330,
9524, 9716, 9911, 10102, 10293, 10485, 10680, 10866,
11058, 11253, 11427, 11620, 11811, 12005, 12198, 12386,
12583, 12778, 12972, 13166, 13361, 13555, 13750, 13945,
14138]


I also had plotObservations.py write the ports to a file, missingSrcPorts.csv so that we can grep for them:

grep -f missingSrcPorts.csv test3A.csv


Hmm, none of the missing srcPorts were logged by Scapy. The good news is that the VPC network did not drop any packets because all sent packets have been accounted for. The bad news is that there is bug in generate-test-traffic.py that occurs about every 195 seconds. I suspect that it is an issue with my generateTraffic function, most likely due to inaccuracies with time.sleep() compounded by variations in the amount of time it takes Scapy to write the packet out to the network.

def generateTraffic(stop_time_epoch):
    datetime_TZ = datetime.now(tz_TZ)
    seconds = int(time.time())             # Current Epoch Time
    modulo = seconds % (2 ** 16)           # Calculate the SrcPort
    datetime_time=datetime_TZ.strftime("%Y-%m-%d %H:%M:%S %Z%z")
    print(f"Epoch Time: {seconds}   Time: {datetime_time}   srcPort: {modulo}   {RemainingTimeString(stop_time_epoch)}      ", end = "\r")
    write_str='"'+str(seconds)+'","'+datetime_time+'","'+str(modulo)+'"\n'
    f.write(write_str)
    tcp=TCP(sport=modulo,dport=7777,flags="S",options=[('Timestamp',(0,0))])
    pay=logfile+" - "+str(modulo)
    # Trap the output of scapy send so it does not print to console
    state = sys.stdout
    sys.stdout = open('/dev/null', 'w')
    send(ip/tcp/pay)                        # Scapy
    sys.stdout.close()
    sys.stdout = state
    # Wait one second between packets
    time.sleep(1)
    return


OBJECTIVE 4 - Contemplate the implications of our findings

Tests 1 and 2 demonstrate that we need to budget ample time ( at least 16 minutes) to allow AWS to provision the flow logs. We observed that flow logs sent to CloudWatch Logs provision faster than logs sent to a S3 bucket.

Test 3 demonstrated that with a max-aggregation-interval set to “600,” it is possible to see an ingestion delay of up to 10.42 minutes (cat logs-insights-results.csv | cut -d"," -f4 | sort -u -n | tail -n1) and that it is a function of how long ago the last aggregation occurred. The minimum ingestion delay was 1.26 minutes (cat logs-insights-results.csv | cut -d"," -f4 | sort -u -n | head -n2). Lots of bad stuff could be detected late if you were expecting to use flow logs for real-time detection. I think that AWS would tell you that is not the use case that they were designed for. Instead use something like a Web Application Firewall (WAF).

We observed an unaccounted-for error in the ‘@timestamp’ field generated by Cloud Watch Logs Insights of +1 to -60 seconds during our test and +60 to -58 during a previous test. This demonstrates the value of benchmarking critical security systems.

In this experiment, we demonstrated a novel technique for timestamping packets that will be reduced to a flow log by using the source port field. We showed how to verify the accuracy of the technique by comparing the epoch time calculated from the source port with the time logged by TCPdump.

We also demonstrated how to identify any missing packets using this technique and in the process identified a minor bug in generate-test-traffic.py that caused a packet not to be sent about every 195 seconds. This demonstrates (again) the importance of understanding the limitations of the security tools that we use. At some point, I will circle back and fix this bug.

Remember that the limited amount of testing that was performed is not sufficient to draw too many conclusions. Tests 1 and 2 should be repeated multiple times at different times of day, over the course of several days, in different regions to get more precise statistics. Test 3 should be repeated multiple times and should consider the impact of changing the max-aggregation-interval setting. The more the testing process is automated the lower the burden of repeating each test. But, I think that I have taken this little experiment far enough for now.

Wrap Up

Wow, we covered quite a bit in this Episode:

  We made extensive use of the AWS CLI
  We used a Bash “while loop” to time AWS CLI calls
  We crafted packets using Scapy and read those packets using TCPdump.
  We also created a fairly sophisticated CloudWatch Logs Insights query, and
  Learned how we can use Python to visualize data.


To learn more about TCPdump, Scapy, and packet crafting check out SANS SEC503: Intrusion Analysis In-Depth. To learn more about using Python to create security tools like generate-test-traffic.py consider SANS SEC573: Automating Information Security with Python.

Closing Comments

If you have thoughts or comments on today’s episode, feel free to chime in on the comments for this YouTube video.

If you appreciate this video and want to see more like it, be sure to give it a “thumbs up.”

Stay tuned for another installment of “Head in the Clouds” as announcements of new episodes are made on the SANS Cloud Security Twitter feed.

Meanwhile, be sure to check out the other great videos on the SANS Cloud Security YouTube Channel.

Take care.
">
<meta name="twitter:site" content="@kennethghartman">
<meta name="twitter:creator" content="@kennethghartman">

<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="/images/default-thumb.png">

<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Episode 18 - Benchmarking AWS Flow Logs">
<meta property="og:description" content="18 - Benchmarking AWS Flow Logs



Welcome to Episode 18 of the “Head in the Clouds” Video Series. I am Ken Hartman, a SANS Certified Instructor and content creator for the SANS Cloud Security Curriculum.

Today’s episode is titled: “Benchmarking AWS Flow Logs”

In many of the of the courses in the SANS Cloud Curriculum, we will have one or more labs that involves flow logs. However, a portion of the lab time is spent waiting for the flow logs to get provisioned. It’s kind of like watching and waiting for water to boil. Then, once the flow logs are provisioned, it takes time for the measured traffic to show up. Therefore, I thought it would be very interesting to determine what one should expect via experimentation and observation. Along the way, I made some very interesting discoveries, and I cannot wait to show them to you.

Objectives

Anytime we use a tool, technique, or service, we should be aware of its limitations and possible pitfalls. As such, here are the objectives for today:


  Experimentally determine the typical time to provision flow logs to an S3 bucket.
  Experimentally determine the typical time to provision flow logs to a CloudWatch logs group.
  Experimentally determine the typical delay for ongoing traffic to be captured in the flow log record.
  Contemplate the implications of our findings.


Setup

For this episode, we will assume that you have an AWS EC2 virtual machine running in a VPC that has a “Name” tag of “hitc-vpc.” The t2.micro type that is in the free tier will do just fine. I recommend that you use the Ubuntu 20.04 AMI for your region, as this will allow me to provide you with the exact commands to install some additional software later in this episode.

I used Terraform to deploy my environment as described in the HitC Episode titled Episode 10 - Demonstration of Terraform Modules; Deploy a VM in AWS, Azure and GCP at once as I had originally planned to benchmark all three cloud service providers. However, as I got deeper and deeper into my research into AWS flow logs, I decided to stay focused on just AWS and may circle back to Azure and GCP flow logs in one or more future episodes.

In this episode, we will be making extensive use of the AWS command line interface. This has two benefits:

  It will make it very easy for you to reproduce my work, and
  It will allow us to measure the time that it takes for things to happen.


The machine on which you are running the CLI commands will need jq in addition to the AWS command line interface.

OBJECTIVE 1: Experimentally determine the typical time to provision flow logs to an S3 bucket

To start with, we need to set some variables that will be used by our commands:

UNIQ_ID=$RANDOM; echo $UNIQ_ID
ACCOUNT=$(aws sts get-caller-identity | jq -r .Account); echo $ACCOUNT
YYYY=$(date -Idate | cut -d"-" -f1); echo $YYYY
MM=$(date -Idate | cut -d"-" -f2); echo $MM
DD=$(date -Idate | cut -d"-" -f3); echo $DD
REGION=$(aws ec2 describe-availability-zones --output text --query 'AvailabilityZones[0].[RegionName]'); echo $REGION



We generate a unique ID to ensure that the S3 bucket that we create is unique. Linux has a variable called $RANDOM that we can use for this purpose.

The YYYY, MM, and DD are captured so we can look into the bucket for our data.

Lastly, we use a trick from StackOverflow.com to get the region, as yours may be different from mine.

Next, we will create a variety of “helper commands.”

Get the VPC ID

To get the VPC ID of the VPC that is named “hitc-vpc” we can use:

VPC_ID=$(aws ec2 describe-vpcs --filters Name=tag:Name,Values=hitc-vpc --query "Vpcs[0].VpcId" --output text); echo $VPC_ID



Create the Bucket for the Flow Logs

Next, we need to create an S3 bucket to receive our flow logs:

aws s3 mb s3://hitc-benchmark-flowlogs-$UNIQ_ID



Set up the Flow Logs for the VPC and Send to S3

Our next task is to make sure we have the command to set the flow logs for the VPC and send them to S3:

aws ec2 create-flow-logs \
    --resource-type VPC \
    --resource-ids $VPC_ID \
    --traffic-type ALL \
    --log-destination-type s3 \
    --log-destination arn:aws:s3:::hitc-benchmark-flowlogs-$UNIQ_ID \
    --max-aggregation-interval 600 \
    --tag-specifications 'ResourceType=vpc-flow-log,Tags=[{Key=Name,Value=hitc-flow-log}]'



Describe the Flow Logs for the VPC

Having done that, we may find it helpful to be able to describe the flow logs for the VPC in question:

aws ec2 describe-flow-logs --filter "Name=resource-id,Values=$VPC_ID"



Delete the Flow Logs for the VPC

Next, we need to be able to delete the flow logs that we just created. To do this we need to know the Flow Log ID.

FLOW_LOG_ID=$(aws ec2 describe-flow-logs --filter "Name=resource-id,Values=$VPC_ID" --query 'FlowLogs[0].FlowLogId' --output text)
echo $FLOW_LOG_ID
aws ec2 delete-flow-logs --flow-log-ids $FLOW_LOG_ID



List the contents of the Bucket

To verify the presence of a flow log, we will need to be able to drill deep into the S3 bucket. This is facilitated by the variables we set earlier.

aws s3 ls s3://hitc-benchmark-flowlogs-$UNIQ_ID/AWSLogs/$ACCOUNT/vpcflowlogs/$REGION/$YYYY/$MM/$DD/



Empty the Bucket

We also need the ability to empty the bucket between successive iterations of our test:

aws s3 rm s3://hitc-benchmark-flowlogs-$UNIQ_ID --recursive



Remove the Bucket

It is also good idea to remove the bucket when we are done with our testing:

aws s3 rb s3://hitc-benchmark-flowlogs-$UNIQ_ID


Generate Flow Log Traffic

Now that we’ve got our helper scripts figured out and tested, we need to generate some traffic to get captured by our flow log. SSH into your easy to instance and run the following command:

while true ; do wget --timeout=10 http://1.1.1.1:81; sleep 20; done



Note that I selected CloudFlare as the target as they are big enough that this would not be considered abusive and by sending the traffic to port 81, the retries will keep working until exhausted after 20 attempts and will then start over after 20 seconds. Anything that repetitively generates traffic will work here.

TEST 1 - Time the provisioning of the flow Log until first flow log record in S3

With that done, we can now measure the time between the provisioning of the flow log until first flow log record in S3. Every 20 seconds we will check the S3 bucket to see if the flow log showed up yet. Our “TEST” checks to see if the length of response of the aws s3 ls command is zero. Once there is a flow log record in the bucket, the infinite loop will exit and we subtract the start time from the end time. Working in epoch time, makes this easy.

NOTE: This assumes that the S3 bucket exists, but is empty

# Set up the Flow Logs for the VPC
aws ec2 create-flow-logs \
    --resource-type VPC \
    --resource-ids $VPC_ID \
    --traffic-type ALL \
    --log-destination-type s3 \
    --log-destination arn:aws:s3:::hitc-benchmark-flowlogs-$UNIQ_ID \
    --max-aggregation-interval 600 \
    --tag-specifications 'ResourceType=vpc-flow-log,Tags=[{Key=Name,Value=hitc-flow-log}]'
# Watch for initial flow logs
printf "Waiting for the flow logs to apear in S3 bucket "
START_TIMER=$(date +%s)
TEST=""
while [ -z "$TEST" ] ; do
  printf "."
  sleep 20
  TEST=$(aws s3 ls s3://hitc-benchmark-flowlogs-$UNIQ_ID/AWSLogs/$ACCOUNT/vpcflowlogs/$REGION/$YYYY/$MM/$DD/)
done
STOP_TIMER=$(date +%s)
echo "DONE"
echo $((STOP_TIMER-START_TIMER))" seconds have elapsed"



While this script executes, it will print a dot to stdout every 20 seconds. When it is done, the output will look like this:

772 seconds have elapsed


Ok, 772 seconds is 12.87 minutes. We now have our TEST 1 measurement. Note that this can be off by 20 seconds due to the sleep timer, which can be removed.

Tear down TEST 1

To repeat the test, we need to delete the flow log and empty the S3 bucket, but leave the S3 bucket in place.

# Delete the Flow Logs for the VPC
FLOW_LOG_ID=$(aws ec2 describe-flow-logs --filter "Name=resource-id,Values=$VPC_ID" --query 'FlowLogs[0].FlowLogId' --output text)
echo $FLOW_LOG_ID
aws ec2 delete-flow-logs --flow-log-ids $FLOW_LOG_ID
# Empty the Bucket
aws s3 rm s3://hitc-benchmark-flowlogs-$UNIQ_ID --recursive



TEST 1 Summary

This table shows the results of repeating the test 3 more times:


  
    
      Script Results
      Minutes
    
  
  
    
      772 seconds have elapsed
      12.86 Minutes
    
    
      417 seconds have elapsed
      6.97 Minutes
    
    
      813 seconds have elapsed
      13.55 Minutes
    
    
      960 seconds have elapsed
      16.00 Minutes
    
  


Wow, it can take 16 minutes to get your first flow logs!

So, what is up with this? Well, the catch phrase that the cloud service providers use is eventual consistency. We know that the big guys use queuing and focus intently on optimization, so I am guessing that it is for those reasons that there is a provisioning delay.

Final Cleanup TEST 1

Now that we have obtained our results, make sure that you delete the flow log and empty the bucket again, using the last set of commands and then remove the bucket, as we no longer need it.

# Remove the Bucket
aws s3 rb s3://hitc-benchmark-flowlogs-$UNIQ_ID



OBJECTIVE 2: Experimentally determine the typical time to provision flow logs to a Cloud Watch logs group

Now, let’s take a look at Flow Logs sent to Cloud Watch Logs.

Create the CloudWatch Logs Group

First we need to create the CloudWatch Logs Group:

aws logs create-log-group --log-group-name hitc-flow-logs



Delete the Log Group

And, here is the corresponding command to delete the log group:

aws logs delete-log-group --log-group-name hitc-flow-logs



Create the CloudWatch Logs Role

Next, we need create the CloudWatch Logs Role. To do this we will create the Trust Policy JSON file, the Permissions Policy JSON file, and then attach them to a new role.

Create the Trust Policy JSON file. I like to use the “heredoc” technique, so that everything can be scripted:

cat &lt;&lt; EOF &gt; trust-policy.json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "vpc-flow-logs.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
EOF



Create the Permissions Policy JSON file:

cat &lt;&lt; EOF &gt; cw-flow-logs-policy.json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "logs:CreateLogGroup",
        "logs:CreateLogStream",
        "logs:PutLogEvents",
        "logs:DescribeLogGroups",
        "logs:DescribeLogStreams"
      ],
      "Resource": "*"
    }
  ]
}   
EOF



Next, run the following commands to create the policies and the role:

aws iam create-policy --policy-name hitc-cw-flow-logs --policy-document file://cw-flow-logs-policy.json
aws iam create-role --role-name hitc-cw-flow-logs --assume-role-policy-document file://trust-policy.json
aws iam attach-role-policy --policy-arn "arn:aws:iam::$ACCOUNT:policy/hitc-cw-flow-logs" --role-name hitc-cw-flow-logs
aws iam list-attached-role-policies --role-name hitc-cw-flow-logs



TEST 2 - Time the provisioning of the flow Log until first flow log record in CloudWatch

With our setup for Test 2 in place, we can now conduct it. This “TEST” checks to see if the length of response of the aws logs describe-log-streams command is zero. Once there is a flow log record in CW logs, the infinite loop will exit and we subtract the start time from the end time.

NOTE: This assumes that the CloudWatch Log Group exists, but is empty.

# Set up the Flow Logs for the VPC
aws ec2 create-flow-logs \
    --resource-type VPC \
    --resource-ids $VPC_ID \
    --traffic-type ALL \
    --log-destination-type cloud-watch-logs \
    --log-group-name hitc-flow-logs \
    --max-aggregation-interval 600 \
    --tag-specifications 'ResourceType=vpc-flow-log,Tags=[{Key=Name,Value=hitc-flow-log}]' \
    --deliver-logs-permission-arn 'arn:aws:iam::690634326977:role/hitc-cw-flow-logs'
# Watch for initial flow logs
printf "Waiting for the flow logs to appear in CW Logs Group "
START_TIMER=$(date +%s)
TEST=""
while [ -z "$TEST" ] ; do
  printf "."
  sleep 20
  TEST=$(aws logs describe-log-streams --log-group-name hitc-flow-logs --output text)
done
STOP_TIMER=$(date +%s)
echo "DONE"
echo $((STOP_TIMER-START_TIMER))" seconds have elapsed"



Tear down TEST 2

To re-run the test, delete the flow log and reset the CW Logs Group.

# Delete the Flow Logs for the VPC
FLOW_LOG_ID=$(aws ec2 describe-flow-logs --filter "Name=resource-id,Values=$VPC_ID" --query 'FlowLogs[0].FlowLogId' --output text)
echo $FLOW_LOG_ID
aws ec2 delete-flow-logs --flow-log-ids $FLOW_LOG_ID
# Delete and Recreate the CW Logs Group
aws logs delete-log-group --log-group-name hitc-flow-logs
aws logs create-log-group --log-group-name hitc-flow-logs



Final Cleanup TEST 2

And when we are all done, we can run the following commands to delete the role and associated policy. But hold off for now as we will need this role for Test 3.

# CLI commands to tear down the role and IAM policy
aws iam detach-role-policy --role-name hitc-cw-flow-logs --policy-arn "arn:aws:iam::$ACCOUNT:policy/hitc-cw-flow-logs"
aws iam delete-role --role-name hitc-cw-flow-logs
aws iam delete-policy --policy-arn "arn:aws:iam::$ACCOUNT:policy/hitc-cw-flow-logs"



TEST 2 Summary


  
    
      Script Results
      Minutes
    
  
  
    
      21 seconds have elapsed
      0.35 Minutes
    
    
      541 seconds have elapsed
      9.02 Minutes
    
    
      522 seconds have elapsed
      8.70 Minutes
    
    
      375 seconds have elapsed
      6.25 Minutes
    
  


This is much better, but it can still take up to 10 minutes and is somewhat variable.

OBJECTIVE 3: Experimentally determine the typical delay for ongoing traffic to be captured in the flow log record.

Now, onto what is probably the most interesting and has the biggest implications for security monitoring. We want to know how long will it take for traffic that occurs in my VPC to show up as recorded in my flow logs? In other words, “I want to know how much lag there is in any detective control that I implement (that is based on flow logs).”

The security implications of this are fairly significant.

The Problem Statement

Wouldn’t it be great to time stamp a packet, send it across the virtual network of our VPC, look at the packet in CloudWatch Logs, and determine how much delay there was between the time the packet was sent, and when it was recorded?

That would be fantastic. The only problem is that a flow log contains only metadata, and not any payload. Here is a screenshot of what we have to work with:



As it turns out, about the only thing that we have to fiddle with is the Source port and the Destination port. So, I will use the source port to encode the timestamp and send it to an unchanging port (TCP port 7777) at a target IP address outside of my VPC.

Will this work?

Let’s do some math here. We know that there are 65536 (2 to the power of 16) possible TCP ports.

If we send one packet per second from a different port, we could do that for 1092 Minutes:

&gt;&gt;&gt; (2 ** 16) / 60
1092.2666666666667


Or converting that to hours:

&gt;&gt;&gt; (2 ** 16) / 60 / 60
18.204444444444444


We get 18 hours. Hence, we can send traffic at a rate of one packet per second from a unique source port for over 18 hours before using up all possible ports!

Converting back and forth from the source port to the epoch time

To calculate which port to use, we will take the current epoch time and divide it by 65536 and then take the integer portion of the remainder and use that for the source port. The math function that returns the remainder is called “modulo” and is represented as a % in python

source port = int(current_epoch_time % 65536)


To convert back to the epoch time from the source port, take the current epoch time and divide it by 65536 and then keep the quotient portion. Divide the source port by 65536 and take that result and add it to the quotient. Multiply the new total by 65536 and the new result is the epoch time of when the data was sent.

epoch_timestamp = int((modulo / 65536.0 + int(current_epoch_time / 65536)) * 65536)


This conversion assumes that the current epoch time is within the 18 hour window represented by the source port value.

Introducing Scapy

To generate a packet per second with an ever increasing source port that represents the time that the packet was sent, I used scapy. Scapy is written as a python module and is ideal for programmatically crafting packets.

As we will be running Scapy on our EC2 instance to send traffic out of our VPC, we need to install Scapy, its dependencies, and the additional modules my python program will use:

sudo apt update
sudo apt install -y python3-pip tcpdump  python3-matplotlib
sudo pip3 install pytz
sudo pip install --pre scapy[complete]


Note that using sudo with pip to install software is generally a bad idea. See this link for a detailed discussion of the security issues.

However, in this case, Scapy needs to run as root to be able to send packets, and we will destroy our EC2 instance at the end of our experimentation.

To launch Scapy, just type sudo scapy at the Ubuntu command line as shown below:

ubuntu@ip-10-0-1-112:~$ sudo scapy
INFO: PyX dependencies are not installed ! Please install TexLive or MikTeX.

                     aSPY//YASa
             apyyyyCY//////////YCa       |
            sY//////YSpcs  scpCY//Pp     | Welcome to Scapy
 ayp ayyyyyyySCP//Pp           syY//C    | Version 2.4.5
 AYAsAYYYYYYYY///Ps              cY//S   |
         pCCCCY//p          cSSps y//Y   | https://github.com/secdev/scapy
         SPPPP///a          pP///AC//Y   |
              A//A            cyP////C   | Have fun!
              p///Ac            sC///a   |
              P////YCpc           A//A   | We are in France, we say Skappee.
       scccccp///pSP///p          p//Y   | OK? Merci.
      sY/////////y  caa           S//P   |             -- Sebastien Chabal
       cayCyayP//Ya              pY/Ya   |
        sY/PsY////YCc          aC//Yp
         sc  sccaCY//PCypaapyCP//YSs
                  spCPY//////YPSps
                       ccaacs
                                       using IPython 8.1.1
&gt;&gt;&gt;


I set up a target VM in another VPC with the security group set to allow inbound TCP port 7777 from the system with Scapy installed. As a test, I am running TCPdump on the target machine, using the following command:

sudo tcpdump -i eth0 'port 7777'



Next, use Scapy to send the target a packet. Enter the following commands at the Scapy IPython prompt, but substitute my target IP address (35.168.8.211) for the one you are using:

ip=IP(dst="35.168.8.211")
tcp=TCP(sport=1111,dport=7777,flags="S",options=[('Timestamp',(0,0))])
pay="This is Test3"
test3=ip/tcp/pay
send(test3)


The result should look like:

.
Sent 1 packets.


NOTE: If you get a “Operation not permitted” error, you forgot to run Scapy as root

Exit back to the Ubuntu command line using exit()

Looking over at the target system, we can see that TCPdump generated the following two records:

22:25:43.461121 IP ec2-34-201-148-40.compute-1.amazonaws.com.1111 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13
22:25:43.461156 IP ip-172-31-48-150.ec2.internal.7777 &gt; ec2-34-201-148-40.compute-1.amazonaws.com.1111: Flags [R.], seq 0, ack 14, win 0, length 0


We Sent a TCP “Syn” packet and a “Reset” was sent back because there is not a service listening on port 7777.

Putting the theory to work

Ok, so now that we know the Scapy commands to craft a packet. We can write a program to determine the Source Port to use, to craft the packet and sent it. We also can prove that the packet was received by the target system and see the timestamp printed to standard out by TCPdump.

The program is called generate-test-traffic.py and can be downloaded from my Github Repo

Copy this program to the EC2 instance that has Scapy Installed.

wget https://raw.githubusercontent.com/Resistor52/hitc-benchmark-flow-logs/hitc-ep18/generate-test-traffic.py



Run the program as root using

sudo python3 ./generate-test-traffic.py



Once the program runs, it will ask four questions as shown below and then start to send packets at a rate of one per second.

ubuntu@ip-10-0-1-112:~$ sudo python3 ./generate-test-traffic.py
Enter the name for the CSV log file (without the csv extension): test3A
Enter the desired target IP aggress in the format "111.111.111.111": 35.168.8.211
Enter the desired start time in the format "HH:MM" for local time zone: 19:00
Enter the desired end time in the format "HH:MM" for local time zone: 22:00
Traffic generation will start at: 19:00
Traffic generation will continue until: 22:00
**************** Start of Traffic **************************
.Epoch Time: 1646344800   Time: 2022-03-03 22:00:00 UTC+0000   srcPort: 14944   Remaining Time: 0 Seconds
***************** End of Traffic ***************************


In the example above, I am calling the log file test3A and provided 35.168.8.211 as the target IP. (Again, be sure to use the IP address of the target that you set up and has TCPdump listening.) I started the program at 19:00 hours UTC, simply because that was on the hour, and had it run for 3 hours, ending at 22:00. The program uses UTC because that is the time zone that AWS uses for all its cloud services, including EC2.

Now, while we are waiting for the traffic generation to start, rerun TCPdump on the target system, this time capturing the packets to a file:

sudo tcpdump -i eth0 'port 7777' -w tcpdump.pcap


Initial Look at the Data

The generate-test-traffic.py script created a log, which I named test3A.csv. Note that the script adds the “csv” extension to the filename. The top 10 rows of the file are shown below:

"Epoch_Time","Date_Time","Source_Port"
"1646334000","2022-03-03 19:00:00 UTC+0000","4144"
"1646334001","2022-03-03 19:00:01 UTC+0000","4145"
"1646334002","2022-03-03 19:00:02 UTC+0000","4146"
"1646334003","2022-03-03 19:00:03 UTC+0000","4147"
"1646334004","2022-03-03 19:00:04 UTC+0000","4148"
"1646334005","2022-03-03 19:00:05 UTC+0000","4149"
"1646334006","2022-03-03 19:00:06 UTC+0000","4150"
"1646334007","2022-03-03 19:00:07 UTC+0000","4151"
"1646334008","2022-03-03 19:00:08 UTC+0000","4152"


We can look at the first 10 packets received by the Target system using the following tcpdump command:

tcpdump -r tcpdump.pcap -nn -c10 'dst port 7777'


And the output is:

19:00:00.013928 IP ec2-34-201-148-40.compute-1.amazonaws.com.4144 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13
19:00:01.018892 IP ec2-34-201-148-40.compute-1.amazonaws.com.4145 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13
19:00:02.023309 IP ec2-34-201-148-40.compute-1.amazonaws.com.4146 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13
19:00:03.027319 IP ec2-34-201-148-40.compute-1.amazonaws.com.4147 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13
19:00:04.032328 IP ec2-34-201-148-40.compute-1.amazonaws.com.4148 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13
19:00:05.050076 IP ec2-34-201-148-40.compute-1.amazonaws.com.4149 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13
19:00:06.061365 IP ec2-34-201-148-40.compute-1.amazonaws.com.4150 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13
19:00:07.066456 IP ec2-34-201-148-40.compute-1.amazonaws.com.4151 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13
19:00:08.071536 IP ec2-34-201-148-40.compute-1.amazonaws.com.4152 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13
19:00:09.077085 IP ec2-34-201-148-40.compute-1.amazonaws.com.4153 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13


We can extract the two critical data elements from packet capture using the following command:

tcpdump -r tcpdump.pcap -nn -c10 'dst port 7777' | cut -d" " -f1,3 | cut -d"." -f1,6 | tr "." " "


And this command will output the timestamp and the source port of the packet received by the target system:

19:00:00 4144
19:00:01 4145
19:00:02 4146
19:00:03 4147
19:00:04 4148
19:00:05 4149
19:00:06 4150
19:00:07 4151
19:00:08 4152
19:00:09 4153


Great, everything correlates! Now lets write the whole packet to a file:

tcpdump -r tcpdump.pcap -nn 'dst port 7777' | cut -d" " -f1,3 | cut -d"." -f1,6 | tr "." " " &gt; test3A.log


We will save test3A.log for later, as it will come in handy.

Looking in CloudWatch Logs

The CloudWatch Logs Web Console is a great tool to slice and dice log data. Paste in the following query:

fields @ingestionTime, @timestamp, ((toMillis(@ingestionTime) - toMillis(@timestamp))/1000) as IngestDelaySecs, ((toMillis(@ingestionTime) - toMillis(@timestamp))/1000/60)
as IngestDelayMins, toMillis(@timestamp)/1000 as TimeStampEpoch, (65536*floor(TimeStampEpoch/65536))+srcPort as SentTimeEpoch, fromMillis(SentTimeEpoch * 1000) as SentTime,
TimeStampEpoch-SentTimeEpoch as TimeStampDelaySec, srcPort
| sort srcPort asc
| limit 10000
| filter srcAddr = "10.0.1.112" and dstAddr = "35.168.8.211" and dstPort != "22" and srcPort != "22")


Be sure to set the time filter in the upper window to include the window in which the packets were sent.



Looking at the output, we see that AWS provides two fields @ingestionTime and @timestamp so we can calculate the delay the Ingestion Delay Seconds (IngestDelaySecs) by converting both time values to Epoch time and subtracting them.

But how accurate is the timestamp? Well, we can use the CloudWatch Logs Query Language to calculate a SentTimeEpoch based on the srcPort. We can also convert that to SentTime so it is human-readable. If we subtract the SentTimeEpoch from the  TimeStampEpoch we get the TimeStampDelaySec.

Disregard the first record as that was captured when I demonstrated how to use Scapy with IPython using an arbitrary source port. (I generated the traffic using the Python Script a few hours beforehand and that explains the chronology of this timestamp.) This record will be removed from the downloaded dataset.

Another thing, that you may have noticed in the screenshot, is that the first srcPort is 4184 which is 40 seconds after we sent the packet with srcPort 4144!

Very interesting. It looks like we have “negative delays.” or to phrase it more accurately, the @timestamp value attached by AWS has an error that fluctuates within a range.

Use the “Export Results” button to save the file as logs-insights-results.csv

Looking at the Data Graphically

Since we see that we have an Ingest Delay and a Timestamp Error, it would be good to look at this data graphically. Toward that end, I have create a python program called plotObservations.py which can be downloaded from my Github Repo.

Copy this program to the EC2 instance that has Scapy Installed.

wget https://raw.githubusercontent.com/Resistor52/hitc-benchmark-flow-logs/hitc-ep18/plotObservations.py



Run the program as using:

python3 ./plotObservations.py



The program will read logs-insights-results.csv and will generate the four plots shown below.


PLOT 1 - Packets 1000 to 2000


PLOT 2 - Packets 1020 to 1200


PLOT 3 - The First 10,000 Packets


Delta - Seconds Between Missing Data

I saved the charts generated from data captured in the exact same manner 24 hours prior to this set, and those are shown below for comparison purposes.


PLOT 1b - Packets 1000 to 2000 (Previous Run)


PLOT 3b - The First 10,000 Packets (Previous Run)

Observations

Ingest Delay - We can see the ingest delay varies from a little over 60 seconds to almost 11 minutes every 10 minutes. Note that when we configured the flow logs we used a setting of max-aggregation-interval 600. This test should be repeated using a max-aggregation-interval 60 to measure the impact on the Ingest Delay.

Timestamp Error Seconds - The TimeStampDelaySec is calculated by subtracting the SentTimeEpoch from TimeStampEpoch. Chronologically, the SentTimeEpoch should occur first, as that is the timestamp of when Scapy sent the packet. Similarly, the TimeStampEpoch is when the VPC Flow Logs system detects the packet on the virtual network. I expected the error to be within milliseconds, but not almost a whole minute off! I am not sure how to explain this. At first, I thought that it might be a math error on my part. But we can take a look at a single packet as it was:


  Recorded in the flow log (logs-insights-results.csv),
  Sent by Scapy (test3A.csv), and
  Recorded by TCPdump at the Target System (tcpdump.pcap)


We can use grep -E ',-58,|srcPort' logs-insights-results.csv | head -n6 | cut -d"," -f5,6-9 to generate the following table:


  
    
      TimeStampEpoch
      SentTimeEpoch
      SentTime
      TimeStampDelaySec
      srcPort
    
  
  
    
      1646334040
      1646334098
      2022-03-03 19:01:38.000
      -58
      4242
    
    
      1646334099
      1646334157
      2022-03-03 19:02:37.000
      -58
      4301
    
    
      1646334160
      1646334218
      2022-03-03 19:03:38.000
      -58
      4362
    
    
      1646334219
      1646334277
      2022-03-03 19:04:37.000
      -58
      4421
    
    
      1646334280
      1646334338
      2022-03-03 19:05:38.000
      -58
      4482
    
  


Selected packets from logs-insights-results.csv

Now that we have identified the first five packets with the highest error (-58), we can look for those packets in the log generated by Scapy using grep -E '"4242"|"4301"|"4362"|"4421"|"4482"' test3A.csv


  
    
      Epoch_Time
      Date_Time
      Source_Port
    
  
  
    
      1646334098
      2022-03-03 19:01:38 UTC+0000
      4242
    
    
      1646334157
      2022-03-03 19:02:37 UTC+0000
      4301
    
    
      1646334218
      2022-03-03 19:03:38 UTC+0000
      4362
    
    
      1646334277
      2022-03-03 19:04:37 UTC+0000
      4421
    
    
      1646334338
      2022-03-03 19:05:38 UTC+0000
      4482
    
  


Selected packets from test3A.csv

We can see that the SentTimeEpoch as calculated by my CloudWatch query logic is performed correctly as it matches the Epoch_Time as logged by Scapy.

For completeness, let’s look at TCP dump:

tcpdump -tt -nn -r tcpdump.pcap 'src port 4242 or src port 4301 or src port 4362 or src port 4421 or src port 4482' | cut -d":" -f1


produces:

1646334098.569321 IP 34.201.148.40.4242 &gt; 172.31.48.150.7777
1646334157.873827 IP 34.201.148.40.4301 &gt; 172.31.48.150.7777
1646334218.182541 IP 34.201.148.40.4362 &gt; 172.31.48.150.7777
1646334277.489424 IP 34.201.148.40.4421 &gt; 172.31.48.150.7777
1646334338.805517 IP 34.201.148.40.4482 &gt; 172.31.48.150.7777


As expected, we can see that the epoch time recorded by TCPdump matches the other observations in the tables above.

Missing Data - Looking at the Missing Data plots in the figures above initially raised some concerns. Is the VPC dropping packets? To investigate what is going on, I tweaked plotObservations.py to generate the “Delta” plot to show the periodicity of the missing data as well as to output the srcPorts for which data is missing. That output is shown below:

The following srcPorts are missing:
[4327, 4506, 4694, 4890, 5082, 5276, 5471, 5666, 5855,
6049, 6245, 6439, 6632, 6824, 7018, 7211, 7407, 7600,
7791, 7987, 8174, 8365, 8555, 8750, 8942, 9135, 9330,
9524, 9716, 9911, 10102, 10293, 10485, 10680, 10866,
11058, 11253, 11427, 11620, 11811, 12005, 12198, 12386,
12583, 12778, 12972, 13166, 13361, 13555, 13750, 13945,
14138]


I also had plotObservations.py write the ports to a file, missingSrcPorts.csv so that we can grep for them:

grep -f missingSrcPorts.csv test3A.csv


Hmm, none of the missing srcPorts were logged by Scapy. The good news is that the VPC network did not drop any packets because all sent packets have been accounted for. The bad news is that there is bug in generate-test-traffic.py that occurs about every 195 seconds. I suspect that it is an issue with my generateTraffic function, most likely due to inaccuracies with time.sleep() compounded by variations in the amount of time it takes Scapy to write the packet out to the network.

def generateTraffic(stop_time_epoch):
    datetime_TZ = datetime.now(tz_TZ)
    seconds = int(time.time())             # Current Epoch Time
    modulo = seconds % (2 ** 16)           # Calculate the SrcPort
    datetime_time=datetime_TZ.strftime("%Y-%m-%d %H:%M:%S %Z%z")
    print(f"Epoch Time: {seconds}   Time: {datetime_time}   srcPort: {modulo}   {RemainingTimeString(stop_time_epoch)}      ", end = "\r")
    write_str='"'+str(seconds)+'","'+datetime_time+'","'+str(modulo)+'"\n'
    f.write(write_str)
    tcp=TCP(sport=modulo,dport=7777,flags="S",options=[('Timestamp',(0,0))])
    pay=logfile+" - "+str(modulo)
    # Trap the output of scapy send so it does not print to console
    state = sys.stdout
    sys.stdout = open('/dev/null', 'w')
    send(ip/tcp/pay)                        # Scapy
    sys.stdout.close()
    sys.stdout = state
    # Wait one second between packets
    time.sleep(1)
    return


OBJECTIVE 4 - Contemplate the implications of our findings

Tests 1 and 2 demonstrate that we need to budget ample time ( at least 16 minutes) to allow AWS to provision the flow logs. We observed that flow logs sent to CloudWatch Logs provision faster than logs sent to a S3 bucket.

Test 3 demonstrated that with a max-aggregation-interval set to “600,” it is possible to see an ingestion delay of up to 10.42 minutes (cat logs-insights-results.csv | cut -d"," -f4 | sort -u -n | tail -n1) and that it is a function of how long ago the last aggregation occurred. The minimum ingestion delay was 1.26 minutes (cat logs-insights-results.csv | cut -d"," -f4 | sort -u -n | head -n2). Lots of bad stuff could be detected late if you were expecting to use flow logs for real-time detection. I think that AWS would tell you that is not the use case that they were designed for. Instead use something like a Web Application Firewall (WAF).

We observed an unaccounted-for error in the ‘@timestamp’ field generated by Cloud Watch Logs Insights of +1 to -60 seconds during our test and +60 to -58 during a previous test. This demonstrates the value of benchmarking critical security systems.

In this experiment, we demonstrated a novel technique for timestamping packets that will be reduced to a flow log by using the source port field. We showed how to verify the accuracy of the technique by comparing the epoch time calculated from the source port with the time logged by TCPdump.

We also demonstrated how to identify any missing packets using this technique and in the process identified a minor bug in generate-test-traffic.py that caused a packet not to be sent about every 195 seconds. This demonstrates (again) the importance of understanding the limitations of the security tools that we use. At some point, I will circle back and fix this bug.

Remember that the limited amount of testing that was performed is not sufficient to draw too many conclusions. Tests 1 and 2 should be repeated multiple times at different times of day, over the course of several days, in different regions to get more precise statistics. Test 3 should be repeated multiple times and should consider the impact of changing the max-aggregation-interval setting. The more the testing process is automated the lower the burden of repeating each test. But, I think that I have taken this little experiment far enough for now.

Wrap Up

Wow, we covered quite a bit in this Episode:

  We made extensive use of the AWS CLI
  We used a Bash “while loop” to time AWS CLI calls
  We crafted packets using Scapy and read those packets using TCPdump.
  We also created a fairly sophisticated CloudWatch Logs Insights query, and
  Learned how we can use Python to visualize data.


To learn more about TCPdump, Scapy, and packet crafting check out SANS SEC503: Intrusion Analysis In-Depth. To learn more about using Python to create security tools like generate-test-traffic.py consider SANS SEC573: Automating Information Security with Python.

Closing Comments

If you have thoughts or comments on today’s episode, feel free to chime in on the comments for this YouTube video.

If you appreciate this video and want to see more like it, be sure to give it a “thumbs up.”

Stay tuned for another installment of “Head in the Clouds” as announcements of new episodes are made on the SANS Cloud Security Twitter feed.

Meanwhile, be sure to check out the other great videos on the SANS Cloud Security YouTube Channel.

Take care.
">
<meta property="og:url" content="/episodes/episode18/">
<meta property="og:site_name" content="Head in the Clouds">

<meta property="og:image" content="/images/default-thumb.png">






<link rel="canonical" href="/episodes/episode18/">
<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Head in the Clouds Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">


<meta http-equiv="cleartype" content="on">


<!-- Modernizr -->
<script src="/assets/js/vendor/modernizr-2.7.1.custom.min.js"></script>


<link href='//fonts.googleapis.com/css?family=PT+Sans+Narrow:400,700%7CPT+Serif:400,700,400italic' rel='stylesheet' type='text/css'>
<link rel="stylesheet" href="/assets/css/academicons/css/academicons.css"/>
<!-- Icons -->
<!-- 16x16 -->
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">

<!--Jekyll-seo plugin-->
<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Episode 18 - Benchmarking AWS Flow Logs | Head in the Clouds</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Episode 18 - Benchmarking AWS Flow Logs" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="18 - Benchmarking AWS Flow Logs Welcome to Episode 18 of the “Head in the Clouds” Video Series. I am Ken Hartman, a SANS Certified Instructor and content creator for the SANS Cloud Security Curriculum. Today’s episode is titled: “Benchmarking AWS Flow Logs” In many of the of the courses in the SANS Cloud Curriculum, we will have one or more labs that involves flow logs. However, a portion of the lab time is spent waiting for the flow logs to get provisioned. It’s kind of like watching and waiting for water to boil. Then, once the flow logs are provisioned, it takes time for the measured traffic to show up. Therefore, I thought it would be very interesting to determine what one should expect via experimentation and observation. Along the way, I made some very interesting discoveries, and I cannot wait to show them to you. Objectives Anytime we use a tool, technique, or service, we should be aware of its limitations and possible pitfalls. As such, here are the objectives for today: Experimentally determine the typical time to provision flow logs to an S3 bucket. Experimentally determine the typical time to provision flow logs to a CloudWatch logs group. Experimentally determine the typical delay for ongoing traffic to be captured in the flow log record. Contemplate the implications of our findings. Setup For this episode, we will assume that you have an AWS EC2 virtual machine running in a VPC that has a “Name” tag of “hitc-vpc.” The t2.micro type that is in the free tier will do just fine. I recommend that you use the Ubuntu 20.04 AMI for your region, as this will allow me to provide you with the exact commands to install some additional software later in this episode. I used Terraform to deploy my environment as described in the HitC Episode titled Episode 10 - Demonstration of Terraform Modules; Deploy a VM in AWS, Azure and GCP at once as I had originally planned to benchmark all three cloud service providers. However, as I got deeper and deeper into my research into AWS flow logs, I decided to stay focused on just AWS and may circle back to Azure and GCP flow logs in one or more future episodes. In this episode, we will be making extensive use of the AWS command line interface. This has two benefits: It will make it very easy for you to reproduce my work, and It will allow us to measure the time that it takes for things to happen. The machine on which you are running the CLI commands will need jq in addition to the AWS command line interface. OBJECTIVE 1: Experimentally determine the typical time to provision flow logs to an S3 bucket To start with, we need to set some variables that will be used by our commands: UNIQ_ID=$RANDOM; echo $UNIQ_ID ACCOUNT=$(aws sts get-caller-identity | jq -r .Account); echo $ACCOUNT YYYY=$(date -Idate | cut -d&quot;-&quot; -f1); echo $YYYY MM=$(date -Idate | cut -d&quot;-&quot; -f2); echo $MM DD=$(date -Idate | cut -d&quot;-&quot; -f3); echo $DD REGION=$(aws ec2 describe-availability-zones --output text --query &#39;AvailabilityZones[0].[RegionName]&#39;); echo $REGION We generate a unique ID to ensure that the S3 bucket that we create is unique. Linux has a variable called $RANDOM that we can use for this purpose. The YYYY, MM, and DD are captured so we can look into the bucket for our data. Lastly, we use a trick from StackOverflow.com to get the region, as yours may be different from mine. Next, we will create a variety of “helper commands.” Get the VPC ID To get the VPC ID of the VPC that is named “hitc-vpc” we can use: VPC_ID=$(aws ec2 describe-vpcs --filters Name=tag:Name,Values=hitc-vpc --query &quot;Vpcs[0].VpcId&quot; --output text); echo $VPC_ID Create the Bucket for the Flow Logs Next, we need to create an S3 bucket to receive our flow logs: aws s3 mb s3://hitc-benchmark-flowlogs-$UNIQ_ID Set up the Flow Logs for the VPC and Send to S3 Our next task is to make sure we have the command to set the flow logs for the VPC and send them to S3: aws ec2 create-flow-logs \ --resource-type VPC \ --resource-ids $VPC_ID \ --traffic-type ALL \ --log-destination-type s3 \ --log-destination arn:aws:s3:::hitc-benchmark-flowlogs-$UNIQ_ID \ --max-aggregation-interval 600 \ --tag-specifications &#39;ResourceType=vpc-flow-log,Tags=[{Key=Name,Value=hitc-flow-log}]&#39; Describe the Flow Logs for the VPC Having done that, we may find it helpful to be able to describe the flow logs for the VPC in question: aws ec2 describe-flow-logs --filter &quot;Name=resource-id,Values=$VPC_ID&quot; Delete the Flow Logs for the VPC Next, we need to be able to delete the flow logs that we just created. To do this we need to know the Flow Log ID. FLOW_LOG_ID=$(aws ec2 describe-flow-logs --filter &quot;Name=resource-id,Values=$VPC_ID&quot; --query &#39;FlowLogs[0].FlowLogId&#39; --output text) echo $FLOW_LOG_ID aws ec2 delete-flow-logs --flow-log-ids $FLOW_LOG_ID List the contents of the Bucket To verify the presence of a flow log, we will need to be able to drill deep into the S3 bucket. This is facilitated by the variables we set earlier. aws s3 ls s3://hitc-benchmark-flowlogs-$UNIQ_ID/AWSLogs/$ACCOUNT/vpcflowlogs/$REGION/$YYYY/$MM/$DD/ Empty the Bucket We also need the ability to empty the bucket between successive iterations of our test: aws s3 rm s3://hitc-benchmark-flowlogs-$UNIQ_ID --recursive Remove the Bucket It is also good idea to remove the bucket when we are done with our testing: aws s3 rb s3://hitc-benchmark-flowlogs-$UNIQ_ID Generate Flow Log Traffic Now that we’ve got our helper scripts figured out and tested, we need to generate some traffic to get captured by our flow log. SSH into your easy to instance and run the following command: while true ; do wget --timeout=10 http://1.1.1.1:81; sleep 20; done Note that I selected CloudFlare as the target as they are big enough that this would not be considered abusive and by sending the traffic to port 81, the retries will keep working until exhausted after 20 attempts and will then start over after 20 seconds. Anything that repetitively generates traffic will work here. TEST 1 - Time the provisioning of the flow Log until first flow log record in S3 With that done, we can now measure the time between the provisioning of the flow log until first flow log record in S3. Every 20 seconds we will check the S3 bucket to see if the flow log showed up yet. Our “TEST” checks to see if the length of response of the aws s3 ls command is zero. Once there is a flow log record in the bucket, the infinite loop will exit and we subtract the start time from the end time. Working in epoch time, makes this easy. NOTE: This assumes that the S3 bucket exists, but is empty # Set up the Flow Logs for the VPC aws ec2 create-flow-logs \ --resource-type VPC \ --resource-ids $VPC_ID \ --traffic-type ALL \ --log-destination-type s3 \ --log-destination arn:aws:s3:::hitc-benchmark-flowlogs-$UNIQ_ID \ --max-aggregation-interval 600 \ --tag-specifications &#39;ResourceType=vpc-flow-log,Tags=[{Key=Name,Value=hitc-flow-log}]&#39; # Watch for initial flow logs printf &quot;Waiting for the flow logs to apear in S3 bucket &quot; START_TIMER=$(date +%s) TEST=&quot;&quot; while [ -z &quot;$TEST&quot; ] ; do printf &quot;.&quot; sleep 20 TEST=$(aws s3 ls s3://hitc-benchmark-flowlogs-$UNIQ_ID/AWSLogs/$ACCOUNT/vpcflowlogs/$REGION/$YYYY/$MM/$DD/) done STOP_TIMER=$(date +%s) echo &quot;DONE&quot; echo $((STOP_TIMER-START_TIMER))&quot; seconds have elapsed&quot; While this script executes, it will print a dot to stdout every 20 seconds. When it is done, the output will look like this: 772 seconds have elapsed Ok, 772 seconds is 12.87 minutes. We now have our TEST 1 measurement. Note that this can be off by 20 seconds due to the sleep timer, which can be removed. Tear down TEST 1 To repeat the test, we need to delete the flow log and empty the S3 bucket, but leave the S3 bucket in place. # Delete the Flow Logs for the VPC FLOW_LOG_ID=$(aws ec2 describe-flow-logs --filter &quot;Name=resource-id,Values=$VPC_ID&quot; --query &#39;FlowLogs[0].FlowLogId&#39; --output text) echo $FLOW_LOG_ID aws ec2 delete-flow-logs --flow-log-ids $FLOW_LOG_ID # Empty the Bucket aws s3 rm s3://hitc-benchmark-flowlogs-$UNIQ_ID --recursive TEST 1 Summary This table shows the results of repeating the test 3 more times: Script Results Minutes 772 seconds have elapsed 12.86 Minutes 417 seconds have elapsed 6.97 Minutes 813 seconds have elapsed 13.55 Minutes 960 seconds have elapsed 16.00 Minutes Wow, it can take 16 minutes to get your first flow logs! So, what is up with this? Well, the catch phrase that the cloud service providers use is eventual consistency. We know that the big guys use queuing and focus intently on optimization, so I am guessing that it is for those reasons that there is a provisioning delay. Final Cleanup TEST 1 Now that we have obtained our results, make sure that you delete the flow log and empty the bucket again, using the last set of commands and then remove the bucket, as we no longer need it. # Remove the Bucket aws s3 rb s3://hitc-benchmark-flowlogs-$UNIQ_ID OBJECTIVE 2: Experimentally determine the typical time to provision flow logs to a Cloud Watch logs group Now, let’s take a look at Flow Logs sent to Cloud Watch Logs. Create the CloudWatch Logs Group First we need to create the CloudWatch Logs Group: aws logs create-log-group --log-group-name hitc-flow-logs Delete the Log Group And, here is the corresponding command to delete the log group: aws logs delete-log-group --log-group-name hitc-flow-logs Create the CloudWatch Logs Role Next, we need create the CloudWatch Logs Role. To do this we will create the Trust Policy JSON file, the Permissions Policy JSON file, and then attach them to a new role. Create the Trust Policy JSON file. I like to use the “heredoc” technique, so that everything can be scripted: cat &lt;&lt; EOF &gt; trust-policy.json { &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Effect&quot;: &quot;Allow&quot;, &quot;Principal&quot;: { &quot;Service&quot;: &quot;vpc-flow-logs.amazonaws.com&quot; }, &quot;Action&quot;: &quot;sts:AssumeRole&quot; } ] } EOF Create the Permissions Policy JSON file: cat &lt;&lt; EOF &gt; cw-flow-logs-policy.json { &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: [ &quot;logs:CreateLogGroup&quot;, &quot;logs:CreateLogStream&quot;, &quot;logs:PutLogEvents&quot;, &quot;logs:DescribeLogGroups&quot;, &quot;logs:DescribeLogStreams&quot; ], &quot;Resource&quot;: &quot;*&quot; } ] } EOF Next, run the following commands to create the policies and the role: aws iam create-policy --policy-name hitc-cw-flow-logs --policy-document file://cw-flow-logs-policy.json aws iam create-role --role-name hitc-cw-flow-logs --assume-role-policy-document file://trust-policy.json aws iam attach-role-policy --policy-arn &quot;arn:aws:iam::$ACCOUNT:policy/hitc-cw-flow-logs&quot; --role-name hitc-cw-flow-logs aws iam list-attached-role-policies --role-name hitc-cw-flow-logs TEST 2 - Time the provisioning of the flow Log until first flow log record in CloudWatch With our setup for Test 2 in place, we can now conduct it. This “TEST” checks to see if the length of response of the aws logs describe-log-streams command is zero. Once there is a flow log record in CW logs, the infinite loop will exit and we subtract the start time from the end time. NOTE: This assumes that the CloudWatch Log Group exists, but is empty. # Set up the Flow Logs for the VPC aws ec2 create-flow-logs \ --resource-type VPC \ --resource-ids $VPC_ID \ --traffic-type ALL \ --log-destination-type cloud-watch-logs \ --log-group-name hitc-flow-logs \ --max-aggregation-interval 600 \ --tag-specifications &#39;ResourceType=vpc-flow-log,Tags=[{Key=Name,Value=hitc-flow-log}]&#39; \ --deliver-logs-permission-arn &#39;arn:aws:iam::690634326977:role/hitc-cw-flow-logs&#39; # Watch for initial flow logs printf &quot;Waiting for the flow logs to appear in CW Logs Group &quot; START_TIMER=$(date +%s) TEST=&quot;&quot; while [ -z &quot;$TEST&quot; ] ; do printf &quot;.&quot; sleep 20 TEST=$(aws logs describe-log-streams --log-group-name hitc-flow-logs --output text) done STOP_TIMER=$(date +%s) echo &quot;DONE&quot; echo $((STOP_TIMER-START_TIMER))&quot; seconds have elapsed&quot; Tear down TEST 2 To re-run the test, delete the flow log and reset the CW Logs Group. # Delete the Flow Logs for the VPC FLOW_LOG_ID=$(aws ec2 describe-flow-logs --filter &quot;Name=resource-id,Values=$VPC_ID&quot; --query &#39;FlowLogs[0].FlowLogId&#39; --output text) echo $FLOW_LOG_ID aws ec2 delete-flow-logs --flow-log-ids $FLOW_LOG_ID # Delete and Recreate the CW Logs Group aws logs delete-log-group --log-group-name hitc-flow-logs aws logs create-log-group --log-group-name hitc-flow-logs Final Cleanup TEST 2 And when we are all done, we can run the following commands to delete the role and associated policy. But hold off for now as we will need this role for Test 3. # CLI commands to tear down the role and IAM policy aws iam detach-role-policy --role-name hitc-cw-flow-logs --policy-arn &quot;arn:aws:iam::$ACCOUNT:policy/hitc-cw-flow-logs&quot; aws iam delete-role --role-name hitc-cw-flow-logs aws iam delete-policy --policy-arn &quot;arn:aws:iam::$ACCOUNT:policy/hitc-cw-flow-logs&quot; TEST 2 Summary Script Results Minutes 21 seconds have elapsed 0.35 Minutes 541 seconds have elapsed 9.02 Minutes 522 seconds have elapsed 8.70 Minutes 375 seconds have elapsed 6.25 Minutes This is much better, but it can still take up to 10 minutes and is somewhat variable. OBJECTIVE 3: Experimentally determine the typical delay for ongoing traffic to be captured in the flow log record. Now, onto what is probably the most interesting and has the biggest implications for security monitoring. We want to know how long will it take for traffic that occurs in my VPC to show up as recorded in my flow logs? In other words, “I want to know how much lag there is in any detective control that I implement (that is based on flow logs).” The security implications of this are fairly significant. The Problem Statement Wouldn’t it be great to time stamp a packet, send it across the virtual network of our VPC, look at the packet in CloudWatch Logs, and determine how much delay there was between the time the packet was sent, and when it was recorded? That would be fantastic. The only problem is that a flow log contains only metadata, and not any payload. Here is a screenshot of what we have to work with: As it turns out, about the only thing that we have to fiddle with is the Source port and the Destination port. So, I will use the source port to encode the timestamp and send it to an unchanging port (TCP port 7777) at a target IP address outside of my VPC. Will this work? Let’s do some math here. We know that there are 65536 (2 to the power of 16) possible TCP ports. If we send one packet per second from a different port, we could do that for 1092 Minutes: &gt;&gt;&gt; (2 ** 16) / 60 1092.2666666666667 Or converting that to hours: &gt;&gt;&gt; (2 ** 16) / 60 / 60 18.204444444444444 We get 18 hours. Hence, we can send traffic at a rate of one packet per second from a unique source port for over 18 hours before using up all possible ports! Converting back and forth from the source port to the epoch time To calculate which port to use, we will take the current epoch time and divide it by 65536 and then take the integer portion of the remainder and use that for the source port. The math function that returns the remainder is called “modulo” and is represented as a % in python source port = int(current_epoch_time % 65536) To convert back to the epoch time from the source port, take the current epoch time and divide it by 65536 and then keep the quotient portion. Divide the source port by 65536 and take that result and add it to the quotient. Multiply the new total by 65536 and the new result is the epoch time of when the data was sent. epoch_timestamp = int((modulo / 65536.0 + int(current_epoch_time / 65536)) * 65536) This conversion assumes that the current epoch time is within the 18 hour window represented by the source port value. Introducing Scapy To generate a packet per second with an ever increasing source port that represents the time that the packet was sent, I used scapy. Scapy is written as a python module and is ideal for programmatically crafting packets. As we will be running Scapy on our EC2 instance to send traffic out of our VPC, we need to install Scapy, its dependencies, and the additional modules my python program will use: sudo apt update sudo apt install -y python3-pip tcpdump python3-matplotlib sudo pip3 install pytz sudo pip install --pre scapy[complete] Note that using sudo with pip to install software is generally a bad idea. See this link for a detailed discussion of the security issues. However, in this case, Scapy needs to run as root to be able to send packets, and we will destroy our EC2 instance at the end of our experimentation. To launch Scapy, just type sudo scapy at the Ubuntu command line as shown below: ubuntu@ip-10-0-1-112:~$ sudo scapy INFO: PyX dependencies are not installed ! Please install TexLive or MikTeX. aSPY//YASa apyyyyCY//////////YCa | sY//////YSpcs scpCY//Pp | Welcome to Scapy ayp ayyyyyyySCP//Pp syY//C | Version 2.4.5 AYAsAYYYYYYYY///Ps cY//S | pCCCCY//p cSSps y//Y | https://github.com/secdev/scapy SPPPP///a pP///AC//Y | A//A cyP////C | Have fun! p///Ac sC///a | P////YCpc A//A | We are in France, we say Skappee. scccccp///pSP///p p//Y | OK? Merci. sY/////////y caa S//P | -- Sebastien Chabal cayCyayP//Ya pY/Ya | sY/PsY////YCc aC//Yp sc sccaCY//PCypaapyCP//YSs spCPY//////YPSps ccaacs using IPython 8.1.1 &gt;&gt;&gt; I set up a target VM in another VPC with the security group set to allow inbound TCP port 7777 from the system with Scapy installed. As a test, I am running TCPdump on the target machine, using the following command: sudo tcpdump -i eth0 &#39;port 7777&#39; Next, use Scapy to send the target a packet. Enter the following commands at the Scapy IPython prompt, but substitute my target IP address (35.168.8.211) for the one you are using: ip=IP(dst=&quot;35.168.8.211&quot;) tcp=TCP(sport=1111,dport=7777,flags=&quot;S&quot;,options=[(&#39;Timestamp&#39;,(0,0))]) pay=&quot;This is Test3&quot; test3=ip/tcp/pay send(test3) The result should look like: . Sent 1 packets. NOTE: If you get a “Operation not permitted” error, you forgot to run Scapy as root Exit back to the Ubuntu command line using exit() Looking over at the target system, we can see that TCPdump generated the following two records: 22:25:43.461121 IP ec2-34-201-148-40.compute-1.amazonaws.com.1111 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13 22:25:43.461156 IP ip-172-31-48-150.ec2.internal.7777 &gt; ec2-34-201-148-40.compute-1.amazonaws.com.1111: Flags [R.], seq 0, ack 14, win 0, length 0 We Sent a TCP “Syn” packet and a “Reset” was sent back because there is not a service listening on port 7777. Putting the theory to work Ok, so now that we know the Scapy commands to craft a packet. We can write a program to determine the Source Port to use, to craft the packet and sent it. We also can prove that the packet was received by the target system and see the timestamp printed to standard out by TCPdump. The program is called generate-test-traffic.py and can be downloaded from my Github Repo Copy this program to the EC2 instance that has Scapy Installed. wget https://raw.githubusercontent.com/Resistor52/hitc-benchmark-flow-logs/hitc-ep18/generate-test-traffic.py Run the program as root using sudo python3 ./generate-test-traffic.py Once the program runs, it will ask four questions as shown below and then start to send packets at a rate of one per second. ubuntu@ip-10-0-1-112:~$ sudo python3 ./generate-test-traffic.py Enter the name for the CSV log file (without the csv extension): test3A Enter the desired target IP aggress in the format &quot;111.111.111.111&quot;: 35.168.8.211 Enter the desired start time in the format &quot;HH:MM&quot; for local time zone: 19:00 Enter the desired end time in the format &quot;HH:MM&quot; for local time zone: 22:00 Traffic generation will start at: 19:00 Traffic generation will continue until: 22:00 **************** Start of Traffic ************************** .Epoch Time: 1646344800 Time: 2022-03-03 22:00:00 UTC+0000 srcPort: 14944 Remaining Time: 0 Seconds ***************** End of Traffic *************************** In the example above, I am calling the log file test3A and provided 35.168.8.211 as the target IP. (Again, be sure to use the IP address of the target that you set up and has TCPdump listening.) I started the program at 19:00 hours UTC, simply because that was on the hour, and had it run for 3 hours, ending at 22:00. The program uses UTC because that is the time zone that AWS uses for all its cloud services, including EC2. Now, while we are waiting for the traffic generation to start, rerun TCPdump on the target system, this time capturing the packets to a file: sudo tcpdump -i eth0 &#39;port 7777&#39; -w tcpdump.pcap Initial Look at the Data The generate-test-traffic.py script created a log, which I named test3A.csv. Note that the script adds the “csv” extension to the filename. The top 10 rows of the file are shown below: &quot;Epoch_Time&quot;,&quot;Date_Time&quot;,&quot;Source_Port&quot; &quot;1646334000&quot;,&quot;2022-03-03 19:00:00 UTC+0000&quot;,&quot;4144&quot; &quot;1646334001&quot;,&quot;2022-03-03 19:00:01 UTC+0000&quot;,&quot;4145&quot; &quot;1646334002&quot;,&quot;2022-03-03 19:00:02 UTC+0000&quot;,&quot;4146&quot; &quot;1646334003&quot;,&quot;2022-03-03 19:00:03 UTC+0000&quot;,&quot;4147&quot; &quot;1646334004&quot;,&quot;2022-03-03 19:00:04 UTC+0000&quot;,&quot;4148&quot; &quot;1646334005&quot;,&quot;2022-03-03 19:00:05 UTC+0000&quot;,&quot;4149&quot; &quot;1646334006&quot;,&quot;2022-03-03 19:00:06 UTC+0000&quot;,&quot;4150&quot; &quot;1646334007&quot;,&quot;2022-03-03 19:00:07 UTC+0000&quot;,&quot;4151&quot; &quot;1646334008&quot;,&quot;2022-03-03 19:00:08 UTC+0000&quot;,&quot;4152&quot; We can look at the first 10 packets received by the Target system using the following tcpdump command: tcpdump -r tcpdump.pcap -nn -c10 &#39;dst port 7777&#39; And the output is: 19:00:00.013928 IP ec2-34-201-148-40.compute-1.amazonaws.com.4144 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13 19:00:01.018892 IP ec2-34-201-148-40.compute-1.amazonaws.com.4145 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13 19:00:02.023309 IP ec2-34-201-148-40.compute-1.amazonaws.com.4146 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13 19:00:03.027319 IP ec2-34-201-148-40.compute-1.amazonaws.com.4147 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13 19:00:04.032328 IP ec2-34-201-148-40.compute-1.amazonaws.com.4148 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13 19:00:05.050076 IP ec2-34-201-148-40.compute-1.amazonaws.com.4149 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13 19:00:06.061365 IP ec2-34-201-148-40.compute-1.amazonaws.com.4150 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13 19:00:07.066456 IP ec2-34-201-148-40.compute-1.amazonaws.com.4151 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13 19:00:08.071536 IP ec2-34-201-148-40.compute-1.amazonaws.com.4152 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13 19:00:09.077085 IP ec2-34-201-148-40.compute-1.amazonaws.com.4153 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13 We can extract the two critical data elements from packet capture using the following command: tcpdump -r tcpdump.pcap -nn -c10 &#39;dst port 7777&#39; | cut -d&quot; &quot; -f1,3 | cut -d&quot;.&quot; -f1,6 | tr &quot;.&quot; &quot; &quot; And this command will output the timestamp and the source port of the packet received by the target system: 19:00:00 4144 19:00:01 4145 19:00:02 4146 19:00:03 4147 19:00:04 4148 19:00:05 4149 19:00:06 4150 19:00:07 4151 19:00:08 4152 19:00:09 4153 Great, everything correlates! Now lets write the whole packet to a file: tcpdump -r tcpdump.pcap -nn &#39;dst port 7777&#39; | cut -d&quot; &quot; -f1,3 | cut -d&quot;.&quot; -f1,6 | tr &quot;.&quot; &quot; &quot; &gt; test3A.log We will save test3A.log for later, as it will come in handy. Looking in CloudWatch Logs The CloudWatch Logs Web Console is a great tool to slice and dice log data. Paste in the following query: fields @ingestionTime, @timestamp, ((toMillis(@ingestionTime) - toMillis(@timestamp))/1000) as IngestDelaySecs, ((toMillis(@ingestionTime) - toMillis(@timestamp))/1000/60) as IngestDelayMins, toMillis(@timestamp)/1000 as TimeStampEpoch, (65536*floor(TimeStampEpoch/65536))+srcPort as SentTimeEpoch, fromMillis(SentTimeEpoch * 1000) as SentTime, TimeStampEpoch-SentTimeEpoch as TimeStampDelaySec, srcPort | sort srcPort asc | limit 10000 | filter srcAddr = &quot;10.0.1.112&quot; and dstAddr = &quot;35.168.8.211&quot; and dstPort != &quot;22&quot; and srcPort != &quot;22&quot;) Be sure to set the time filter in the upper window to include the window in which the packets were sent. Looking at the output, we see that AWS provides two fields @ingestionTime and @timestamp so we can calculate the delay the Ingestion Delay Seconds (IngestDelaySecs) by converting both time values to Epoch time and subtracting them. But how accurate is the timestamp? Well, we can use the CloudWatch Logs Query Language to calculate a SentTimeEpoch based on the srcPort. We can also convert that to SentTime so it is human-readable. If we subtract the SentTimeEpoch from the TimeStampEpoch we get the TimeStampDelaySec. Disregard the first record as that was captured when I demonstrated how to use Scapy with IPython using an arbitrary source port. (I generated the traffic using the Python Script a few hours beforehand and that explains the chronology of this timestamp.) This record will be removed from the downloaded dataset. Another thing, that you may have noticed in the screenshot, is that the first srcPort is 4184 which is 40 seconds after we sent the packet with srcPort 4144! Very interesting. It looks like we have “negative delays.” or to phrase it more accurately, the @timestamp value attached by AWS has an error that fluctuates within a range. Use the “Export Results” button to save the file as logs-insights-results.csv Looking at the Data Graphically Since we see that we have an Ingest Delay and a Timestamp Error, it would be good to look at this data graphically. Toward that end, I have create a python program called plotObservations.py which can be downloaded from my Github Repo. Copy this program to the EC2 instance that has Scapy Installed. wget https://raw.githubusercontent.com/Resistor52/hitc-benchmark-flow-logs/hitc-ep18/plotObservations.py Run the program as using: python3 ./plotObservations.py The program will read logs-insights-results.csv and will generate the four plots shown below. PLOT 1 - Packets 1000 to 2000 PLOT 2 - Packets 1020 to 1200 PLOT 3 - The First 10,000 Packets Delta - Seconds Between Missing Data I saved the charts generated from data captured in the exact same manner 24 hours prior to this set, and those are shown below for comparison purposes. PLOT 1b - Packets 1000 to 2000 (Previous Run) PLOT 3b - The First 10,000 Packets (Previous Run) Observations Ingest Delay - We can see the ingest delay varies from a little over 60 seconds to almost 11 minutes every 10 minutes. Note that when we configured the flow logs we used a setting of max-aggregation-interval 600. This test should be repeated using a max-aggregation-interval 60 to measure the impact on the Ingest Delay. Timestamp Error Seconds - The TimeStampDelaySec is calculated by subtracting the SentTimeEpoch from TimeStampEpoch. Chronologically, the SentTimeEpoch should occur first, as that is the timestamp of when Scapy sent the packet. Similarly, the TimeStampEpoch is when the VPC Flow Logs system detects the packet on the virtual network. I expected the error to be within milliseconds, but not almost a whole minute off! I am not sure how to explain this. At first, I thought that it might be a math error on my part. But we can take a look at a single packet as it was: Recorded in the flow log (logs-insights-results.csv), Sent by Scapy (test3A.csv), and Recorded by TCPdump at the Target System (tcpdump.pcap) We can use grep -E &#39;,-58,|srcPort&#39; logs-insights-results.csv | head -n6 | cut -d&quot;,&quot; -f5,6-9 to generate the following table: TimeStampEpoch SentTimeEpoch SentTime TimeStampDelaySec srcPort 1646334040 1646334098 2022-03-03 19:01:38.000 -58 4242 1646334099 1646334157 2022-03-03 19:02:37.000 -58 4301 1646334160 1646334218 2022-03-03 19:03:38.000 -58 4362 1646334219 1646334277 2022-03-03 19:04:37.000 -58 4421 1646334280 1646334338 2022-03-03 19:05:38.000 -58 4482 Selected packets from logs-insights-results.csv Now that we have identified the first five packets with the highest error (-58), we can look for those packets in the log generated by Scapy using grep -E &#39;&quot;4242&quot;|&quot;4301&quot;|&quot;4362&quot;|&quot;4421&quot;|&quot;4482&quot;&#39; test3A.csv Epoch_Time Date_Time Source_Port 1646334098 2022-03-03 19:01:38 UTC+0000 4242 1646334157 2022-03-03 19:02:37 UTC+0000 4301 1646334218 2022-03-03 19:03:38 UTC+0000 4362 1646334277 2022-03-03 19:04:37 UTC+0000 4421 1646334338 2022-03-03 19:05:38 UTC+0000 4482 Selected packets from test3A.csv We can see that the SentTimeEpoch as calculated by my CloudWatch query logic is performed correctly as it matches the Epoch_Time as logged by Scapy. For completeness, let’s look at TCP dump: tcpdump -tt -nn -r tcpdump.pcap &#39;src port 4242 or src port 4301 or src port 4362 or src port 4421 or src port 4482&#39; | cut -d&quot;:&quot; -f1 produces: 1646334098.569321 IP 34.201.148.40.4242 &gt; 172.31.48.150.7777 1646334157.873827 IP 34.201.148.40.4301 &gt; 172.31.48.150.7777 1646334218.182541 IP 34.201.148.40.4362 &gt; 172.31.48.150.7777 1646334277.489424 IP 34.201.148.40.4421 &gt; 172.31.48.150.7777 1646334338.805517 IP 34.201.148.40.4482 &gt; 172.31.48.150.7777 As expected, we can see that the epoch time recorded by TCPdump matches the other observations in the tables above. Missing Data - Looking at the Missing Data plots in the figures above initially raised some concerns. Is the VPC dropping packets? To investigate what is going on, I tweaked plotObservations.py to generate the “Delta” plot to show the periodicity of the missing data as well as to output the srcPorts for which data is missing. That output is shown below: The following srcPorts are missing: [4327, 4506, 4694, 4890, 5082, 5276, 5471, 5666, 5855, 6049, 6245, 6439, 6632, 6824, 7018, 7211, 7407, 7600, 7791, 7987, 8174, 8365, 8555, 8750, 8942, 9135, 9330, 9524, 9716, 9911, 10102, 10293, 10485, 10680, 10866, 11058, 11253, 11427, 11620, 11811, 12005, 12198, 12386, 12583, 12778, 12972, 13166, 13361, 13555, 13750, 13945, 14138] I also had plotObservations.py write the ports to a file, missingSrcPorts.csv so that we can grep for them: grep -f missingSrcPorts.csv test3A.csv Hmm, none of the missing srcPorts were logged by Scapy. The good news is that the VPC network did not drop any packets because all sent packets have been accounted for. The bad news is that there is bug in generate-test-traffic.py that occurs about every 195 seconds. I suspect that it is an issue with my generateTraffic function, most likely due to inaccuracies with time.sleep() compounded by variations in the amount of time it takes Scapy to write the packet out to the network. def generateTraffic(stop_time_epoch): datetime_TZ = datetime.now(tz_TZ) seconds = int(time.time()) # Current Epoch Time modulo = seconds % (2 ** 16) # Calculate the SrcPort datetime_time=datetime_TZ.strftime(&quot;%Y-%m-%d %H:%M:%S %Z%z&quot;) print(f&quot;Epoch Time: {seconds} Time: {datetime_time} srcPort: {modulo} {RemainingTimeString(stop_time_epoch)} &quot;, end = &quot;\r&quot;) write_str=&#39;&quot;&#39;+str(seconds)+&#39;&quot;,&quot;&#39;+datetime_time+&#39;&quot;,&quot;&#39;+str(modulo)+&#39;&quot;\n&#39; f.write(write_str) tcp=TCP(sport=modulo,dport=7777,flags=&quot;S&quot;,options=[(&#39;Timestamp&#39;,(0,0))]) pay=logfile+&quot; - &quot;+str(modulo) # Trap the output of scapy send so it does not print to console state = sys.stdout sys.stdout = open(&#39;/dev/null&#39;, &#39;w&#39;) send(ip/tcp/pay) # Scapy sys.stdout.close() sys.stdout = state # Wait one second between packets time.sleep(1) return OBJECTIVE 4 - Contemplate the implications of our findings Tests 1 and 2 demonstrate that we need to budget ample time ( at least 16 minutes) to allow AWS to provision the flow logs. We observed that flow logs sent to CloudWatch Logs provision faster than logs sent to a S3 bucket. Test 3 demonstrated that with a max-aggregation-interval set to “600,” it is possible to see an ingestion delay of up to 10.42 minutes (cat logs-insights-results.csv | cut -d&quot;,&quot; -f4 | sort -u -n | tail -n1) and that it is a function of how long ago the last aggregation occurred. The minimum ingestion delay was 1.26 minutes (cat logs-insights-results.csv | cut -d&quot;,&quot; -f4 | sort -u -n | head -n2). Lots of bad stuff could be detected late if you were expecting to use flow logs for real-time detection. I think that AWS would tell you that is not the use case that they were designed for. Instead use something like a Web Application Firewall (WAF). We observed an unaccounted-for error in the ‘@timestamp’ field generated by Cloud Watch Logs Insights of +1 to -60 seconds during our test and +60 to -58 during a previous test. This demonstrates the value of benchmarking critical security systems. In this experiment, we demonstrated a novel technique for timestamping packets that will be reduced to a flow log by using the source port field. We showed how to verify the accuracy of the technique by comparing the epoch time calculated from the source port with the time logged by TCPdump. We also demonstrated how to identify any missing packets using this technique and in the process identified a minor bug in generate-test-traffic.py that caused a packet not to be sent about every 195 seconds. This demonstrates (again) the importance of understanding the limitations of the security tools that we use. At some point, I will circle back and fix this bug. Remember that the limited amount of testing that was performed is not sufficient to draw too many conclusions. Tests 1 and 2 should be repeated multiple times at different times of day, over the course of several days, in different regions to get more precise statistics. Test 3 should be repeated multiple times and should consider the impact of changing the max-aggregation-interval setting. The more the testing process is automated the lower the burden of repeating each test. But, I think that I have taken this little experiment far enough for now. Wrap Up Wow, we covered quite a bit in this Episode: We made extensive use of the AWS CLI We used a Bash “while loop” to time AWS CLI calls We crafted packets using Scapy and read those packets using TCPdump. We also created a fairly sophisticated CloudWatch Logs Insights query, and Learned how we can use Python to visualize data. To learn more about TCPdump, Scapy, and packet crafting check out SANS SEC503: Intrusion Analysis In-Depth. To learn more about using Python to create security tools like generate-test-traffic.py consider SANS SEC573: Automating Information Security with Python. Closing Comments If you have thoughts or comments on today’s episode, feel free to chime in on the comments for this YouTube video. If you appreciate this video and want to see more like it, be sure to give it a “thumbs up.” Stay tuned for another installment of “Head in the Clouds” as announcements of new episodes are made on the SANS Cloud Security Twitter feed. Meanwhile, be sure to check out the other great videos on the SANS Cloud Security YouTube Channel. Take care." />
<meta property="og:description" content="18 - Benchmarking AWS Flow Logs Welcome to Episode 18 of the “Head in the Clouds” Video Series. I am Ken Hartman, a SANS Certified Instructor and content creator for the SANS Cloud Security Curriculum. Today’s episode is titled: “Benchmarking AWS Flow Logs” In many of the of the courses in the SANS Cloud Curriculum, we will have one or more labs that involves flow logs. However, a portion of the lab time is spent waiting for the flow logs to get provisioned. It’s kind of like watching and waiting for water to boil. Then, once the flow logs are provisioned, it takes time for the measured traffic to show up. Therefore, I thought it would be very interesting to determine what one should expect via experimentation and observation. Along the way, I made some very interesting discoveries, and I cannot wait to show them to you. Objectives Anytime we use a tool, technique, or service, we should be aware of its limitations and possible pitfalls. As such, here are the objectives for today: Experimentally determine the typical time to provision flow logs to an S3 bucket. Experimentally determine the typical time to provision flow logs to a CloudWatch logs group. Experimentally determine the typical delay for ongoing traffic to be captured in the flow log record. Contemplate the implications of our findings. Setup For this episode, we will assume that you have an AWS EC2 virtual machine running in a VPC that has a “Name” tag of “hitc-vpc.” The t2.micro type that is in the free tier will do just fine. I recommend that you use the Ubuntu 20.04 AMI for your region, as this will allow me to provide you with the exact commands to install some additional software later in this episode. I used Terraform to deploy my environment as described in the HitC Episode titled Episode 10 - Demonstration of Terraform Modules; Deploy a VM in AWS, Azure and GCP at once as I had originally planned to benchmark all three cloud service providers. However, as I got deeper and deeper into my research into AWS flow logs, I decided to stay focused on just AWS and may circle back to Azure and GCP flow logs in one or more future episodes. In this episode, we will be making extensive use of the AWS command line interface. This has two benefits: It will make it very easy for you to reproduce my work, and It will allow us to measure the time that it takes for things to happen. The machine on which you are running the CLI commands will need jq in addition to the AWS command line interface. OBJECTIVE 1: Experimentally determine the typical time to provision flow logs to an S3 bucket To start with, we need to set some variables that will be used by our commands: UNIQ_ID=$RANDOM; echo $UNIQ_ID ACCOUNT=$(aws sts get-caller-identity | jq -r .Account); echo $ACCOUNT YYYY=$(date -Idate | cut -d&quot;-&quot; -f1); echo $YYYY MM=$(date -Idate | cut -d&quot;-&quot; -f2); echo $MM DD=$(date -Idate | cut -d&quot;-&quot; -f3); echo $DD REGION=$(aws ec2 describe-availability-zones --output text --query &#39;AvailabilityZones[0].[RegionName]&#39;); echo $REGION We generate a unique ID to ensure that the S3 bucket that we create is unique. Linux has a variable called $RANDOM that we can use for this purpose. The YYYY, MM, and DD are captured so we can look into the bucket for our data. Lastly, we use a trick from StackOverflow.com to get the region, as yours may be different from mine. Next, we will create a variety of “helper commands.” Get the VPC ID To get the VPC ID of the VPC that is named “hitc-vpc” we can use: VPC_ID=$(aws ec2 describe-vpcs --filters Name=tag:Name,Values=hitc-vpc --query &quot;Vpcs[0].VpcId&quot; --output text); echo $VPC_ID Create the Bucket for the Flow Logs Next, we need to create an S3 bucket to receive our flow logs: aws s3 mb s3://hitc-benchmark-flowlogs-$UNIQ_ID Set up the Flow Logs for the VPC and Send to S3 Our next task is to make sure we have the command to set the flow logs for the VPC and send them to S3: aws ec2 create-flow-logs \ --resource-type VPC \ --resource-ids $VPC_ID \ --traffic-type ALL \ --log-destination-type s3 \ --log-destination arn:aws:s3:::hitc-benchmark-flowlogs-$UNIQ_ID \ --max-aggregation-interval 600 \ --tag-specifications &#39;ResourceType=vpc-flow-log,Tags=[{Key=Name,Value=hitc-flow-log}]&#39; Describe the Flow Logs for the VPC Having done that, we may find it helpful to be able to describe the flow logs for the VPC in question: aws ec2 describe-flow-logs --filter &quot;Name=resource-id,Values=$VPC_ID&quot; Delete the Flow Logs for the VPC Next, we need to be able to delete the flow logs that we just created. To do this we need to know the Flow Log ID. FLOW_LOG_ID=$(aws ec2 describe-flow-logs --filter &quot;Name=resource-id,Values=$VPC_ID&quot; --query &#39;FlowLogs[0].FlowLogId&#39; --output text) echo $FLOW_LOG_ID aws ec2 delete-flow-logs --flow-log-ids $FLOW_LOG_ID List the contents of the Bucket To verify the presence of a flow log, we will need to be able to drill deep into the S3 bucket. This is facilitated by the variables we set earlier. aws s3 ls s3://hitc-benchmark-flowlogs-$UNIQ_ID/AWSLogs/$ACCOUNT/vpcflowlogs/$REGION/$YYYY/$MM/$DD/ Empty the Bucket We also need the ability to empty the bucket between successive iterations of our test: aws s3 rm s3://hitc-benchmark-flowlogs-$UNIQ_ID --recursive Remove the Bucket It is also good idea to remove the bucket when we are done with our testing: aws s3 rb s3://hitc-benchmark-flowlogs-$UNIQ_ID Generate Flow Log Traffic Now that we’ve got our helper scripts figured out and tested, we need to generate some traffic to get captured by our flow log. SSH into your easy to instance and run the following command: while true ; do wget --timeout=10 http://1.1.1.1:81; sleep 20; done Note that I selected CloudFlare as the target as they are big enough that this would not be considered abusive and by sending the traffic to port 81, the retries will keep working until exhausted after 20 attempts and will then start over after 20 seconds. Anything that repetitively generates traffic will work here. TEST 1 - Time the provisioning of the flow Log until first flow log record in S3 With that done, we can now measure the time between the provisioning of the flow log until first flow log record in S3. Every 20 seconds we will check the S3 bucket to see if the flow log showed up yet. Our “TEST” checks to see if the length of response of the aws s3 ls command is zero. Once there is a flow log record in the bucket, the infinite loop will exit and we subtract the start time from the end time. Working in epoch time, makes this easy. NOTE: This assumes that the S3 bucket exists, but is empty # Set up the Flow Logs for the VPC aws ec2 create-flow-logs \ --resource-type VPC \ --resource-ids $VPC_ID \ --traffic-type ALL \ --log-destination-type s3 \ --log-destination arn:aws:s3:::hitc-benchmark-flowlogs-$UNIQ_ID \ --max-aggregation-interval 600 \ --tag-specifications &#39;ResourceType=vpc-flow-log,Tags=[{Key=Name,Value=hitc-flow-log}]&#39; # Watch for initial flow logs printf &quot;Waiting for the flow logs to apear in S3 bucket &quot; START_TIMER=$(date +%s) TEST=&quot;&quot; while [ -z &quot;$TEST&quot; ] ; do printf &quot;.&quot; sleep 20 TEST=$(aws s3 ls s3://hitc-benchmark-flowlogs-$UNIQ_ID/AWSLogs/$ACCOUNT/vpcflowlogs/$REGION/$YYYY/$MM/$DD/) done STOP_TIMER=$(date +%s) echo &quot;DONE&quot; echo $((STOP_TIMER-START_TIMER))&quot; seconds have elapsed&quot; While this script executes, it will print a dot to stdout every 20 seconds. When it is done, the output will look like this: 772 seconds have elapsed Ok, 772 seconds is 12.87 minutes. We now have our TEST 1 measurement. Note that this can be off by 20 seconds due to the sleep timer, which can be removed. Tear down TEST 1 To repeat the test, we need to delete the flow log and empty the S3 bucket, but leave the S3 bucket in place. # Delete the Flow Logs for the VPC FLOW_LOG_ID=$(aws ec2 describe-flow-logs --filter &quot;Name=resource-id,Values=$VPC_ID&quot; --query &#39;FlowLogs[0].FlowLogId&#39; --output text) echo $FLOW_LOG_ID aws ec2 delete-flow-logs --flow-log-ids $FLOW_LOG_ID # Empty the Bucket aws s3 rm s3://hitc-benchmark-flowlogs-$UNIQ_ID --recursive TEST 1 Summary This table shows the results of repeating the test 3 more times: Script Results Minutes 772 seconds have elapsed 12.86 Minutes 417 seconds have elapsed 6.97 Minutes 813 seconds have elapsed 13.55 Minutes 960 seconds have elapsed 16.00 Minutes Wow, it can take 16 minutes to get your first flow logs! So, what is up with this? Well, the catch phrase that the cloud service providers use is eventual consistency. We know that the big guys use queuing and focus intently on optimization, so I am guessing that it is for those reasons that there is a provisioning delay. Final Cleanup TEST 1 Now that we have obtained our results, make sure that you delete the flow log and empty the bucket again, using the last set of commands and then remove the bucket, as we no longer need it. # Remove the Bucket aws s3 rb s3://hitc-benchmark-flowlogs-$UNIQ_ID OBJECTIVE 2: Experimentally determine the typical time to provision flow logs to a Cloud Watch logs group Now, let’s take a look at Flow Logs sent to Cloud Watch Logs. Create the CloudWatch Logs Group First we need to create the CloudWatch Logs Group: aws logs create-log-group --log-group-name hitc-flow-logs Delete the Log Group And, here is the corresponding command to delete the log group: aws logs delete-log-group --log-group-name hitc-flow-logs Create the CloudWatch Logs Role Next, we need create the CloudWatch Logs Role. To do this we will create the Trust Policy JSON file, the Permissions Policy JSON file, and then attach them to a new role. Create the Trust Policy JSON file. I like to use the “heredoc” technique, so that everything can be scripted: cat &lt;&lt; EOF &gt; trust-policy.json { &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Effect&quot;: &quot;Allow&quot;, &quot;Principal&quot;: { &quot;Service&quot;: &quot;vpc-flow-logs.amazonaws.com&quot; }, &quot;Action&quot;: &quot;sts:AssumeRole&quot; } ] } EOF Create the Permissions Policy JSON file: cat &lt;&lt; EOF &gt; cw-flow-logs-policy.json { &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: [ &quot;logs:CreateLogGroup&quot;, &quot;logs:CreateLogStream&quot;, &quot;logs:PutLogEvents&quot;, &quot;logs:DescribeLogGroups&quot;, &quot;logs:DescribeLogStreams&quot; ], &quot;Resource&quot;: &quot;*&quot; } ] } EOF Next, run the following commands to create the policies and the role: aws iam create-policy --policy-name hitc-cw-flow-logs --policy-document file://cw-flow-logs-policy.json aws iam create-role --role-name hitc-cw-flow-logs --assume-role-policy-document file://trust-policy.json aws iam attach-role-policy --policy-arn &quot;arn:aws:iam::$ACCOUNT:policy/hitc-cw-flow-logs&quot; --role-name hitc-cw-flow-logs aws iam list-attached-role-policies --role-name hitc-cw-flow-logs TEST 2 - Time the provisioning of the flow Log until first flow log record in CloudWatch With our setup for Test 2 in place, we can now conduct it. This “TEST” checks to see if the length of response of the aws logs describe-log-streams command is zero. Once there is a flow log record in CW logs, the infinite loop will exit and we subtract the start time from the end time. NOTE: This assumes that the CloudWatch Log Group exists, but is empty. # Set up the Flow Logs for the VPC aws ec2 create-flow-logs \ --resource-type VPC \ --resource-ids $VPC_ID \ --traffic-type ALL \ --log-destination-type cloud-watch-logs \ --log-group-name hitc-flow-logs \ --max-aggregation-interval 600 \ --tag-specifications &#39;ResourceType=vpc-flow-log,Tags=[{Key=Name,Value=hitc-flow-log}]&#39; \ --deliver-logs-permission-arn &#39;arn:aws:iam::690634326977:role/hitc-cw-flow-logs&#39; # Watch for initial flow logs printf &quot;Waiting for the flow logs to appear in CW Logs Group &quot; START_TIMER=$(date +%s) TEST=&quot;&quot; while [ -z &quot;$TEST&quot; ] ; do printf &quot;.&quot; sleep 20 TEST=$(aws logs describe-log-streams --log-group-name hitc-flow-logs --output text) done STOP_TIMER=$(date +%s) echo &quot;DONE&quot; echo $((STOP_TIMER-START_TIMER))&quot; seconds have elapsed&quot; Tear down TEST 2 To re-run the test, delete the flow log and reset the CW Logs Group. # Delete the Flow Logs for the VPC FLOW_LOG_ID=$(aws ec2 describe-flow-logs --filter &quot;Name=resource-id,Values=$VPC_ID&quot; --query &#39;FlowLogs[0].FlowLogId&#39; --output text) echo $FLOW_LOG_ID aws ec2 delete-flow-logs --flow-log-ids $FLOW_LOG_ID # Delete and Recreate the CW Logs Group aws logs delete-log-group --log-group-name hitc-flow-logs aws logs create-log-group --log-group-name hitc-flow-logs Final Cleanup TEST 2 And when we are all done, we can run the following commands to delete the role and associated policy. But hold off for now as we will need this role for Test 3. # CLI commands to tear down the role and IAM policy aws iam detach-role-policy --role-name hitc-cw-flow-logs --policy-arn &quot;arn:aws:iam::$ACCOUNT:policy/hitc-cw-flow-logs&quot; aws iam delete-role --role-name hitc-cw-flow-logs aws iam delete-policy --policy-arn &quot;arn:aws:iam::$ACCOUNT:policy/hitc-cw-flow-logs&quot; TEST 2 Summary Script Results Minutes 21 seconds have elapsed 0.35 Minutes 541 seconds have elapsed 9.02 Minutes 522 seconds have elapsed 8.70 Minutes 375 seconds have elapsed 6.25 Minutes This is much better, but it can still take up to 10 minutes and is somewhat variable. OBJECTIVE 3: Experimentally determine the typical delay for ongoing traffic to be captured in the flow log record. Now, onto what is probably the most interesting and has the biggest implications for security monitoring. We want to know how long will it take for traffic that occurs in my VPC to show up as recorded in my flow logs? In other words, “I want to know how much lag there is in any detective control that I implement (that is based on flow logs).” The security implications of this are fairly significant. The Problem Statement Wouldn’t it be great to time stamp a packet, send it across the virtual network of our VPC, look at the packet in CloudWatch Logs, and determine how much delay there was between the time the packet was sent, and when it was recorded? That would be fantastic. The only problem is that a flow log contains only metadata, and not any payload. Here is a screenshot of what we have to work with: As it turns out, about the only thing that we have to fiddle with is the Source port and the Destination port. So, I will use the source port to encode the timestamp and send it to an unchanging port (TCP port 7777) at a target IP address outside of my VPC. Will this work? Let’s do some math here. We know that there are 65536 (2 to the power of 16) possible TCP ports. If we send one packet per second from a different port, we could do that for 1092 Minutes: &gt;&gt;&gt; (2 ** 16) / 60 1092.2666666666667 Or converting that to hours: &gt;&gt;&gt; (2 ** 16) / 60 / 60 18.204444444444444 We get 18 hours. Hence, we can send traffic at a rate of one packet per second from a unique source port for over 18 hours before using up all possible ports! Converting back and forth from the source port to the epoch time To calculate which port to use, we will take the current epoch time and divide it by 65536 and then take the integer portion of the remainder and use that for the source port. The math function that returns the remainder is called “modulo” and is represented as a % in python source port = int(current_epoch_time % 65536) To convert back to the epoch time from the source port, take the current epoch time and divide it by 65536 and then keep the quotient portion. Divide the source port by 65536 and take that result and add it to the quotient. Multiply the new total by 65536 and the new result is the epoch time of when the data was sent. epoch_timestamp = int((modulo / 65536.0 + int(current_epoch_time / 65536)) * 65536) This conversion assumes that the current epoch time is within the 18 hour window represented by the source port value. Introducing Scapy To generate a packet per second with an ever increasing source port that represents the time that the packet was sent, I used scapy. Scapy is written as a python module and is ideal for programmatically crafting packets. As we will be running Scapy on our EC2 instance to send traffic out of our VPC, we need to install Scapy, its dependencies, and the additional modules my python program will use: sudo apt update sudo apt install -y python3-pip tcpdump python3-matplotlib sudo pip3 install pytz sudo pip install --pre scapy[complete] Note that using sudo with pip to install software is generally a bad idea. See this link for a detailed discussion of the security issues. However, in this case, Scapy needs to run as root to be able to send packets, and we will destroy our EC2 instance at the end of our experimentation. To launch Scapy, just type sudo scapy at the Ubuntu command line as shown below: ubuntu@ip-10-0-1-112:~$ sudo scapy INFO: PyX dependencies are not installed ! Please install TexLive or MikTeX. aSPY//YASa apyyyyCY//////////YCa | sY//////YSpcs scpCY//Pp | Welcome to Scapy ayp ayyyyyyySCP//Pp syY//C | Version 2.4.5 AYAsAYYYYYYYY///Ps cY//S | pCCCCY//p cSSps y//Y | https://github.com/secdev/scapy SPPPP///a pP///AC//Y | A//A cyP////C | Have fun! p///Ac sC///a | P////YCpc A//A | We are in France, we say Skappee. scccccp///pSP///p p//Y | OK? Merci. sY/////////y caa S//P | -- Sebastien Chabal cayCyayP//Ya pY/Ya | sY/PsY////YCc aC//Yp sc sccaCY//PCypaapyCP//YSs spCPY//////YPSps ccaacs using IPython 8.1.1 &gt;&gt;&gt; I set up a target VM in another VPC with the security group set to allow inbound TCP port 7777 from the system with Scapy installed. As a test, I am running TCPdump on the target machine, using the following command: sudo tcpdump -i eth0 &#39;port 7777&#39; Next, use Scapy to send the target a packet. Enter the following commands at the Scapy IPython prompt, but substitute my target IP address (35.168.8.211) for the one you are using: ip=IP(dst=&quot;35.168.8.211&quot;) tcp=TCP(sport=1111,dport=7777,flags=&quot;S&quot;,options=[(&#39;Timestamp&#39;,(0,0))]) pay=&quot;This is Test3&quot; test3=ip/tcp/pay send(test3) The result should look like: . Sent 1 packets. NOTE: If you get a “Operation not permitted” error, you forgot to run Scapy as root Exit back to the Ubuntu command line using exit() Looking over at the target system, we can see that TCPdump generated the following two records: 22:25:43.461121 IP ec2-34-201-148-40.compute-1.amazonaws.com.1111 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13 22:25:43.461156 IP ip-172-31-48-150.ec2.internal.7777 &gt; ec2-34-201-148-40.compute-1.amazonaws.com.1111: Flags [R.], seq 0, ack 14, win 0, length 0 We Sent a TCP “Syn” packet and a “Reset” was sent back because there is not a service listening on port 7777. Putting the theory to work Ok, so now that we know the Scapy commands to craft a packet. We can write a program to determine the Source Port to use, to craft the packet and sent it. We also can prove that the packet was received by the target system and see the timestamp printed to standard out by TCPdump. The program is called generate-test-traffic.py and can be downloaded from my Github Repo Copy this program to the EC2 instance that has Scapy Installed. wget https://raw.githubusercontent.com/Resistor52/hitc-benchmark-flow-logs/hitc-ep18/generate-test-traffic.py Run the program as root using sudo python3 ./generate-test-traffic.py Once the program runs, it will ask four questions as shown below and then start to send packets at a rate of one per second. ubuntu@ip-10-0-1-112:~$ sudo python3 ./generate-test-traffic.py Enter the name for the CSV log file (without the csv extension): test3A Enter the desired target IP aggress in the format &quot;111.111.111.111&quot;: 35.168.8.211 Enter the desired start time in the format &quot;HH:MM&quot; for local time zone: 19:00 Enter the desired end time in the format &quot;HH:MM&quot; for local time zone: 22:00 Traffic generation will start at: 19:00 Traffic generation will continue until: 22:00 **************** Start of Traffic ************************** .Epoch Time: 1646344800 Time: 2022-03-03 22:00:00 UTC+0000 srcPort: 14944 Remaining Time: 0 Seconds ***************** End of Traffic *************************** In the example above, I am calling the log file test3A and provided 35.168.8.211 as the target IP. (Again, be sure to use the IP address of the target that you set up and has TCPdump listening.) I started the program at 19:00 hours UTC, simply because that was on the hour, and had it run for 3 hours, ending at 22:00. The program uses UTC because that is the time zone that AWS uses for all its cloud services, including EC2. Now, while we are waiting for the traffic generation to start, rerun TCPdump on the target system, this time capturing the packets to a file: sudo tcpdump -i eth0 &#39;port 7777&#39; -w tcpdump.pcap Initial Look at the Data The generate-test-traffic.py script created a log, which I named test3A.csv. Note that the script adds the “csv” extension to the filename. The top 10 rows of the file are shown below: &quot;Epoch_Time&quot;,&quot;Date_Time&quot;,&quot;Source_Port&quot; &quot;1646334000&quot;,&quot;2022-03-03 19:00:00 UTC+0000&quot;,&quot;4144&quot; &quot;1646334001&quot;,&quot;2022-03-03 19:00:01 UTC+0000&quot;,&quot;4145&quot; &quot;1646334002&quot;,&quot;2022-03-03 19:00:02 UTC+0000&quot;,&quot;4146&quot; &quot;1646334003&quot;,&quot;2022-03-03 19:00:03 UTC+0000&quot;,&quot;4147&quot; &quot;1646334004&quot;,&quot;2022-03-03 19:00:04 UTC+0000&quot;,&quot;4148&quot; &quot;1646334005&quot;,&quot;2022-03-03 19:00:05 UTC+0000&quot;,&quot;4149&quot; &quot;1646334006&quot;,&quot;2022-03-03 19:00:06 UTC+0000&quot;,&quot;4150&quot; &quot;1646334007&quot;,&quot;2022-03-03 19:00:07 UTC+0000&quot;,&quot;4151&quot; &quot;1646334008&quot;,&quot;2022-03-03 19:00:08 UTC+0000&quot;,&quot;4152&quot; We can look at the first 10 packets received by the Target system using the following tcpdump command: tcpdump -r tcpdump.pcap -nn -c10 &#39;dst port 7777&#39; And the output is: 19:00:00.013928 IP ec2-34-201-148-40.compute-1.amazonaws.com.4144 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13 19:00:01.018892 IP ec2-34-201-148-40.compute-1.amazonaws.com.4145 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13 19:00:02.023309 IP ec2-34-201-148-40.compute-1.amazonaws.com.4146 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13 19:00:03.027319 IP ec2-34-201-148-40.compute-1.amazonaws.com.4147 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13 19:00:04.032328 IP ec2-34-201-148-40.compute-1.amazonaws.com.4148 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13 19:00:05.050076 IP ec2-34-201-148-40.compute-1.amazonaws.com.4149 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13 19:00:06.061365 IP ec2-34-201-148-40.compute-1.amazonaws.com.4150 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13 19:00:07.066456 IP ec2-34-201-148-40.compute-1.amazonaws.com.4151 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13 19:00:08.071536 IP ec2-34-201-148-40.compute-1.amazonaws.com.4152 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13 19:00:09.077085 IP ec2-34-201-148-40.compute-1.amazonaws.com.4153 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13 We can extract the two critical data elements from packet capture using the following command: tcpdump -r tcpdump.pcap -nn -c10 &#39;dst port 7777&#39; | cut -d&quot; &quot; -f1,3 | cut -d&quot;.&quot; -f1,6 | tr &quot;.&quot; &quot; &quot; And this command will output the timestamp and the source port of the packet received by the target system: 19:00:00 4144 19:00:01 4145 19:00:02 4146 19:00:03 4147 19:00:04 4148 19:00:05 4149 19:00:06 4150 19:00:07 4151 19:00:08 4152 19:00:09 4153 Great, everything correlates! Now lets write the whole packet to a file: tcpdump -r tcpdump.pcap -nn &#39;dst port 7777&#39; | cut -d&quot; &quot; -f1,3 | cut -d&quot;.&quot; -f1,6 | tr &quot;.&quot; &quot; &quot; &gt; test3A.log We will save test3A.log for later, as it will come in handy. Looking in CloudWatch Logs The CloudWatch Logs Web Console is a great tool to slice and dice log data. Paste in the following query: fields @ingestionTime, @timestamp, ((toMillis(@ingestionTime) - toMillis(@timestamp))/1000) as IngestDelaySecs, ((toMillis(@ingestionTime) - toMillis(@timestamp))/1000/60) as IngestDelayMins, toMillis(@timestamp)/1000 as TimeStampEpoch, (65536*floor(TimeStampEpoch/65536))+srcPort as SentTimeEpoch, fromMillis(SentTimeEpoch * 1000) as SentTime, TimeStampEpoch-SentTimeEpoch as TimeStampDelaySec, srcPort | sort srcPort asc | limit 10000 | filter srcAddr = &quot;10.0.1.112&quot; and dstAddr = &quot;35.168.8.211&quot; and dstPort != &quot;22&quot; and srcPort != &quot;22&quot;) Be sure to set the time filter in the upper window to include the window in which the packets were sent. Looking at the output, we see that AWS provides two fields @ingestionTime and @timestamp so we can calculate the delay the Ingestion Delay Seconds (IngestDelaySecs) by converting both time values to Epoch time and subtracting them. But how accurate is the timestamp? Well, we can use the CloudWatch Logs Query Language to calculate a SentTimeEpoch based on the srcPort. We can also convert that to SentTime so it is human-readable. If we subtract the SentTimeEpoch from the TimeStampEpoch we get the TimeStampDelaySec. Disregard the first record as that was captured when I demonstrated how to use Scapy with IPython using an arbitrary source port. (I generated the traffic using the Python Script a few hours beforehand and that explains the chronology of this timestamp.) This record will be removed from the downloaded dataset. Another thing, that you may have noticed in the screenshot, is that the first srcPort is 4184 which is 40 seconds after we sent the packet with srcPort 4144! Very interesting. It looks like we have “negative delays.” or to phrase it more accurately, the @timestamp value attached by AWS has an error that fluctuates within a range. Use the “Export Results” button to save the file as logs-insights-results.csv Looking at the Data Graphically Since we see that we have an Ingest Delay and a Timestamp Error, it would be good to look at this data graphically. Toward that end, I have create a python program called plotObservations.py which can be downloaded from my Github Repo. Copy this program to the EC2 instance that has Scapy Installed. wget https://raw.githubusercontent.com/Resistor52/hitc-benchmark-flow-logs/hitc-ep18/plotObservations.py Run the program as using: python3 ./plotObservations.py The program will read logs-insights-results.csv and will generate the four plots shown below. PLOT 1 - Packets 1000 to 2000 PLOT 2 - Packets 1020 to 1200 PLOT 3 - The First 10,000 Packets Delta - Seconds Between Missing Data I saved the charts generated from data captured in the exact same manner 24 hours prior to this set, and those are shown below for comparison purposes. PLOT 1b - Packets 1000 to 2000 (Previous Run) PLOT 3b - The First 10,000 Packets (Previous Run) Observations Ingest Delay - We can see the ingest delay varies from a little over 60 seconds to almost 11 minutes every 10 minutes. Note that when we configured the flow logs we used a setting of max-aggregation-interval 600. This test should be repeated using a max-aggregation-interval 60 to measure the impact on the Ingest Delay. Timestamp Error Seconds - The TimeStampDelaySec is calculated by subtracting the SentTimeEpoch from TimeStampEpoch. Chronologically, the SentTimeEpoch should occur first, as that is the timestamp of when Scapy sent the packet. Similarly, the TimeStampEpoch is when the VPC Flow Logs system detects the packet on the virtual network. I expected the error to be within milliseconds, but not almost a whole minute off! I am not sure how to explain this. At first, I thought that it might be a math error on my part. But we can take a look at a single packet as it was: Recorded in the flow log (logs-insights-results.csv), Sent by Scapy (test3A.csv), and Recorded by TCPdump at the Target System (tcpdump.pcap) We can use grep -E &#39;,-58,|srcPort&#39; logs-insights-results.csv | head -n6 | cut -d&quot;,&quot; -f5,6-9 to generate the following table: TimeStampEpoch SentTimeEpoch SentTime TimeStampDelaySec srcPort 1646334040 1646334098 2022-03-03 19:01:38.000 -58 4242 1646334099 1646334157 2022-03-03 19:02:37.000 -58 4301 1646334160 1646334218 2022-03-03 19:03:38.000 -58 4362 1646334219 1646334277 2022-03-03 19:04:37.000 -58 4421 1646334280 1646334338 2022-03-03 19:05:38.000 -58 4482 Selected packets from logs-insights-results.csv Now that we have identified the first five packets with the highest error (-58), we can look for those packets in the log generated by Scapy using grep -E &#39;&quot;4242&quot;|&quot;4301&quot;|&quot;4362&quot;|&quot;4421&quot;|&quot;4482&quot;&#39; test3A.csv Epoch_Time Date_Time Source_Port 1646334098 2022-03-03 19:01:38 UTC+0000 4242 1646334157 2022-03-03 19:02:37 UTC+0000 4301 1646334218 2022-03-03 19:03:38 UTC+0000 4362 1646334277 2022-03-03 19:04:37 UTC+0000 4421 1646334338 2022-03-03 19:05:38 UTC+0000 4482 Selected packets from test3A.csv We can see that the SentTimeEpoch as calculated by my CloudWatch query logic is performed correctly as it matches the Epoch_Time as logged by Scapy. For completeness, let’s look at TCP dump: tcpdump -tt -nn -r tcpdump.pcap &#39;src port 4242 or src port 4301 or src port 4362 or src port 4421 or src port 4482&#39; | cut -d&quot;:&quot; -f1 produces: 1646334098.569321 IP 34.201.148.40.4242 &gt; 172.31.48.150.7777 1646334157.873827 IP 34.201.148.40.4301 &gt; 172.31.48.150.7777 1646334218.182541 IP 34.201.148.40.4362 &gt; 172.31.48.150.7777 1646334277.489424 IP 34.201.148.40.4421 &gt; 172.31.48.150.7777 1646334338.805517 IP 34.201.148.40.4482 &gt; 172.31.48.150.7777 As expected, we can see that the epoch time recorded by TCPdump matches the other observations in the tables above. Missing Data - Looking at the Missing Data plots in the figures above initially raised some concerns. Is the VPC dropping packets? To investigate what is going on, I tweaked plotObservations.py to generate the “Delta” plot to show the periodicity of the missing data as well as to output the srcPorts for which data is missing. That output is shown below: The following srcPorts are missing: [4327, 4506, 4694, 4890, 5082, 5276, 5471, 5666, 5855, 6049, 6245, 6439, 6632, 6824, 7018, 7211, 7407, 7600, 7791, 7987, 8174, 8365, 8555, 8750, 8942, 9135, 9330, 9524, 9716, 9911, 10102, 10293, 10485, 10680, 10866, 11058, 11253, 11427, 11620, 11811, 12005, 12198, 12386, 12583, 12778, 12972, 13166, 13361, 13555, 13750, 13945, 14138] I also had plotObservations.py write the ports to a file, missingSrcPorts.csv so that we can grep for them: grep -f missingSrcPorts.csv test3A.csv Hmm, none of the missing srcPorts were logged by Scapy. The good news is that the VPC network did not drop any packets because all sent packets have been accounted for. The bad news is that there is bug in generate-test-traffic.py that occurs about every 195 seconds. I suspect that it is an issue with my generateTraffic function, most likely due to inaccuracies with time.sleep() compounded by variations in the amount of time it takes Scapy to write the packet out to the network. def generateTraffic(stop_time_epoch): datetime_TZ = datetime.now(tz_TZ) seconds = int(time.time()) # Current Epoch Time modulo = seconds % (2 ** 16) # Calculate the SrcPort datetime_time=datetime_TZ.strftime(&quot;%Y-%m-%d %H:%M:%S %Z%z&quot;) print(f&quot;Epoch Time: {seconds} Time: {datetime_time} srcPort: {modulo} {RemainingTimeString(stop_time_epoch)} &quot;, end = &quot;\r&quot;) write_str=&#39;&quot;&#39;+str(seconds)+&#39;&quot;,&quot;&#39;+datetime_time+&#39;&quot;,&quot;&#39;+str(modulo)+&#39;&quot;\n&#39; f.write(write_str) tcp=TCP(sport=modulo,dport=7777,flags=&quot;S&quot;,options=[(&#39;Timestamp&#39;,(0,0))]) pay=logfile+&quot; - &quot;+str(modulo) # Trap the output of scapy send so it does not print to console state = sys.stdout sys.stdout = open(&#39;/dev/null&#39;, &#39;w&#39;) send(ip/tcp/pay) # Scapy sys.stdout.close() sys.stdout = state # Wait one second between packets time.sleep(1) return OBJECTIVE 4 - Contemplate the implications of our findings Tests 1 and 2 demonstrate that we need to budget ample time ( at least 16 minutes) to allow AWS to provision the flow logs. We observed that flow logs sent to CloudWatch Logs provision faster than logs sent to a S3 bucket. Test 3 demonstrated that with a max-aggregation-interval set to “600,” it is possible to see an ingestion delay of up to 10.42 minutes (cat logs-insights-results.csv | cut -d&quot;,&quot; -f4 | sort -u -n | tail -n1) and that it is a function of how long ago the last aggregation occurred. The minimum ingestion delay was 1.26 minutes (cat logs-insights-results.csv | cut -d&quot;,&quot; -f4 | sort -u -n | head -n2). Lots of bad stuff could be detected late if you were expecting to use flow logs for real-time detection. I think that AWS would tell you that is not the use case that they were designed for. Instead use something like a Web Application Firewall (WAF). We observed an unaccounted-for error in the ‘@timestamp’ field generated by Cloud Watch Logs Insights of +1 to -60 seconds during our test and +60 to -58 during a previous test. This demonstrates the value of benchmarking critical security systems. In this experiment, we demonstrated a novel technique for timestamping packets that will be reduced to a flow log by using the source port field. We showed how to verify the accuracy of the technique by comparing the epoch time calculated from the source port with the time logged by TCPdump. We also demonstrated how to identify any missing packets using this technique and in the process identified a minor bug in generate-test-traffic.py that caused a packet not to be sent about every 195 seconds. This demonstrates (again) the importance of understanding the limitations of the security tools that we use. At some point, I will circle back and fix this bug. Remember that the limited amount of testing that was performed is not sufficient to draw too many conclusions. Tests 1 and 2 should be repeated multiple times at different times of day, over the course of several days, in different regions to get more precise statistics. Test 3 should be repeated multiple times and should consider the impact of changing the max-aggregation-interval setting. The more the testing process is automated the lower the burden of repeating each test. But, I think that I have taken this little experiment far enough for now. Wrap Up Wow, we covered quite a bit in this Episode: We made extensive use of the AWS CLI We used a Bash “while loop” to time AWS CLI calls We crafted packets using Scapy and read those packets using TCPdump. We also created a fairly sophisticated CloudWatch Logs Insights query, and Learned how we can use Python to visualize data. To learn more about TCPdump, Scapy, and packet crafting check out SANS SEC503: Intrusion Analysis In-Depth. To learn more about using Python to create security tools like generate-test-traffic.py consider SANS SEC573: Automating Information Security with Python. Closing Comments If you have thoughts or comments on today’s episode, feel free to chime in on the comments for this YouTube video. If you appreciate this video and want to see more like it, be sure to give it a “thumbs up.” Stay tuned for another installment of “Head in the Clouds” as announcements of new episodes are made on the SANS Cloud Security Twitter feed. Meanwhile, be sure to check out the other great videos on the SANS Cloud Security YouTube Channel. Take care." />
<link rel="canonical" href="http://localhost:4000/episodes/episode18/" />
<meta property="og:url" content="http://localhost:4000/episodes/episode18/" />
<meta property="og:site_name" content="Head in the Clouds" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-05-24T15:17:05-03:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Episode 18 - Benchmarking AWS Flow Logs" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-05-24T14:59:40-03:00","datePublished":"2022-05-24T15:17:05-03:00","description":"18 - Benchmarking AWS Flow Logs Welcome to Episode 18 of the “Head in the Clouds” Video Series. I am Ken Hartman, a SANS Certified Instructor and content creator for the SANS Cloud Security Curriculum. Today’s episode is titled: “Benchmarking AWS Flow Logs” In many of the of the courses in the SANS Cloud Curriculum, we will have one or more labs that involves flow logs. However, a portion of the lab time is spent waiting for the flow logs to get provisioned. It’s kind of like watching and waiting for water to boil. Then, once the flow logs are provisioned, it takes time for the measured traffic to show up. Therefore, I thought it would be very interesting to determine what one should expect via experimentation and observation. Along the way, I made some very interesting discoveries, and I cannot wait to show them to you. Objectives Anytime we use a tool, technique, or service, we should be aware of its limitations and possible pitfalls. As such, here are the objectives for today: Experimentally determine the typical time to provision flow logs to an S3 bucket. Experimentally determine the typical time to provision flow logs to a CloudWatch logs group. Experimentally determine the typical delay for ongoing traffic to be captured in the flow log record. Contemplate the implications of our findings. Setup For this episode, we will assume that you have an AWS EC2 virtual machine running in a VPC that has a “Name” tag of “hitc-vpc.” The t2.micro type that is in the free tier will do just fine. I recommend that you use the Ubuntu 20.04 AMI for your region, as this will allow me to provide you with the exact commands to install some additional software later in this episode. I used Terraform to deploy my environment as described in the HitC Episode titled Episode 10 - Demonstration of Terraform Modules; Deploy a VM in AWS, Azure and GCP at once as I had originally planned to benchmark all three cloud service providers. However, as I got deeper and deeper into my research into AWS flow logs, I decided to stay focused on just AWS and may circle back to Azure and GCP flow logs in one or more future episodes. In this episode, we will be making extensive use of the AWS command line interface. This has two benefits: It will make it very easy for you to reproduce my work, and It will allow us to measure the time that it takes for things to happen. The machine on which you are running the CLI commands will need jq in addition to the AWS command line interface. OBJECTIVE 1: Experimentally determine the typical time to provision flow logs to an S3 bucket To start with, we need to set some variables that will be used by our commands: UNIQ_ID=$RANDOM; echo $UNIQ_ID ACCOUNT=$(aws sts get-caller-identity | jq -r .Account); echo $ACCOUNT YYYY=$(date -Idate | cut -d&quot;-&quot; -f1); echo $YYYY MM=$(date -Idate | cut -d&quot;-&quot; -f2); echo $MM DD=$(date -Idate | cut -d&quot;-&quot; -f3); echo $DD REGION=$(aws ec2 describe-availability-zones --output text --query &#39;AvailabilityZones[0].[RegionName]&#39;); echo $REGION We generate a unique ID to ensure that the S3 bucket that we create is unique. Linux has a variable called $RANDOM that we can use for this purpose. The YYYY, MM, and DD are captured so we can look into the bucket for our data. Lastly, we use a trick from StackOverflow.com to get the region, as yours may be different from mine. Next, we will create a variety of “helper commands.” Get the VPC ID To get the VPC ID of the VPC that is named “hitc-vpc” we can use: VPC_ID=$(aws ec2 describe-vpcs --filters Name=tag:Name,Values=hitc-vpc --query &quot;Vpcs[0].VpcId&quot; --output text); echo $VPC_ID Create the Bucket for the Flow Logs Next, we need to create an S3 bucket to receive our flow logs: aws s3 mb s3://hitc-benchmark-flowlogs-$UNIQ_ID Set up the Flow Logs for the VPC and Send to S3 Our next task is to make sure we have the command to set the flow logs for the VPC and send them to S3: aws ec2 create-flow-logs \\ --resource-type VPC \\ --resource-ids $VPC_ID \\ --traffic-type ALL \\ --log-destination-type s3 \\ --log-destination arn:aws:s3:::hitc-benchmark-flowlogs-$UNIQ_ID \\ --max-aggregation-interval 600 \\ --tag-specifications &#39;ResourceType=vpc-flow-log,Tags=[{Key=Name,Value=hitc-flow-log}]&#39; Describe the Flow Logs for the VPC Having done that, we may find it helpful to be able to describe the flow logs for the VPC in question: aws ec2 describe-flow-logs --filter &quot;Name=resource-id,Values=$VPC_ID&quot; Delete the Flow Logs for the VPC Next, we need to be able to delete the flow logs that we just created. To do this we need to know the Flow Log ID. FLOW_LOG_ID=$(aws ec2 describe-flow-logs --filter &quot;Name=resource-id,Values=$VPC_ID&quot; --query &#39;FlowLogs[0].FlowLogId&#39; --output text) echo $FLOW_LOG_ID aws ec2 delete-flow-logs --flow-log-ids $FLOW_LOG_ID List the contents of the Bucket To verify the presence of a flow log, we will need to be able to drill deep into the S3 bucket. This is facilitated by the variables we set earlier. aws s3 ls s3://hitc-benchmark-flowlogs-$UNIQ_ID/AWSLogs/$ACCOUNT/vpcflowlogs/$REGION/$YYYY/$MM/$DD/ Empty the Bucket We also need the ability to empty the bucket between successive iterations of our test: aws s3 rm s3://hitc-benchmark-flowlogs-$UNIQ_ID --recursive Remove the Bucket It is also good idea to remove the bucket when we are done with our testing: aws s3 rb s3://hitc-benchmark-flowlogs-$UNIQ_ID Generate Flow Log Traffic Now that we’ve got our helper scripts figured out and tested, we need to generate some traffic to get captured by our flow log. SSH into your easy to instance and run the following command: while true ; do wget --timeout=10 http://1.1.1.1:81; sleep 20; done Note that I selected CloudFlare as the target as they are big enough that this would not be considered abusive and by sending the traffic to port 81, the retries will keep working until exhausted after 20 attempts and will then start over after 20 seconds. Anything that repetitively generates traffic will work here. TEST 1 - Time the provisioning of the flow Log until first flow log record in S3 With that done, we can now measure the time between the provisioning of the flow log until first flow log record in S3. Every 20 seconds we will check the S3 bucket to see if the flow log showed up yet. Our “TEST” checks to see if the length of response of the aws s3 ls command is zero. Once there is a flow log record in the bucket, the infinite loop will exit and we subtract the start time from the end time. Working in epoch time, makes this easy. NOTE: This assumes that the S3 bucket exists, but is empty # Set up the Flow Logs for the VPC aws ec2 create-flow-logs \\ --resource-type VPC \\ --resource-ids $VPC_ID \\ --traffic-type ALL \\ --log-destination-type s3 \\ --log-destination arn:aws:s3:::hitc-benchmark-flowlogs-$UNIQ_ID \\ --max-aggregation-interval 600 \\ --tag-specifications &#39;ResourceType=vpc-flow-log,Tags=[{Key=Name,Value=hitc-flow-log}]&#39; # Watch for initial flow logs printf &quot;Waiting for the flow logs to apear in S3 bucket &quot; START_TIMER=$(date +%s) TEST=&quot;&quot; while [ -z &quot;$TEST&quot; ] ; do printf &quot;.&quot; sleep 20 TEST=$(aws s3 ls s3://hitc-benchmark-flowlogs-$UNIQ_ID/AWSLogs/$ACCOUNT/vpcflowlogs/$REGION/$YYYY/$MM/$DD/) done STOP_TIMER=$(date +%s) echo &quot;DONE&quot; echo $((STOP_TIMER-START_TIMER))&quot; seconds have elapsed&quot; While this script executes, it will print a dot to stdout every 20 seconds. When it is done, the output will look like this: 772 seconds have elapsed Ok, 772 seconds is 12.87 minutes. We now have our TEST 1 measurement. Note that this can be off by 20 seconds due to the sleep timer, which can be removed. Tear down TEST 1 To repeat the test, we need to delete the flow log and empty the S3 bucket, but leave the S3 bucket in place. # Delete the Flow Logs for the VPC FLOW_LOG_ID=$(aws ec2 describe-flow-logs --filter &quot;Name=resource-id,Values=$VPC_ID&quot; --query &#39;FlowLogs[0].FlowLogId&#39; --output text) echo $FLOW_LOG_ID aws ec2 delete-flow-logs --flow-log-ids $FLOW_LOG_ID # Empty the Bucket aws s3 rm s3://hitc-benchmark-flowlogs-$UNIQ_ID --recursive TEST 1 Summary This table shows the results of repeating the test 3 more times: Script Results Minutes 772 seconds have elapsed 12.86 Minutes 417 seconds have elapsed 6.97 Minutes 813 seconds have elapsed 13.55 Minutes 960 seconds have elapsed 16.00 Minutes Wow, it can take 16 minutes to get your first flow logs! So, what is up with this? Well, the catch phrase that the cloud service providers use is eventual consistency. We know that the big guys use queuing and focus intently on optimization, so I am guessing that it is for those reasons that there is a provisioning delay. Final Cleanup TEST 1 Now that we have obtained our results, make sure that you delete the flow log and empty the bucket again, using the last set of commands and then remove the bucket, as we no longer need it. # Remove the Bucket aws s3 rb s3://hitc-benchmark-flowlogs-$UNIQ_ID OBJECTIVE 2: Experimentally determine the typical time to provision flow logs to a Cloud Watch logs group Now, let’s take a look at Flow Logs sent to Cloud Watch Logs. Create the CloudWatch Logs Group First we need to create the CloudWatch Logs Group: aws logs create-log-group --log-group-name hitc-flow-logs Delete the Log Group And, here is the corresponding command to delete the log group: aws logs delete-log-group --log-group-name hitc-flow-logs Create the CloudWatch Logs Role Next, we need create the CloudWatch Logs Role. To do this we will create the Trust Policy JSON file, the Permissions Policy JSON file, and then attach them to a new role. Create the Trust Policy JSON file. I like to use the “heredoc” technique, so that everything can be scripted: cat &lt;&lt; EOF &gt; trust-policy.json { &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Effect&quot;: &quot;Allow&quot;, &quot;Principal&quot;: { &quot;Service&quot;: &quot;vpc-flow-logs.amazonaws.com&quot; }, &quot;Action&quot;: &quot;sts:AssumeRole&quot; } ] } EOF Create the Permissions Policy JSON file: cat &lt;&lt; EOF &gt; cw-flow-logs-policy.json { &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: [ &quot;logs:CreateLogGroup&quot;, &quot;logs:CreateLogStream&quot;, &quot;logs:PutLogEvents&quot;, &quot;logs:DescribeLogGroups&quot;, &quot;logs:DescribeLogStreams&quot; ], &quot;Resource&quot;: &quot;*&quot; } ] } EOF Next, run the following commands to create the policies and the role: aws iam create-policy --policy-name hitc-cw-flow-logs --policy-document file://cw-flow-logs-policy.json aws iam create-role --role-name hitc-cw-flow-logs --assume-role-policy-document file://trust-policy.json aws iam attach-role-policy --policy-arn &quot;arn:aws:iam::$ACCOUNT:policy/hitc-cw-flow-logs&quot; --role-name hitc-cw-flow-logs aws iam list-attached-role-policies --role-name hitc-cw-flow-logs TEST 2 - Time the provisioning of the flow Log until first flow log record in CloudWatch With our setup for Test 2 in place, we can now conduct it. This “TEST” checks to see if the length of response of the aws logs describe-log-streams command is zero. Once there is a flow log record in CW logs, the infinite loop will exit and we subtract the start time from the end time. NOTE: This assumes that the CloudWatch Log Group exists, but is empty. # Set up the Flow Logs for the VPC aws ec2 create-flow-logs \\ --resource-type VPC \\ --resource-ids $VPC_ID \\ --traffic-type ALL \\ --log-destination-type cloud-watch-logs \\ --log-group-name hitc-flow-logs \\ --max-aggregation-interval 600 \\ --tag-specifications &#39;ResourceType=vpc-flow-log,Tags=[{Key=Name,Value=hitc-flow-log}]&#39; \\ --deliver-logs-permission-arn &#39;arn:aws:iam::690634326977:role/hitc-cw-flow-logs&#39; # Watch for initial flow logs printf &quot;Waiting for the flow logs to appear in CW Logs Group &quot; START_TIMER=$(date +%s) TEST=&quot;&quot; while [ -z &quot;$TEST&quot; ] ; do printf &quot;.&quot; sleep 20 TEST=$(aws logs describe-log-streams --log-group-name hitc-flow-logs --output text) done STOP_TIMER=$(date +%s) echo &quot;DONE&quot; echo $((STOP_TIMER-START_TIMER))&quot; seconds have elapsed&quot; Tear down TEST 2 To re-run the test, delete the flow log and reset the CW Logs Group. # Delete the Flow Logs for the VPC FLOW_LOG_ID=$(aws ec2 describe-flow-logs --filter &quot;Name=resource-id,Values=$VPC_ID&quot; --query &#39;FlowLogs[0].FlowLogId&#39; --output text) echo $FLOW_LOG_ID aws ec2 delete-flow-logs --flow-log-ids $FLOW_LOG_ID # Delete and Recreate the CW Logs Group aws logs delete-log-group --log-group-name hitc-flow-logs aws logs create-log-group --log-group-name hitc-flow-logs Final Cleanup TEST 2 And when we are all done, we can run the following commands to delete the role and associated policy. But hold off for now as we will need this role for Test 3. # CLI commands to tear down the role and IAM policy aws iam detach-role-policy --role-name hitc-cw-flow-logs --policy-arn &quot;arn:aws:iam::$ACCOUNT:policy/hitc-cw-flow-logs&quot; aws iam delete-role --role-name hitc-cw-flow-logs aws iam delete-policy --policy-arn &quot;arn:aws:iam::$ACCOUNT:policy/hitc-cw-flow-logs&quot; TEST 2 Summary Script Results Minutes 21 seconds have elapsed 0.35 Minutes 541 seconds have elapsed 9.02 Minutes 522 seconds have elapsed 8.70 Minutes 375 seconds have elapsed 6.25 Minutes This is much better, but it can still take up to 10 minutes and is somewhat variable. OBJECTIVE 3: Experimentally determine the typical delay for ongoing traffic to be captured in the flow log record. Now, onto what is probably the most interesting and has the biggest implications for security monitoring. We want to know how long will it take for traffic that occurs in my VPC to show up as recorded in my flow logs? In other words, “I want to know how much lag there is in any detective control that I implement (that is based on flow logs).” The security implications of this are fairly significant. The Problem Statement Wouldn’t it be great to time stamp a packet, send it across the virtual network of our VPC, look at the packet in CloudWatch Logs, and determine how much delay there was between the time the packet was sent, and when it was recorded? That would be fantastic. The only problem is that a flow log contains only metadata, and not any payload. Here is a screenshot of what we have to work with: As it turns out, about the only thing that we have to fiddle with is the Source port and the Destination port. So, I will use the source port to encode the timestamp and send it to an unchanging port (TCP port 7777) at a target IP address outside of my VPC. Will this work? Let’s do some math here. We know that there are 65536 (2 to the power of 16) possible TCP ports. If we send one packet per second from a different port, we could do that for 1092 Minutes: &gt;&gt;&gt; (2 ** 16) / 60 1092.2666666666667 Or converting that to hours: &gt;&gt;&gt; (2 ** 16) / 60 / 60 18.204444444444444 We get 18 hours. Hence, we can send traffic at a rate of one packet per second from a unique source port for over 18 hours before using up all possible ports! Converting back and forth from the source port to the epoch time To calculate which port to use, we will take the current epoch time and divide it by 65536 and then take the integer portion of the remainder and use that for the source port. The math function that returns the remainder is called “modulo” and is represented as a % in python source port = int(current_epoch_time % 65536) To convert back to the epoch time from the source port, take the current epoch time and divide it by 65536 and then keep the quotient portion. Divide the source port by 65536 and take that result and add it to the quotient. Multiply the new total by 65536 and the new result is the epoch time of when the data was sent. epoch_timestamp = int((modulo / 65536.0 + int(current_epoch_time / 65536)) * 65536) This conversion assumes that the current epoch time is within the 18 hour window represented by the source port value. Introducing Scapy To generate a packet per second with an ever increasing source port that represents the time that the packet was sent, I used scapy. Scapy is written as a python module and is ideal for programmatically crafting packets. As we will be running Scapy on our EC2 instance to send traffic out of our VPC, we need to install Scapy, its dependencies, and the additional modules my python program will use: sudo apt update sudo apt install -y python3-pip tcpdump python3-matplotlib sudo pip3 install pytz sudo pip install --pre scapy[complete] Note that using sudo with pip to install software is generally a bad idea. See this link for a detailed discussion of the security issues. However, in this case, Scapy needs to run as root to be able to send packets, and we will destroy our EC2 instance at the end of our experimentation. To launch Scapy, just type sudo scapy at the Ubuntu command line as shown below: ubuntu@ip-10-0-1-112:~$ sudo scapy INFO: PyX dependencies are not installed ! Please install TexLive or MikTeX. aSPY//YASa apyyyyCY//////////YCa | sY//////YSpcs scpCY//Pp | Welcome to Scapy ayp ayyyyyyySCP//Pp syY//C | Version 2.4.5 AYAsAYYYYYYYY///Ps cY//S | pCCCCY//p cSSps y//Y | https://github.com/secdev/scapy SPPPP///a pP///AC//Y | A//A cyP////C | Have fun! p///Ac sC///a | P////YCpc A//A | We are in France, we say Skappee. scccccp///pSP///p p//Y | OK? Merci. sY/////////y caa S//P | -- Sebastien Chabal cayCyayP//Ya pY/Ya | sY/PsY////YCc aC//Yp sc sccaCY//PCypaapyCP//YSs spCPY//////YPSps ccaacs using IPython 8.1.1 &gt;&gt;&gt; I set up a target VM in another VPC with the security group set to allow inbound TCP port 7777 from the system with Scapy installed. As a test, I am running TCPdump on the target machine, using the following command: sudo tcpdump -i eth0 &#39;port 7777&#39; Next, use Scapy to send the target a packet. Enter the following commands at the Scapy IPython prompt, but substitute my target IP address (35.168.8.211) for the one you are using: ip=IP(dst=&quot;35.168.8.211&quot;) tcp=TCP(sport=1111,dport=7777,flags=&quot;S&quot;,options=[(&#39;Timestamp&#39;,(0,0))]) pay=&quot;This is Test3&quot; test3=ip/tcp/pay send(test3) The result should look like: . Sent 1 packets. NOTE: If you get a “Operation not permitted” error, you forgot to run Scapy as root Exit back to the Ubuntu command line using exit() Looking over at the target system, we can see that TCPdump generated the following two records: 22:25:43.461121 IP ec2-34-201-148-40.compute-1.amazonaws.com.1111 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13 22:25:43.461156 IP ip-172-31-48-150.ec2.internal.7777 &gt; ec2-34-201-148-40.compute-1.amazonaws.com.1111: Flags [R.], seq 0, ack 14, win 0, length 0 We Sent a TCP “Syn” packet and a “Reset” was sent back because there is not a service listening on port 7777. Putting the theory to work Ok, so now that we know the Scapy commands to craft a packet. We can write a program to determine the Source Port to use, to craft the packet and sent it. We also can prove that the packet was received by the target system and see the timestamp printed to standard out by TCPdump. The program is called generate-test-traffic.py and can be downloaded from my Github Repo Copy this program to the EC2 instance that has Scapy Installed. wget https://raw.githubusercontent.com/Resistor52/hitc-benchmark-flow-logs/hitc-ep18/generate-test-traffic.py Run the program as root using sudo python3 ./generate-test-traffic.py Once the program runs, it will ask four questions as shown below and then start to send packets at a rate of one per second. ubuntu@ip-10-0-1-112:~$ sudo python3 ./generate-test-traffic.py Enter the name for the CSV log file (without the csv extension): test3A Enter the desired target IP aggress in the format &quot;111.111.111.111&quot;: 35.168.8.211 Enter the desired start time in the format &quot;HH:MM&quot; for local time zone: 19:00 Enter the desired end time in the format &quot;HH:MM&quot; for local time zone: 22:00 Traffic generation will start at: 19:00 Traffic generation will continue until: 22:00 **************** Start of Traffic ************************** .Epoch Time: 1646344800 Time: 2022-03-03 22:00:00 UTC+0000 srcPort: 14944 Remaining Time: 0 Seconds ***************** End of Traffic *************************** In the example above, I am calling the log file test3A and provided 35.168.8.211 as the target IP. (Again, be sure to use the IP address of the target that you set up and has TCPdump listening.) I started the program at 19:00 hours UTC, simply because that was on the hour, and had it run for 3 hours, ending at 22:00. The program uses UTC because that is the time zone that AWS uses for all its cloud services, including EC2. Now, while we are waiting for the traffic generation to start, rerun TCPdump on the target system, this time capturing the packets to a file: sudo tcpdump -i eth0 &#39;port 7777&#39; -w tcpdump.pcap Initial Look at the Data The generate-test-traffic.py script created a log, which I named test3A.csv. Note that the script adds the “csv” extension to the filename. The top 10 rows of the file are shown below: &quot;Epoch_Time&quot;,&quot;Date_Time&quot;,&quot;Source_Port&quot; &quot;1646334000&quot;,&quot;2022-03-03 19:00:00 UTC+0000&quot;,&quot;4144&quot; &quot;1646334001&quot;,&quot;2022-03-03 19:00:01 UTC+0000&quot;,&quot;4145&quot; &quot;1646334002&quot;,&quot;2022-03-03 19:00:02 UTC+0000&quot;,&quot;4146&quot; &quot;1646334003&quot;,&quot;2022-03-03 19:00:03 UTC+0000&quot;,&quot;4147&quot; &quot;1646334004&quot;,&quot;2022-03-03 19:00:04 UTC+0000&quot;,&quot;4148&quot; &quot;1646334005&quot;,&quot;2022-03-03 19:00:05 UTC+0000&quot;,&quot;4149&quot; &quot;1646334006&quot;,&quot;2022-03-03 19:00:06 UTC+0000&quot;,&quot;4150&quot; &quot;1646334007&quot;,&quot;2022-03-03 19:00:07 UTC+0000&quot;,&quot;4151&quot; &quot;1646334008&quot;,&quot;2022-03-03 19:00:08 UTC+0000&quot;,&quot;4152&quot; We can look at the first 10 packets received by the Target system using the following tcpdump command: tcpdump -r tcpdump.pcap -nn -c10 &#39;dst port 7777&#39; And the output is: 19:00:00.013928 IP ec2-34-201-148-40.compute-1.amazonaws.com.4144 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13 19:00:01.018892 IP ec2-34-201-148-40.compute-1.amazonaws.com.4145 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13 19:00:02.023309 IP ec2-34-201-148-40.compute-1.amazonaws.com.4146 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13 19:00:03.027319 IP ec2-34-201-148-40.compute-1.amazonaws.com.4147 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13 19:00:04.032328 IP ec2-34-201-148-40.compute-1.amazonaws.com.4148 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13 19:00:05.050076 IP ec2-34-201-148-40.compute-1.amazonaws.com.4149 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13 19:00:06.061365 IP ec2-34-201-148-40.compute-1.amazonaws.com.4150 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13 19:00:07.066456 IP ec2-34-201-148-40.compute-1.amazonaws.com.4151 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13 19:00:08.071536 IP ec2-34-201-148-40.compute-1.amazonaws.com.4152 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13 19:00:09.077085 IP ec2-34-201-148-40.compute-1.amazonaws.com.4153 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13 We can extract the two critical data elements from packet capture using the following command: tcpdump -r tcpdump.pcap -nn -c10 &#39;dst port 7777&#39; | cut -d&quot; &quot; -f1,3 | cut -d&quot;.&quot; -f1,6 | tr &quot;.&quot; &quot; &quot; And this command will output the timestamp and the source port of the packet received by the target system: 19:00:00 4144 19:00:01 4145 19:00:02 4146 19:00:03 4147 19:00:04 4148 19:00:05 4149 19:00:06 4150 19:00:07 4151 19:00:08 4152 19:00:09 4153 Great, everything correlates! Now lets write the whole packet to a file: tcpdump -r tcpdump.pcap -nn &#39;dst port 7777&#39; | cut -d&quot; &quot; -f1,3 | cut -d&quot;.&quot; -f1,6 | tr &quot;.&quot; &quot; &quot; &gt; test3A.log We will save test3A.log for later, as it will come in handy. Looking in CloudWatch Logs The CloudWatch Logs Web Console is a great tool to slice and dice log data. Paste in the following query: fields @ingestionTime, @timestamp, ((toMillis(@ingestionTime) - toMillis(@timestamp))/1000) as IngestDelaySecs, ((toMillis(@ingestionTime) - toMillis(@timestamp))/1000/60) as IngestDelayMins, toMillis(@timestamp)/1000 as TimeStampEpoch, (65536*floor(TimeStampEpoch/65536))+srcPort as SentTimeEpoch, fromMillis(SentTimeEpoch * 1000) as SentTime, TimeStampEpoch-SentTimeEpoch as TimeStampDelaySec, srcPort | sort srcPort asc | limit 10000 | filter srcAddr = &quot;10.0.1.112&quot; and dstAddr = &quot;35.168.8.211&quot; and dstPort != &quot;22&quot; and srcPort != &quot;22&quot;) Be sure to set the time filter in the upper window to include the window in which the packets were sent. Looking at the output, we see that AWS provides two fields @ingestionTime and @timestamp so we can calculate the delay the Ingestion Delay Seconds (IngestDelaySecs) by converting both time values to Epoch time and subtracting them. But how accurate is the timestamp? Well, we can use the CloudWatch Logs Query Language to calculate a SentTimeEpoch based on the srcPort. We can also convert that to SentTime so it is human-readable. If we subtract the SentTimeEpoch from the TimeStampEpoch we get the TimeStampDelaySec. Disregard the first record as that was captured when I demonstrated how to use Scapy with IPython using an arbitrary source port. (I generated the traffic using the Python Script a few hours beforehand and that explains the chronology of this timestamp.) This record will be removed from the downloaded dataset. Another thing, that you may have noticed in the screenshot, is that the first srcPort is 4184 which is 40 seconds after we sent the packet with srcPort 4144! Very interesting. It looks like we have “negative delays.” or to phrase it more accurately, the @timestamp value attached by AWS has an error that fluctuates within a range. Use the “Export Results” button to save the file as logs-insights-results.csv Looking at the Data Graphically Since we see that we have an Ingest Delay and a Timestamp Error, it would be good to look at this data graphically. Toward that end, I have create a python program called plotObservations.py which can be downloaded from my Github Repo. Copy this program to the EC2 instance that has Scapy Installed. wget https://raw.githubusercontent.com/Resistor52/hitc-benchmark-flow-logs/hitc-ep18/plotObservations.py Run the program as using: python3 ./plotObservations.py The program will read logs-insights-results.csv and will generate the four plots shown below. PLOT 1 - Packets 1000 to 2000 PLOT 2 - Packets 1020 to 1200 PLOT 3 - The First 10,000 Packets Delta - Seconds Between Missing Data I saved the charts generated from data captured in the exact same manner 24 hours prior to this set, and those are shown below for comparison purposes. PLOT 1b - Packets 1000 to 2000 (Previous Run) PLOT 3b - The First 10,000 Packets (Previous Run) Observations Ingest Delay - We can see the ingest delay varies from a little over 60 seconds to almost 11 minutes every 10 minutes. Note that when we configured the flow logs we used a setting of max-aggregation-interval 600. This test should be repeated using a max-aggregation-interval 60 to measure the impact on the Ingest Delay. Timestamp Error Seconds - The TimeStampDelaySec is calculated by subtracting the SentTimeEpoch from TimeStampEpoch. Chronologically, the SentTimeEpoch should occur first, as that is the timestamp of when Scapy sent the packet. Similarly, the TimeStampEpoch is when the VPC Flow Logs system detects the packet on the virtual network. I expected the error to be within milliseconds, but not almost a whole minute off! I am not sure how to explain this. At first, I thought that it might be a math error on my part. But we can take a look at a single packet as it was: Recorded in the flow log (logs-insights-results.csv), Sent by Scapy (test3A.csv), and Recorded by TCPdump at the Target System (tcpdump.pcap) We can use grep -E &#39;,-58,|srcPort&#39; logs-insights-results.csv | head -n6 | cut -d&quot;,&quot; -f5,6-9 to generate the following table: TimeStampEpoch SentTimeEpoch SentTime TimeStampDelaySec srcPort 1646334040 1646334098 2022-03-03 19:01:38.000 -58 4242 1646334099 1646334157 2022-03-03 19:02:37.000 -58 4301 1646334160 1646334218 2022-03-03 19:03:38.000 -58 4362 1646334219 1646334277 2022-03-03 19:04:37.000 -58 4421 1646334280 1646334338 2022-03-03 19:05:38.000 -58 4482 Selected packets from logs-insights-results.csv Now that we have identified the first five packets with the highest error (-58), we can look for those packets in the log generated by Scapy using grep -E &#39;&quot;4242&quot;|&quot;4301&quot;|&quot;4362&quot;|&quot;4421&quot;|&quot;4482&quot;&#39; test3A.csv Epoch_Time Date_Time Source_Port 1646334098 2022-03-03 19:01:38 UTC+0000 4242 1646334157 2022-03-03 19:02:37 UTC+0000 4301 1646334218 2022-03-03 19:03:38 UTC+0000 4362 1646334277 2022-03-03 19:04:37 UTC+0000 4421 1646334338 2022-03-03 19:05:38 UTC+0000 4482 Selected packets from test3A.csv We can see that the SentTimeEpoch as calculated by my CloudWatch query logic is performed correctly as it matches the Epoch_Time as logged by Scapy. For completeness, let’s look at TCP dump: tcpdump -tt -nn -r tcpdump.pcap &#39;src port 4242 or src port 4301 or src port 4362 or src port 4421 or src port 4482&#39; | cut -d&quot;:&quot; -f1 produces: 1646334098.569321 IP 34.201.148.40.4242 &gt; 172.31.48.150.7777 1646334157.873827 IP 34.201.148.40.4301 &gt; 172.31.48.150.7777 1646334218.182541 IP 34.201.148.40.4362 &gt; 172.31.48.150.7777 1646334277.489424 IP 34.201.148.40.4421 &gt; 172.31.48.150.7777 1646334338.805517 IP 34.201.148.40.4482 &gt; 172.31.48.150.7777 As expected, we can see that the epoch time recorded by TCPdump matches the other observations in the tables above. Missing Data - Looking at the Missing Data plots in the figures above initially raised some concerns. Is the VPC dropping packets? To investigate what is going on, I tweaked plotObservations.py to generate the “Delta” plot to show the periodicity of the missing data as well as to output the srcPorts for which data is missing. That output is shown below: The following srcPorts are missing: [4327, 4506, 4694, 4890, 5082, 5276, 5471, 5666, 5855, 6049, 6245, 6439, 6632, 6824, 7018, 7211, 7407, 7600, 7791, 7987, 8174, 8365, 8555, 8750, 8942, 9135, 9330, 9524, 9716, 9911, 10102, 10293, 10485, 10680, 10866, 11058, 11253, 11427, 11620, 11811, 12005, 12198, 12386, 12583, 12778, 12972, 13166, 13361, 13555, 13750, 13945, 14138] I also had plotObservations.py write the ports to a file, missingSrcPorts.csv so that we can grep for them: grep -f missingSrcPorts.csv test3A.csv Hmm, none of the missing srcPorts were logged by Scapy. The good news is that the VPC network did not drop any packets because all sent packets have been accounted for. The bad news is that there is bug in generate-test-traffic.py that occurs about every 195 seconds. I suspect that it is an issue with my generateTraffic function, most likely due to inaccuracies with time.sleep() compounded by variations in the amount of time it takes Scapy to write the packet out to the network. def generateTraffic(stop_time_epoch): datetime_TZ = datetime.now(tz_TZ) seconds = int(time.time()) # Current Epoch Time modulo = seconds % (2 ** 16) # Calculate the SrcPort datetime_time=datetime_TZ.strftime(&quot;%Y-%m-%d %H:%M:%S %Z%z&quot;) print(f&quot;Epoch Time: {seconds} Time: {datetime_time} srcPort: {modulo} {RemainingTimeString(stop_time_epoch)} &quot;, end = &quot;\\r&quot;) write_str=&#39;&quot;&#39;+str(seconds)+&#39;&quot;,&quot;&#39;+datetime_time+&#39;&quot;,&quot;&#39;+str(modulo)+&#39;&quot;\\n&#39; f.write(write_str) tcp=TCP(sport=modulo,dport=7777,flags=&quot;S&quot;,options=[(&#39;Timestamp&#39;,(0,0))]) pay=logfile+&quot; - &quot;+str(modulo) # Trap the output of scapy send so it does not print to console state = sys.stdout sys.stdout = open(&#39;/dev/null&#39;, &#39;w&#39;) send(ip/tcp/pay) # Scapy sys.stdout.close() sys.stdout = state # Wait one second between packets time.sleep(1) return OBJECTIVE 4 - Contemplate the implications of our findings Tests 1 and 2 demonstrate that we need to budget ample time ( at least 16 minutes) to allow AWS to provision the flow logs. We observed that flow logs sent to CloudWatch Logs provision faster than logs sent to a S3 bucket. Test 3 demonstrated that with a max-aggregation-interval set to “600,” it is possible to see an ingestion delay of up to 10.42 minutes (cat logs-insights-results.csv | cut -d&quot;,&quot; -f4 | sort -u -n | tail -n1) and that it is a function of how long ago the last aggregation occurred. The minimum ingestion delay was 1.26 minutes (cat logs-insights-results.csv | cut -d&quot;,&quot; -f4 | sort -u -n | head -n2). Lots of bad stuff could be detected late if you were expecting to use flow logs for real-time detection. I think that AWS would tell you that is not the use case that they were designed for. Instead use something like a Web Application Firewall (WAF). We observed an unaccounted-for error in the ‘@timestamp’ field generated by Cloud Watch Logs Insights of +1 to -60 seconds during our test and +60 to -58 during a previous test. This demonstrates the value of benchmarking critical security systems. In this experiment, we demonstrated a novel technique for timestamping packets that will be reduced to a flow log by using the source port field. We showed how to verify the accuracy of the technique by comparing the epoch time calculated from the source port with the time logged by TCPdump. We also demonstrated how to identify any missing packets using this technique and in the process identified a minor bug in generate-test-traffic.py that caused a packet not to be sent about every 195 seconds. This demonstrates (again) the importance of understanding the limitations of the security tools that we use. At some point, I will circle back and fix this bug. Remember that the limited amount of testing that was performed is not sufficient to draw too many conclusions. Tests 1 and 2 should be repeated multiple times at different times of day, over the course of several days, in different regions to get more precise statistics. Test 3 should be repeated multiple times and should consider the impact of changing the max-aggregation-interval setting. The more the testing process is automated the lower the burden of repeating each test. But, I think that I have taken this little experiment far enough for now. Wrap Up Wow, we covered quite a bit in this Episode: We made extensive use of the AWS CLI We used a Bash “while loop” to time AWS CLI calls We crafted packets using Scapy and read those packets using TCPdump. We also created a fairly sophisticated CloudWatch Logs Insights query, and Learned how we can use Python to visualize data. To learn more about TCPdump, Scapy, and packet crafting check out SANS SEC503: Intrusion Analysis In-Depth. To learn more about using Python to create security tools like generate-test-traffic.py consider SANS SEC573: Automating Information Security with Python. Closing Comments If you have thoughts or comments on today’s episode, feel free to chime in on the comments for this YouTube video. If you appreciate this video and want to see more like it, be sure to give it a “thumbs up.” Stay tuned for another installment of “Head in the Clouds” as announcements of new episodes are made on the SANS Cloud Security Twitter feed. Meanwhile, be sure to check out the other great videos on the SANS Cloud Security YouTube Channel. Take care.","headline":"Episode 18 - Benchmarking AWS Flow Logs","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/episodes/episode18/"},"url":"http://localhost:4000/episodes/episode18/"}</script>
<!-- End Jekyll SEO tag -->


<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-K6EX94SG73"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-K6EX94SG73');
</script>


</head>

<body class="page">

<!--[if lt IE 9]><div class="browser-upgrade alert alert-info">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div><![endif]-->

<div class="navigation-wrapper">
	<div class="site-name">
		
		<a href="/">Head in the Clouds</a>
		
	</div><!-- /.site-name -->
	<div class="top-navigation">
		<nav role="navigation" id="site-nav" class="nav">
		    <ul>
		        
				    
				    <li><a href="/" >Home</a></li>
				
				    
				    <li><a href="/episodes/" >All Episodes</a></li>
				
				    
				    <li><a href="https://www.sans.org/cyber-security-courses/?focus-area=cloud-security" target="_blank">SANS CyberSec Courses & Certs</a></li>
				
		    </ul>
		</nav>
	</div><!-- /.top-navigation -->
</div><!-- /.navigation-wrapper -->




<div id="main" role="main">
  <div class="article-author-side">
    

<div itemscope itemtype="https://schema.org/Person">


	<img src="/images/Cloud_Ace_Final.png" class="bio-photo" alt="Head in the Clouds bio photo">


  <h3 itemprop="name">Head in the Clouds</h3>
  <p>with Kenneth G. Hartman<p>Certified SANS Instructor</p>
  <a href="mailto:kgh@kennethghartman.com" class="author-social" target="_blank"><i class="fa fa-fw fa-envelope-square"></i> Email</a>
  <a href="https://twitter.com/kennethghartman" class="author-social" target="_blank"><i class="fa fa-fw fa-twitter-square"></i> Twitter</a>
  
  
  
  
  
  
  
  <a href="https://github.com/Resistor52" class="author-social" target="_blank"><i class="fa fa-fw fa-github"></i> GitHub</a>
  
  
  
  
  
  
  
  
  
  
  
  
  
  
</div>

  </div>
  <article class="page">
    <h1>Episode 18 - Benchmarking AWS Flow Logs</h1>
    <div class="article-wrap">
      <p>18 - Benchmarking AWS Flow Logs</p>

<div class="video-container"><iframe width="560" height="315" src="https://www.youtube.com/embed/AmN-1LKPhP0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>

<p>Welcome to Episode 18 of the “Head in the Clouds” Video Series. I am Ken Hartman, a SANS Certified Instructor and content creator for the SANS Cloud Security Curriculum.</p>

<p>Today’s episode is titled: “Benchmarking AWS Flow Logs”</p>

<p>In many of the of the courses in the SANS Cloud Curriculum, we will have one or more labs that involves flow logs. However, a portion of the lab time is spent waiting for the flow logs to get provisioned. It’s kind of like watching and waiting for water to boil. Then, once the flow logs are provisioned, it takes time for the measured traffic to show up. Therefore, I thought it would be very interesting to determine what one should expect via experimentation and observation. Along the way, I made some very interesting discoveries, and I cannot wait to show them to you.</p>

<h2 id="objectives">Objectives</h2>

<p>Anytime we use a tool, technique, or service, we should be aware of its limitations and possible pitfalls. As such, here are the objectives for today:</p>

<ul>
  <li>Experimentally determine the typical time to provision flow logs to an S3 bucket.</li>
  <li>Experimentally determine the typical time to provision flow logs to a CloudWatch logs group.</li>
  <li>Experimentally determine the typical delay for ongoing traffic to be captured in the flow log record.</li>
  <li>Contemplate the implications of our findings.</li>
</ul>

<h2 id="setup">Setup</h2>

<p>For this episode, we will assume that you have an AWS EC2 virtual machine running in a VPC that has a “Name” tag of “hitc-vpc.” The t2.micro type that is in the free tier will do just fine. I recommend that you use the Ubuntu 20.04 AMI for your region, as this will allow me to provide you with the exact commands to install some additional software later in this episode.</p>

<p>I used Terraform to deploy my environment as described in the HitC Episode titled <a href="https://headintheclouds.site/episodes/episode10">Episode 10 - Demonstration of Terraform Modules; Deploy a VM in AWS, Azure and GCP at once</a> as I had originally planned to benchmark all three cloud service providers. However, as I got deeper and deeper into my research into AWS flow logs, I decided to stay focused on just AWS and may circle back to Azure and GCP flow logs in one or more future episodes.</p>

<p>In this episode, we will be making extensive use of the AWS command line interface. This has two benefits:</p>
<ul>
  <li>It will make it very easy for you to reproduce my work, and</li>
  <li>It will allow us to measure the time that it takes for things to happen.</li>
</ul>

<p>The machine on which you are running the CLI commands will need <code class="language-plaintext highlighter-rouge">jq</code> in addition to the AWS command line interface.</p>

<h2 id="objective-1-experimentally-determine-the-typical-time-to-provision-flow-logs-to-an-s3-bucket">OBJECTIVE 1: Experimentally determine the typical time to provision flow logs to an S3 bucket</h2>

<p>To start with, we need to set some variables that will be used by our commands:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>UNIQ_ID=$RANDOM; echo $UNIQ_ID
ACCOUNT=$(aws sts get-caller-identity | jq -r .Account); echo $ACCOUNT
YYYY=$(date -Idate | cut -d"-" -f1); echo $YYYY
MM=$(date -Idate | cut -d"-" -f2); echo $MM
DD=$(date -Idate | cut -d"-" -f3); echo $DD
REGION=$(aws ec2 describe-availability-zones --output text --query 'AvailabilityZones[0].[RegionName]'); echo $REGION

</code></pre></div></div>

<p>We generate a unique ID to ensure that the S3 bucket that we create is unique. Linux has a variable called <code class="language-plaintext highlighter-rouge">$RANDOM</code> that we can use for this purpose.</p>

<p>The <code class="language-plaintext highlighter-rouge">YYYY</code>, <code class="language-plaintext highlighter-rouge">MM</code>, and <code class="language-plaintext highlighter-rouge">DD</code> are captured so we can look into the bucket for our data.</p>

<p>Lastly, we use a trick from <a href="https://stackoverflow.com/questions/31331788/using-aws-cli-what-is-best-way-to-determine-the-current-region">StackOverflow.com</a> to get the region, as yours may be different from mine.</p>

<p>Next, we will create a variety of “helper commands.”</p>

<h3 id="get-the-vpc-id">Get the VPC ID</h3>

<p>To get the VPC ID of the VPC that is named “hitc-vpc” we can use:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>VPC_ID=$(aws ec2 describe-vpcs --filters Name=tag:Name,Values=hitc-vpc --query "Vpcs[0].VpcId" --output text); echo $VPC_ID

</code></pre></div></div>

<h3 id="create-the-bucket-for-the-flow-logs">Create the Bucket for the Flow Logs</h3>

<p>Next, we need to create an S3 bucket to receive our flow logs:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>aws s3 mb s3://hitc-benchmark-flowlogs-$UNIQ_ID

</code></pre></div></div>

<h3 id="set-up-the-flow-logs-for-the-vpc-and-send-to-s3">Set up the Flow Logs for the VPC and Send to S3</h3>

<p>Our next task is to make sure we have the command to set the flow logs for the VPC and send them to S3:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>aws ec2 create-flow-logs \
    --resource-type VPC \
    --resource-ids $VPC_ID \
    --traffic-type ALL \
    --log-destination-type s3 \
    --log-destination arn:aws:s3:::hitc-benchmark-flowlogs-$UNIQ_ID \
    --max-aggregation-interval 600 \
    --tag-specifications 'ResourceType=vpc-flow-log,Tags=[{Key=Name,Value=hitc-flow-log}]'

</code></pre></div></div>

<h3 id="describe-the-flow-logs-for-the-vpc">Describe the Flow Logs for the VPC</h3>

<p>Having done that, we may find it helpful to be able to describe the flow logs for the VPC in question:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>aws ec2 describe-flow-logs --filter "Name=resource-id,Values=$VPC_ID"

</code></pre></div></div>

<h3 id="delete-the-flow-logs-for-the-vpc">Delete the Flow Logs for the VPC</h3>

<p>Next, we need to be able to delete the flow logs that we just created. To do this we need to know the Flow Log ID.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>FLOW_LOG_ID=$(aws ec2 describe-flow-logs --filter "Name=resource-id,Values=$VPC_ID" --query 'FlowLogs[0].FlowLogId' --output text)
echo $FLOW_LOG_ID
aws ec2 delete-flow-logs --flow-log-ids $FLOW_LOG_ID

</code></pre></div></div>

<h3 id="list-the-contents-of-the-bucket">List the contents of the Bucket</h3>

<p>To verify the presence of a flow log, we will need to be able to drill deep into the S3 bucket. This is facilitated by the variables we set earlier.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>aws s3 ls s3://hitc-benchmark-flowlogs-$UNIQ_ID/AWSLogs/$ACCOUNT/vpcflowlogs/$REGION/$YYYY/$MM/$DD/

</code></pre></div></div>

<h3 id="empty-the-bucket">Empty the Bucket</h3>

<p>We also need the ability to empty the bucket between successive iterations of our test:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>aws s3 rm s3://hitc-benchmark-flowlogs-$UNIQ_ID --recursive

</code></pre></div></div>

<h3 id="remove-the-bucket">Remove the Bucket</h3>

<p>It is also good idea to remove the bucket when we are done with our testing:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>aws s3 rb s3://hitc-benchmark-flowlogs-$UNIQ_ID
</code></pre></div></div>

<h3 id="generate-flow-log-traffic">Generate Flow Log Traffic</h3>

<p>Now that we’ve got our helper scripts figured out and tested, we need to generate some traffic to get captured by our flow log. SSH into your easy to instance and run the following command:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>while true ; do wget --timeout=10 http://1.1.1.1:81; sleep 20; done

</code></pre></div></div>

<p>Note that I selected CloudFlare as the target as they are big enough that this would not be considered abusive and by sending the traffic to port 81, the retries will keep working until exhausted after 20 attempts and will then start over after 20 seconds. Anything that repetitively generates traffic will work here.</p>

<h3 id="test-1---time-the-provisioning-of-the-flow-log-until-first-flow-log-record-in-s3">TEST 1 - Time the provisioning of the flow Log until first flow log record in S3</h3>

<p>With that done, we can now measure the time between the provisioning of the flow log until first flow log record in S3. Every 20 seconds we will check the S3 bucket to see if the flow log showed up yet. Our “TEST” checks to see if the length of response of the <code class="language-plaintext highlighter-rouge">aws s3 ls</code> command is zero. Once there is a flow log record in the bucket, the infinite loop will exit and we subtract the start time from the end time. Working in epoch time, makes this easy.</p>

<p>NOTE: This assumes that the S3 bucket exists, but is empty</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Set up the Flow Logs for the VPC
aws ec2 create-flow-logs \
    --resource-type VPC \
    --resource-ids $VPC_ID \
    --traffic-type ALL \
    --log-destination-type s3 \
    --log-destination arn:aws:s3:::hitc-benchmark-flowlogs-$UNIQ_ID \
    --max-aggregation-interval 600 \
    --tag-specifications 'ResourceType=vpc-flow-log,Tags=[{Key=Name,Value=hitc-flow-log}]'
# Watch for initial flow logs
printf "Waiting for the flow logs to apear in S3 bucket "
START_TIMER=$(date +%s)
TEST=""
while [ -z "$TEST" ] ; do
  printf "."
  sleep 20
  TEST=$(aws s3 ls s3://hitc-benchmark-flowlogs-$UNIQ_ID/AWSLogs/$ACCOUNT/vpcflowlogs/$REGION/$YYYY/$MM/$DD/)
done
STOP_TIMER=$(date +%s)
echo "DONE"
echo $((STOP_TIMER-START_TIMER))" seconds have elapsed"

</code></pre></div></div>

<p>While this script executes, it will print a dot to stdout every 20 seconds. When it is done, the output will look like this:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>772 seconds have elapsed
</code></pre></div></div>

<p>Ok, 772 seconds is 12.87 minutes. We now have our TEST 1 measurement. Note that this can be off by 20 seconds due to the sleep timer, which can be removed.</p>

<h3 id="tear-down-test-1">Tear down TEST 1</h3>

<p>To repeat the test, we need to delete the flow log and empty the S3 bucket, but leave the S3 bucket in place.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Delete the Flow Logs for the VPC
FLOW_LOG_ID=$(aws ec2 describe-flow-logs --filter "Name=resource-id,Values=$VPC_ID" --query 'FlowLogs[0].FlowLogId' --output text)
echo $FLOW_LOG_ID
aws ec2 delete-flow-logs --flow-log-ids $FLOW_LOG_ID
# Empty the Bucket
aws s3 rm s3://hitc-benchmark-flowlogs-$UNIQ_ID --recursive

</code></pre></div></div>

<h3 id="test-1-summary">TEST 1 Summary</h3>

<p>This table shows the results of repeating the test 3 more times:</p>

<table>
  <thead>
    <tr>
      <th>Script Results</th>
      <th style="text-align: right">Minutes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>772 seconds have elapsed</td>
      <td style="text-align: right">12.86 Minutes</td>
    </tr>
    <tr>
      <td>417 seconds have elapsed</td>
      <td style="text-align: right">6.97 Minutes</td>
    </tr>
    <tr>
      <td>813 seconds have elapsed</td>
      <td style="text-align: right">13.55 Minutes</td>
    </tr>
    <tr>
      <td>960 seconds have elapsed</td>
      <td style="text-align: right">16.00 Minutes</td>
    </tr>
  </tbody>
</table>

<p>Wow, it can take 16 minutes to get your first flow logs!</p>

<p>So, what is up with this? Well, the catch phrase that the cloud service providers use is <em>eventual consistency.</em> We know that the big guys use queuing and focus intently on optimization, so I am guessing that it is for those reasons that there is a provisioning delay.</p>

<h3 id="final-cleanup-test-1">Final Cleanup TEST 1</h3>

<p>Now that we have obtained our results, make sure that you delete the flow log and empty the bucket again, using the last set of commands and then remove the bucket, as we no longer need it.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Remove the Bucket
aws s3 rb s3://hitc-benchmark-flowlogs-$UNIQ_ID

</code></pre></div></div>

<h2 id="objective-2-experimentally-determine-the-typical-time-to-provision-flow-logs-to-a-cloud-watch-logs-group">OBJECTIVE 2: Experimentally determine the typical time to provision flow logs to a Cloud Watch logs group</h2>

<p>Now, let’s take a look at Flow Logs sent to Cloud Watch Logs.</p>

<h3 id="create-the-cloudwatch-logs-group">Create the CloudWatch Logs Group</h3>

<p>First we need to create the CloudWatch Logs Group:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>aws logs create-log-group --log-group-name hitc-flow-logs

</code></pre></div></div>

<h3 id="delete-the-log-group">Delete the Log Group</h3>

<p>And, here is the corresponding command to delete the log group:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>aws logs delete-log-group --log-group-name hitc-flow-logs

</code></pre></div></div>

<h3 id="create-the-cloudwatch-logs-role">Create the CloudWatch Logs Role</h3>

<p>Next, we need create the CloudWatch Logs Role. To do this we will create the Trust Policy JSON file, the Permissions Policy JSON file, and then attach them to a new role.</p>

<p>Create the Trust Policy JSON file. I like to use the “heredoc” technique, so that everything can be scripted:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cat &lt;&lt; EOF &gt; trust-policy.json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "vpc-flow-logs.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
EOF

</code></pre></div></div>

<p>Create the Permissions Policy JSON file:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cat &lt;&lt; EOF &gt; cw-flow-logs-policy.json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "logs:CreateLogGroup",
        "logs:CreateLogStream",
        "logs:PutLogEvents",
        "logs:DescribeLogGroups",
        "logs:DescribeLogStreams"
      ],
      "Resource": "*"
    }
  ]
}   
EOF

</code></pre></div></div>

<p>Next, run the following commands to create the policies and the role:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>aws iam create-policy --policy-name hitc-cw-flow-logs --policy-document file://cw-flow-logs-policy.json
aws iam create-role --role-name hitc-cw-flow-logs --assume-role-policy-document file://trust-policy.json
aws iam attach-role-policy --policy-arn "arn:aws:iam::$ACCOUNT:policy/hitc-cw-flow-logs" --role-name hitc-cw-flow-logs
aws iam list-attached-role-policies --role-name hitc-cw-flow-logs

</code></pre></div></div>

<h2 id="test-2---time-the-provisioning-of-the-flow-log-until-first-flow-log-record-in-cloudwatch">TEST 2 - Time the provisioning of the flow Log until first flow log record in CloudWatch</h2>

<p>With our setup for Test 2 in place, we can now conduct it. This “TEST” checks to see if the length of response of the <code class="language-plaintext highlighter-rouge">aws logs describe-log-streams</code> command is zero. Once there is a flow log record in CW logs, the infinite loop will exit and we subtract the start time from the end time.</p>

<p>NOTE: This assumes that the CloudWatch Log Group exists, but is empty.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Set up the Flow Logs for the VPC
aws ec2 create-flow-logs \
    --resource-type VPC \
    --resource-ids $VPC_ID \
    --traffic-type ALL \
    --log-destination-type cloud-watch-logs \
    --log-group-name hitc-flow-logs \
    --max-aggregation-interval 600 \
    --tag-specifications 'ResourceType=vpc-flow-log,Tags=[{Key=Name,Value=hitc-flow-log}]' \
    --deliver-logs-permission-arn 'arn:aws:iam::690634326977:role/hitc-cw-flow-logs'
# Watch for initial flow logs
printf "Waiting for the flow logs to appear in CW Logs Group "
START_TIMER=$(date +%s)
TEST=""
while [ -z "$TEST" ] ; do
  printf "."
  sleep 20
  TEST=$(aws logs describe-log-streams --log-group-name hitc-flow-logs --output text)
done
STOP_TIMER=$(date +%s)
echo "DONE"
echo $((STOP_TIMER-START_TIMER))" seconds have elapsed"

</code></pre></div></div>

<h3 id="tear-down-test-2">Tear down TEST 2</h3>

<p>To re-run the test, delete the flow log and reset the CW Logs Group.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Delete the Flow Logs for the VPC
FLOW_LOG_ID=$(aws ec2 describe-flow-logs --filter "Name=resource-id,Values=$VPC_ID" --query 'FlowLogs[0].FlowLogId' --output text)
echo $FLOW_LOG_ID
aws ec2 delete-flow-logs --flow-log-ids $FLOW_LOG_ID
# Delete and Recreate the CW Logs Group
aws logs delete-log-group --log-group-name hitc-flow-logs
aws logs create-log-group --log-group-name hitc-flow-logs

</code></pre></div></div>

<h3 id="final-cleanup-test-2">Final Cleanup TEST 2</h3>

<p>And when we are all done, we can run the following commands to delete the role and associated policy. But hold off for now as we will need this role for Test 3.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># CLI commands to tear down the role and IAM policy
aws iam detach-role-policy --role-name hitc-cw-flow-logs --policy-arn "arn:aws:iam::$ACCOUNT:policy/hitc-cw-flow-logs"
aws iam delete-role --role-name hitc-cw-flow-logs
aws iam delete-policy --policy-arn "arn:aws:iam::$ACCOUNT:policy/hitc-cw-flow-logs"

</code></pre></div></div>

<h3 id="test-2-summary">TEST 2 Summary</h3>

<table>
  <thead>
    <tr>
      <th>Script Results</th>
      <th style="text-align: right">Minutes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>21 seconds have elapsed</td>
      <td style="text-align: right">0.35 Minutes</td>
    </tr>
    <tr>
      <td>541 seconds have elapsed</td>
      <td style="text-align: right">9.02 Minutes</td>
    </tr>
    <tr>
      <td>522 seconds have elapsed</td>
      <td style="text-align: right">8.70 Minutes</td>
    </tr>
    <tr>
      <td>375 seconds have elapsed</td>
      <td style="text-align: right">6.25 Minutes</td>
    </tr>
  </tbody>
</table>

<p>This is much better, but it can still take up to 10 minutes and is somewhat variable.</p>

<h2 id="objective-3-experimentally-determine-the-typical-delay-for-ongoing-traffic-to-be-captured-in-the-flow-log-record">OBJECTIVE 3: Experimentally determine the typical delay for ongoing traffic to be captured in the flow log record.</h2>

<p>Now, onto what is probably the most interesting and has the biggest implications for security monitoring. We want to know how long will it take for traffic that occurs in my VPC to show up as recorded in my flow logs? In other words, “I want to know how much lag there is in any detective control that I implement (that is based on flow logs).”</p>

<p>The security implications of this are fairly significant.</p>

<h3 id="the-problem-statement">The Problem Statement</h3>

<p>Wouldn’t it be great to time stamp a packet, send it across the virtual network of our VPC, look at the packet in CloudWatch Logs, and determine how much delay there was between the time the packet was sent, and when it was recorded?</p>

<p>That would be fantastic. The only problem is that a flow log contains only metadata, and not any payload. Here is a screenshot of what we have to work with:</p>

<p><img src="/images/ep18/FlowLogDataScreenshot.png" alt="Flow Log Data Screenshot" /></p>

<p>As it turns out, about the only thing that we have to fiddle with is the Source port and the Destination port. So, I will use the source port to encode the timestamp and send it to an unchanging port (TCP port 7777) at a target IP address outside of my VPC.</p>

<h3 id="will-this-work">Will this work?</h3>

<p>Let’s do some math here. We know that there are 65536 (2 to the power of 16) possible TCP ports.</p>

<p>If we send one packet per second from a different port, we could do that for 1092 Minutes:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt;&gt;&gt; (2 ** 16) / 60
1092.2666666666667
</code></pre></div></div>

<p>Or converting that to hours:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt;&gt;&gt; (2 ** 16) / 60 / 60
18.204444444444444
</code></pre></div></div>

<p>We get 18 hours. <strong>Hence, we can send traffic at a rate of one packet per second from a unique source port for over 18 hours before using up all possible ports!</strong></p>

<h3 id="converting-back-and-forth-from-the-source-port-to-the-epoch-time">Converting back and forth from the source port to the epoch time</h3>

<p>To calculate which port to use, we will take the current epoch time and divide it by 65536 and then take the integer portion of the remainder and use that for the source port. The math function that returns the remainder is called “modulo” and is represented as a <code class="language-plaintext highlighter-rouge">%</code> in python</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>source port = int(current_epoch_time % 65536)
</code></pre></div></div>

<p>To convert back to the epoch time from the source port, take the current epoch time and divide it by 65536 and then keep the quotient portion. Divide the source port by 65536 and take that result and add it to the quotient. Multiply the new total by 65536 and the new result is the epoch time of when the data was sent.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>epoch_timestamp = int((modulo / 65536.0 + int(current_epoch_time / 65536)) * 65536)
</code></pre></div></div>

<p>This conversion assumes that the <em>current epoch time</em> is within the 18 hour window represented by the source port value.</p>

<h3 id="introducing-scapy">Introducing Scapy</h3>

<p>To generate a packet per second with an ever increasing source port that represents the time that the packet was sent, I used <a href="https://scapy.net/">scapy</a>. Scapy is written as a python module and is ideal for programmatically crafting packets.</p>

<p>As we will be running Scapy on our EC2 instance to send traffic out of our VPC, we need to install Scapy, its dependencies, and the additional modules my python program will use:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sudo apt update
sudo apt install -y python3-pip tcpdump  python3-matplotlib
sudo pip3 install pytz
sudo pip install --pre scapy[complete]
</code></pre></div></div>

<p>Note that using <code class="language-plaintext highlighter-rouge">sudo</code> with <code class="language-plaintext highlighter-rouge">pip</code> to install software is generally a bad idea. <a href="https://dev.to/elabftw/stop-using-sudo-pip-install-52mn">See this link for a detailed discussion of the security issues.</a></p>

<p>However, in this case, Scapy needs to run as root to be able to send packets, and we will destroy our EC2 instance at the end of our experimentation.</p>

<p>To launch Scapy, just type <code class="language-plaintext highlighter-rouge">sudo scapy</code> at the Ubuntu command line as shown below:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ubuntu@ip-10-0-1-112:~$ sudo scapy
INFO: PyX dependencies are not installed ! Please install TexLive or MikTeX.

                     aSPY//YASa
             apyyyyCY//////////YCa       |
            sY//////YSpcs  scpCY//Pp     | Welcome to Scapy
 ayp ayyyyyyySCP//Pp           syY//C    | Version 2.4.5
 AYAsAYYYYYYYY///Ps              cY//S   |
         pCCCCY//p          cSSps y//Y   | https://github.com/secdev/scapy
         SPPPP///a          pP///AC//Y   |
              A//A            cyP////C   | Have fun!
              p///Ac            sC///a   |
              P////YCpc           A//A   | We are in France, we say Skappee.
       scccccp///pSP///p          p//Y   | OK? Merci.
      sY/////////y  caa           S//P   |             -- Sebastien Chabal
       cayCyayP//Ya              pY/Ya   |
        sY/PsY////YCc          aC//Yp
         sc  sccaCY//PCypaapyCP//YSs
                  spCPY//////YPSps
                       ccaacs
                                       using IPython 8.1.1
&gt;&gt;&gt;
</code></pre></div></div>

<p>I set up a target VM in another VPC with the security group set to allow inbound TCP port 7777 from the system with Scapy installed. As a test, I am running TCPdump on the target machine, using the following command:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sudo tcpdump -i eth0 'port 7777'

</code></pre></div></div>

<p>Next, use Scapy to send the target a packet. Enter the following commands at the Scapy IPython prompt, but substitute my target IP address (35.168.8.211) for the one you are using:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ip=IP(dst="35.168.8.211")
tcp=TCP(sport=1111,dport=7777,flags="S",options=[('Timestamp',(0,0))])
pay="This is Test3"
test3=ip/tcp/pay
send(test3)
</code></pre></div></div>

<p>The result should look like:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>.
Sent 1 packets.
</code></pre></div></div>

<p><strong>NOTE:</strong> If you get a “Operation not permitted” error, you forgot to run Scapy as root</p>

<p>Exit back to the Ubuntu command line using <code class="language-plaintext highlighter-rouge">exit()</code></p>

<p>Looking over at the target system, we can see that TCPdump generated the following two records:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>22:25:43.461121 IP ec2-34-201-148-40.compute-1.amazonaws.com.1111 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13
22:25:43.461156 IP ip-172-31-48-150.ec2.internal.7777 &gt; ec2-34-201-148-40.compute-1.amazonaws.com.1111: Flags [R.], seq 0, ack 14, win 0, length 0
</code></pre></div></div>

<p>We Sent a TCP “Syn” packet and a “Reset” was sent back because there is not a service listening on port 7777.</p>

<h3 id="putting-the-theory-to-work">Putting the theory to work</h3>

<p>Ok, so now that we know the Scapy commands to craft a packet. We can write a program to determine the Source Port to use, to craft the packet and sent it. We also can prove that the packet was received by the target system and see the timestamp printed to standard out by TCPdump.</p>

<p>The program is called <code class="language-plaintext highlighter-rouge">generate-test-traffic.py</code> and can be downloaded from <a href="https://raw.githubusercontent.com/Resistor52/hitc-benchmark-flow-logs/hitc-ep18/generate-test-traffic.py">my Github Repo</a></p>

<p>Copy this program to the EC2 instance that has Scapy Installed.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>wget https://raw.githubusercontent.com/Resistor52/hitc-benchmark-flow-logs/hitc-ep18/generate-test-traffic.py

</code></pre></div></div>

<p>Run the program as root using</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sudo python3 ./generate-test-traffic.py

</code></pre></div></div>

<p>Once the program runs, it will ask four questions as shown below and then start to send packets at a rate of one per second.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ubuntu@ip-10-0-1-112:~$ sudo python3 ./generate-test-traffic.py
Enter the name for the CSV log file (without the csv extension): test3A
Enter the desired target IP aggress in the format "111.111.111.111": 35.168.8.211
Enter the desired start time in the format "HH:MM" for local time zone: 19:00
Enter the desired end time in the format "HH:MM" for local time zone: 22:00
Traffic generation will start at: 19:00
Traffic generation will continue until: 22:00
**************** Start of Traffic **************************
.Epoch Time: 1646344800   Time: 2022-03-03 22:00:00 UTC+0000   srcPort: 14944   Remaining Time: 0 Seconds
***************** End of Traffic ***************************
</code></pre></div></div>

<p>In the example above, I am calling the log file <strong>test3A</strong> and provided <strong>35.168.8.211</strong> as the target IP. (Again, be sure to use the IP address of the target that you set up and has TCPdump listening.) I started the program at <strong>19:00</strong> hours UTC, simply because that was on the hour, and had it run for 3 hours, ending at <strong>22:00</strong>. The program uses UTC because that is the time zone that AWS uses for all its cloud services, including EC2.</p>

<p>Now, while we are waiting for the traffic generation to start, rerun TCPdump on the target system, this time capturing the packets to a file:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sudo tcpdump -i eth0 'port 7777' -w tcpdump.pcap
</code></pre></div></div>

<h3 id="initial-look-at-the-data">Initial Look at the Data</h3>

<p>The <code class="language-plaintext highlighter-rouge">generate-test-traffic.py</code> script created a log, which I named <strong>test3A.csv.</strong> Note that the script adds the “csv” extension to the filename. The top 10 rows of the file are shown below:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"Epoch_Time","Date_Time","Source_Port"
"1646334000","2022-03-03 19:00:00 UTC+0000","4144"
"1646334001","2022-03-03 19:00:01 UTC+0000","4145"
"1646334002","2022-03-03 19:00:02 UTC+0000","4146"
"1646334003","2022-03-03 19:00:03 UTC+0000","4147"
"1646334004","2022-03-03 19:00:04 UTC+0000","4148"
"1646334005","2022-03-03 19:00:05 UTC+0000","4149"
"1646334006","2022-03-03 19:00:06 UTC+0000","4150"
"1646334007","2022-03-03 19:00:07 UTC+0000","4151"
"1646334008","2022-03-03 19:00:08 UTC+0000","4152"
</code></pre></div></div>

<p>We can look at the first 10 packets received by the Target system using the following tcpdump command:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tcpdump -r tcpdump.pcap -nn -c10 'dst port 7777'
</code></pre></div></div>

<p>And the output is:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>19:00:00.013928 IP ec2-34-201-148-40.compute-1.amazonaws.com.4144 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13
19:00:01.018892 IP ec2-34-201-148-40.compute-1.amazonaws.com.4145 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13
19:00:02.023309 IP ec2-34-201-148-40.compute-1.amazonaws.com.4146 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13
19:00:03.027319 IP ec2-34-201-148-40.compute-1.amazonaws.com.4147 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13
19:00:04.032328 IP ec2-34-201-148-40.compute-1.amazonaws.com.4148 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13
19:00:05.050076 IP ec2-34-201-148-40.compute-1.amazonaws.com.4149 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13
19:00:06.061365 IP ec2-34-201-148-40.compute-1.amazonaws.com.4150 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13
19:00:07.066456 IP ec2-34-201-148-40.compute-1.amazonaws.com.4151 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13
19:00:08.071536 IP ec2-34-201-148-40.compute-1.amazonaws.com.4152 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13
19:00:09.077085 IP ec2-34-201-148-40.compute-1.amazonaws.com.4153 &gt; ip-172-31-48-150.ec2.internal.7777: Flags [S], seq 0:13, win 8192, options [TS val 0 ecr 0,eol], length 13
</code></pre></div></div>

<p>We can extract the two critical data elements from packet capture using the following command:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tcpdump -r tcpdump.pcap -nn -c10 'dst port 7777' | cut -d" " -f1,3 | cut -d"." -f1,6 | tr "." " "
</code></pre></div></div>

<p>And this command will output the timestamp and the source port of the packet received by the target system:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>19:00:00 4144
19:00:01 4145
19:00:02 4146
19:00:03 4147
19:00:04 4148
19:00:05 4149
19:00:06 4150
19:00:07 4151
19:00:08 4152
19:00:09 4153
</code></pre></div></div>

<p>Great, everything correlates! Now lets write the whole packet to a file:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tcpdump -r tcpdump.pcap -nn 'dst port 7777' | cut -d" " -f1,3 | cut -d"." -f1,6 | tr "." " " &gt; test3A.log
</code></pre></div></div>

<p>We will save <strong>test3A.log</strong> for later, as it will come in handy.</p>

<h3 id="looking-in-cloudwatch-logs">Looking in CloudWatch Logs</h3>

<p>The CloudWatch Logs Web Console is a great tool to slice and dice log data. Paste in the following query:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>fields @ingestionTime, @timestamp, ((toMillis(@ingestionTime) - toMillis(@timestamp))/1000) as IngestDelaySecs, ((toMillis(@ingestionTime) - toMillis(@timestamp))/1000/60)
as IngestDelayMins, toMillis(@timestamp)/1000 as TimeStampEpoch, (65536*floor(TimeStampEpoch/65536))+srcPort as SentTimeEpoch, fromMillis(SentTimeEpoch * 1000) as SentTime,
TimeStampEpoch-SentTimeEpoch as TimeStampDelaySec, srcPort
| sort srcPort asc
| limit 10000
| filter srcAddr = "10.0.1.112" and dstAddr = "35.168.8.211" and dstPort != "22" and srcPort != "22")
</code></pre></div></div>

<p>Be sure to set the time filter in the upper window to include the window in which the packets were sent.</p>

<p><img src="/images/ep18/CloudWatchLogsInsights.png" alt="CloudWatch Logs Insights Screenshot" /></p>

<p>Looking at the output, we see that AWS provides two fields <code class="language-plaintext highlighter-rouge">@ingestionTime</code> and <code class="language-plaintext highlighter-rouge">@timestamp</code> so we can calculate the delay the Ingestion Delay Seconds (IngestDelaySecs) by converting both time values to Epoch time and subtracting them.</p>

<p>But how accurate is the timestamp? Well, we can use the CloudWatch Logs Query Language to calculate a <code class="language-plaintext highlighter-rouge">SentTimeEpoch</code> based on the srcPort. We can also convert that to <code class="language-plaintext highlighter-rouge">SentTime</code> so it is human-readable. If we subtract the <code class="language-plaintext highlighter-rouge">SentTimeEpoch</code> from the  <code class="language-plaintext highlighter-rouge">TimeStampEpoch</code> we get the <code class="language-plaintext highlighter-rouge">TimeStampDelaySec</code>.</p>

<p>Disregard the first record as that was captured when I demonstrated how to use Scapy with IPython using an arbitrary source port. (I generated the traffic using the Python Script a few hours beforehand and that explains the chronology of this timestamp.) This record will be removed from the downloaded dataset.</p>

<p>Another thing, that you may have noticed in the screenshot, is that the first srcPort is <strong>4184</strong> which is 40 seconds after we sent the packet with srcPort <strong>4144</strong>!</p>

<p>Very interesting. It looks like we have “negative delays.” or to phrase it more accurately, the <code class="language-plaintext highlighter-rouge">@timestamp</code> value attached by AWS has an error that fluctuates within a range.</p>

<p>Use the “Export Results” button to save the file as <code class="language-plaintext highlighter-rouge">logs-insights-results.csv</code></p>

<h3 id="looking-at-the-data-graphically">Looking at the Data Graphically</h3>

<p>Since we see that we have an Ingest Delay and a Timestamp Error, it would be good to look at this data graphically. Toward that end, I have create a python program called <code class="language-plaintext highlighter-rouge">plotObservations.py</code> which can be downloaded from <a href="https://raw.githubusercontent.com/Resistor52/hitc-benchmark-flow-logs/hitc-ep18/plotObservations.py">my Github Repo</a>.</p>

<p>Copy this program to the EC2 instance that has Scapy Installed.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>wget https://raw.githubusercontent.com/Resistor52/hitc-benchmark-flow-logs/hitc-ep18/plotObservations.py

</code></pre></div></div>

<p>Run the program as using:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python3 ./plotObservations.py

</code></pre></div></div>

<p>The program will read <code class="language-plaintext highlighter-rouge">logs-insights-results.csv</code> and will generate the four plots shown below.</p>

<p><img src="/images/ep18/plot1.png" alt="plot1.png" />
<em><strong>PLOT 1</strong> - Packets 1000 to 2000</em></p>

<p><img src="/images/ep18/plot2.png" alt="plot2.png" />
<em><strong>PLOT 2</strong> - Packets 1020 to 1200</em></p>

<p><img src="/images/ep18/plot3.png" alt="plot3.png" />
<em><strong>PLOT 3</strong> - The First 10,000 Packets</em></p>

<p><img src="/images/ep18/delta.png" alt="delta.png" />
<em><strong>Delta</strong> - Seconds Between Missing Data</em></p>

<p>I saved the charts generated from data captured in the exact same manner 24 hours prior to this set, and those are shown below for comparison purposes.</p>

<p><img src="/images/ep18/plot1b.png" alt="plot1b.png" />
<em><strong>PLOT 1b</strong> - Packets 1000 to 2000 (Previous Run)</em></p>

<p><img src="/images/ep18/plot3b.png" alt="plot3b.png" />
<em><strong>PLOT 3b</strong> - The First 10,000 Packets (Previous Run)</em></p>

<h3 id="observations">Observations</h3>

<p><strong>Ingest Delay</strong> - We can see the ingest delay varies from a little over 60 seconds to almost 11 minutes every 10 minutes. Note that when we configured the flow logs we used a setting of <code class="language-plaintext highlighter-rouge">max-aggregation-interval 600</code>. This test should be repeated using a <code class="language-plaintext highlighter-rouge">max-aggregation-interval 60</code> to measure the impact on the Ingest Delay.</p>

<p><strong>Timestamp Error Seconds</strong> - The <code class="language-plaintext highlighter-rouge">TimeStampDelaySec</code> is calculated by subtracting the <code class="language-plaintext highlighter-rouge">SentTimeEpoch</code> from <code class="language-plaintext highlighter-rouge">TimeStampEpoch</code>. Chronologically, the <code class="language-plaintext highlighter-rouge">SentTimeEpoch</code> should occur first, as that is the timestamp of when Scapy sent the packet. Similarly, the <code class="language-plaintext highlighter-rouge">TimeStampEpoch</code> is when the VPC Flow Logs system detects the packet on the virtual network. I expected the error to be within milliseconds, but not almost a whole minute off! I am not sure how to explain this. At first, I thought that it might be a math error on my part. But we can take a look at a single packet as it was:</p>

<ul>
  <li>Recorded in the flow log (logs-insights-results.csv),</li>
  <li>Sent by Scapy (test3A.csv), and</li>
  <li>Recorded by TCPdump at the Target System (tcpdump.pcap)</li>
</ul>

<p>We can use <code class="language-plaintext highlighter-rouge">grep -E ',-58,|srcPort' logs-insights-results.csv | head -n6 | cut -d"," -f5,6-9</code> to generate the following table:</p>

<table>
  <thead>
    <tr>
      <th>TimeStampEpoch</th>
      <th>SentTimeEpoch</th>
      <th>SentTime</th>
      <th>TimeStampDelaySec</th>
      <th>srcPort</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1646334040</td>
      <td>1646334098</td>
      <td>2022-03-03 19:01:38.000</td>
      <td>-58</td>
      <td>4242</td>
    </tr>
    <tr>
      <td>1646334099</td>
      <td>1646334157</td>
      <td>2022-03-03 19:02:37.000</td>
      <td>-58</td>
      <td>4301</td>
    </tr>
    <tr>
      <td>1646334160</td>
      <td>1646334218</td>
      <td>2022-03-03 19:03:38.000</td>
      <td>-58</td>
      <td>4362</td>
    </tr>
    <tr>
      <td>1646334219</td>
      <td>1646334277</td>
      <td>2022-03-03 19:04:37.000</td>
      <td>-58</td>
      <td>4421</td>
    </tr>
    <tr>
      <td>1646334280</td>
      <td>1646334338</td>
      <td>2022-03-03 19:05:38.000</td>
      <td>-58</td>
      <td>4482</td>
    </tr>
  </tbody>
</table>

<p><em>Selected packets from logs-insights-results.csv</em></p>

<p>Now that we have identified the first five packets with the highest error (-58), we can look for those packets in the log generated by Scapy using <code class="language-plaintext highlighter-rouge">grep -E '"4242"|"4301"|"4362"|"4421"|"4482"' test3A.csv</code></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Epoch_Time</th>
      <th style="text-align: center">Date_Time</th>
      <th style="text-align: center">Source_Port</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">1646334098</td>
      <td style="text-align: center">2022-03-03 19:01:38 UTC+0000</td>
      <td style="text-align: center">4242</td>
    </tr>
    <tr>
      <td style="text-align: center">1646334157</td>
      <td style="text-align: center">2022-03-03 19:02:37 UTC+0000</td>
      <td style="text-align: center">4301</td>
    </tr>
    <tr>
      <td style="text-align: center">1646334218</td>
      <td style="text-align: center">2022-03-03 19:03:38 UTC+0000</td>
      <td style="text-align: center">4362</td>
    </tr>
    <tr>
      <td style="text-align: center">1646334277</td>
      <td style="text-align: center">2022-03-03 19:04:37 UTC+0000</td>
      <td style="text-align: center">4421</td>
    </tr>
    <tr>
      <td style="text-align: center">1646334338</td>
      <td style="text-align: center">2022-03-03 19:05:38 UTC+0000</td>
      <td style="text-align: center">4482</td>
    </tr>
  </tbody>
</table>

<p><em>Selected packets from test3A.csv</em></p>

<p>We can see that the <code class="language-plaintext highlighter-rouge">SentTimeEpoch</code> as calculated by my CloudWatch query logic is performed correctly as it matches the <code class="language-plaintext highlighter-rouge">Epoch_Time</code> as logged by Scapy.</p>

<p>For completeness, let’s look at TCP dump:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tcpdump -tt -nn -r tcpdump.pcap 'src port 4242 or src port 4301 or src port 4362 or src port 4421 or src port 4482' | cut -d":" -f1
</code></pre></div></div>

<p>produces:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1646334098.569321 IP 34.201.148.40.4242 &gt; 172.31.48.150.7777
1646334157.873827 IP 34.201.148.40.4301 &gt; 172.31.48.150.7777
1646334218.182541 IP 34.201.148.40.4362 &gt; 172.31.48.150.7777
1646334277.489424 IP 34.201.148.40.4421 &gt; 172.31.48.150.7777
1646334338.805517 IP 34.201.148.40.4482 &gt; 172.31.48.150.7777
</code></pre></div></div>

<p>As expected, we can see that the epoch time recorded by TCPdump matches the other observations in the tables above.</p>

<p><strong>Missing Data</strong> - Looking at the Missing Data plots in the figures above initially raised some concerns. Is the VPC dropping packets? To investigate what is going on, I tweaked <code class="language-plaintext highlighter-rouge">plotObservations.py</code> to generate the “Delta” plot to show the periodicity of the missing data as well as to output the srcPorts for which data is missing. That output is shown below:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The following srcPorts are missing:
[4327, 4506, 4694, 4890, 5082, 5276, 5471, 5666, 5855,
6049, 6245, 6439, 6632, 6824, 7018, 7211, 7407, 7600,
7791, 7987, 8174, 8365, 8555, 8750, 8942, 9135, 9330,
9524, 9716, 9911, 10102, 10293, 10485, 10680, 10866,
11058, 11253, 11427, 11620, 11811, 12005, 12198, 12386,
12583, 12778, 12972, 13166, 13361, 13555, 13750, 13945,
14138]
</code></pre></div></div>

<p>I also had <code class="language-plaintext highlighter-rouge">plotObservations.py</code> write the ports to a file, <code class="language-plaintext highlighter-rouge">missingSrcPorts.csv</code> so that we can grep for them:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>grep -f missingSrcPorts.csv test3A.csv
</code></pre></div></div>

<p>Hmm, none of the missing srcPorts were logged by Scapy. The good news is that the VPC network did not drop any packets because all sent packets have been accounted for. The bad news is that there is bug in <code class="language-plaintext highlighter-rouge">generate-test-traffic.py</code> that occurs about every 195 seconds. I suspect that it is an issue with my <code class="language-plaintext highlighter-rouge">generateTraffic</code> function, most likely due to <a href="https://stackoverflow.com/questions/1133857/how-accurate-is-pythons-time-sleep">inaccuracies with time.sleep()</a> compounded by variations in the amount of time it takes Scapy to write the packet out to the network.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def generateTraffic(stop_time_epoch):
    datetime_TZ = datetime.now(tz_TZ)
    seconds = int(time.time())             # Current Epoch Time
    modulo = seconds % (2 ** 16)           # Calculate the SrcPort
    datetime_time=datetime_TZ.strftime("%Y-%m-%d %H:%M:%S %Z%z")
    print(f"Epoch Time: {seconds}   Time: {datetime_time}   srcPort: {modulo}   {RemainingTimeString(stop_time_epoch)}      ", end = "\r")
    write_str='"'+str(seconds)+'","'+datetime_time+'","'+str(modulo)+'"\n'
    f.write(write_str)
    tcp=TCP(sport=modulo,dport=7777,flags="S",options=[('Timestamp',(0,0))])
    pay=logfile+" - "+str(modulo)
    # Trap the output of scapy send so it does not print to console
    state = sys.stdout
    sys.stdout = open('/dev/null', 'w')
    send(ip/tcp/pay)                        # Scapy
    sys.stdout.close()
    sys.stdout = state
    # Wait one second between packets
    time.sleep(1)
    return
</code></pre></div></div>

<h2 id="objective-4---contemplate-the-implications-of-our-findings">OBJECTIVE 4 - Contemplate the implications of our findings</h2>

<p>Tests 1 and 2 demonstrate that we need to budget ample time ( at least 16 minutes) to allow AWS to provision the flow logs. We observed that flow logs sent to CloudWatch Logs provision faster than logs sent to a S3 bucket.</p>

<p>Test 3 demonstrated that with a <code class="language-plaintext highlighter-rouge">max-aggregation-interval</code> set to “600,” it is possible to see an ingestion delay of up to 10.42 minutes (<code class="language-plaintext highlighter-rouge">cat logs-insights-results.csv | cut -d"," -f4 | sort -u -n | tail -n1</code>) and that it is a function of how long ago the last aggregation occurred. The minimum ingestion delay was 1.26 minutes (<code class="language-plaintext highlighter-rouge">cat logs-insights-results.csv | cut -d"," -f4 | sort -u -n | head -n2</code>). Lots of bad stuff could be detected late if you were expecting to use flow logs for real-time detection. I think that AWS would tell you that is not the use case that they were designed for. Instead use something like a Web Application Firewall (WAF).</p>

<p>We observed an unaccounted-for error in the ‘@timestamp’ <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_AnalyzeLogData-discoverable-fields.html">field generated by Cloud Watch Logs Insights</a> of +1 to -60 seconds during our test and +60 to -58 during a previous test. <strong>This demonstrates the value of benchmarking critical security systems.</strong></p>

<p>In this experiment, we demonstrated a novel technique for timestamping packets that will be reduced to a flow log by using the source port field. We showed how to verify the accuracy of the technique by comparing the epoch time calculated from the source port with the time logged by TCPdump.</p>

<p>We also demonstrated how to identify any missing packets using this technique and in the process identified a minor bug in <code class="language-plaintext highlighter-rouge">generate-test-traffic.py</code> that caused a packet not to be sent about every 195 seconds. <strong>This demonstrates (again) the importance of understanding the limitations of the security tools that we use.</strong> At some point, I will circle back and fix this bug.</p>

<p>Remember that the limited amount of testing that was performed is not sufficient to draw too many conclusions. Tests 1 and 2 should be repeated multiple times at different times of day, over the course of several days, in different regions to get more precise statistics. Test 3 should be repeated multiple times and should consider the impact of changing the <code class="language-plaintext highlighter-rouge">max-aggregation-interval</code> setting. The more the testing process is automated the lower the burden of repeating each test. But, I think that I have taken this little experiment far enough for now.</p>

<h2 id="wrap-up">Wrap Up</h2>

<p>Wow, we covered quite a bit in this Episode:</p>
<ul>
  <li>We made extensive use of the AWS CLI</li>
  <li>We used a Bash “while loop” to time AWS CLI calls</li>
  <li>We crafted packets using Scapy and read those packets using TCPdump.</li>
  <li>We also created a fairly sophisticated CloudWatch Logs Insights query, and</li>
  <li>Learned how we can use Python to visualize data.</li>
</ul>

<p>To learn more about TCPdump, Scapy, and packet crafting check out <a href="https://www.sans.org/cyber-security-courses/intrusion-detection-in-depth/">SANS SEC503: Intrusion Analysis In-Depth</a>. To learn more about using Python to create security tools like <code class="language-plaintext highlighter-rouge">generate-test-traffic.py</code> consider <a href="https://www.sans.org/cyber-security-courses/automating-information-security-with-python/">SANS SEC573: Automating Information Security with Python</a>.</p>

<h2 id="closing-comments">Closing Comments</h2>

<p>If you have thoughts or comments on today’s episode, feel free to chime in on the comments for this YouTube video.</p>

<p>If you appreciate this video and want to see more like it, be sure to give it a “thumbs up.”</p>

<p>Stay tuned for another installment of “Head in the Clouds” as announcements of new episodes are made on the SANS Cloud Security Twitter feed.</p>

<p>Meanwhile, be sure to check out the other great videos on the <a href="https://www.youtube.com/c/SANSCloudSecurity">SANS Cloud Security YouTube Channel</a>.</p>

<p>Take care.</p>

      
        <hr />
        <div class="social-share">
  <!--<h4>Share on</h4>
  <ul>
    <li>
      <a href="https://twitter.com/intent/tweet?text=/episodes/episode18/" class="twitter" title="Share on Twitter"><i class="fa fa-twitter"></i><span> Twitter</span></a>
    </li>
    <li>
      <a href="https://www.facebook.com/sharer/sharer.php?u=/episodes/episode18/" class="facebook" title="Share on Facebook"><i class="fa fa-facebook"></i><span> Facebook</span></a>
    </li>
    <li>
      <a href="https://plus.google.com/share?url=/episodes/episode18/" class="google-plus" title="Share on Google Plus"><i class="fa fa-google-plus"></i><span> Google+</span></a>
    </li>
  </ul>-->
</div><!-- /.social-share -->

      
    </div><!-- /.article-wrap -->
    
  </article>
</div><!-- /#index -->

<div class="footer-wrap">
  <footer>
    

<span>&copy; 2022 Head in the Clouds.<br /></span>


  </footer>
</div><!-- /.footer-wrap -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="/assets/js/scripts.min.js"></script>

<!-- Asynchronous Google Analytics snippet -->
<script>
  var _gaq = _gaq || [];
  var pluginUrl =
 '//www.google-analytics.com/plugins/ga/inpage_linkid.js';
  _gaq.push(['_require', 'inpage_linkid', pluginUrl]);
  _gaq.push(['_setAccount', 'UA-25220220-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'https://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>




</body>
</html>
